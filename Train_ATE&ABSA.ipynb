{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import bert_ATE\n",
    "from dataset import dataset_ATM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "lr = 2e-5\n",
    "model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    return model\n",
    "    \n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"D:/FYP_A+/Project_Update/NasiLemakReviewTags.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets using the split data\n",
    "review_train_ds = dataset_ATM(pd.read_csv(\"D:/FYP_A+/Project_Update/train_dataset.csv\"), tokenizer)\n",
    "review_test_ds = dataset_ATM(pd.read_csv(\"D:/FYP_A+/Project_Update/test_dataset.csv\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dataset.dataset_ATM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(review_train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['serving', 'only', 'nas', '##i', 'le', '##ma', '##k', ',', 'this', 'huge', 'restaurant', 'offers', 'various', 'sides', ',', 'fruit', 'juice', '##s', '/', 'smooth', '##ies', ',', 'and', 'dessert', '.', 'there', 'are', '2', 'lines', 'for', 'either', 'dining', 'in', 'or', 'take', 'out', '.', 'portions', 'were', 'not', 'too', 'big', 'and', 'the', 'food', 'was', 'delicious', '.', 'lots', 'of', 'workers', 'and', 'plenty', 'of'], tensor([ 3529,  2069, 17235,  2072,  3393,  2863,  2243,  1010,  2023,  4121,\n",
      "         4825,  4107,  2536,  3903,  1010,  5909, 10869,  2015,  1013,  5744,\n",
      "         3111,  1010,  1998, 18064,  1012,  2045,  2024,  1016,  3210,  2005,\n",
      "         2593,  7759,  1999,  2030,  2202,  2041,  1012,  8810,  2020,  2025,\n",
      "         2205,  2502,  1998,  1996,  2833,  2001, 12090,  1012,  7167,  1997,\n",
      "         3667,  1998,  7564,  1997]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(review_train_ds[12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 - Lengths Match: True\n",
      "Sample 2 - Lengths Match: True\n",
      "Sample 3 - Lengths Match: True\n",
      "Sample 4 - Lengths Match: True\n",
      "Sample 5 - Lengths Match: True\n",
      "Sample 6 - Lengths Match: True\n",
      "Sample 7 - Lengths Match: True\n",
      "Sample 8 - Lengths Match: True\n",
      "Sample 9 - Lengths Match: True\n",
      "Sample 10 - Lengths Match: True\n",
      "Sample 11 - Lengths Match: True\n",
      "Sample 12 - Lengths Match: True\n",
      "Sample 13 - Lengths Match: True\n",
      "Sample 14 - Lengths Match: True\n",
      "Sample 15 - Lengths Match: True\n",
      "Sample 16 - Lengths Match: True\n",
      "Sample 17 - Lengths Match: True\n",
      "Sample 18 - Lengths Match: True\n",
      "Sample 19 - Lengths Match: True\n",
      "Sample 20 - Lengths Match: True\n",
      "Sample 21 - Lengths Match: True\n",
      "Sample 22 - Lengths Match: True\n",
      "Sample 23 - Lengths Match: True\n",
      "Sample 24 - Lengths Match: True\n",
      "Sample 25 - Lengths Match: True\n",
      "Sample 26 - Lengths Match: True\n",
      "Sample 27 - Lengths Match: True\n",
      "Sample 28 - Lengths Match: True\n",
      "Sample 29 - Lengths Match: True\n",
      "Sample 30 - Lengths Match: True\n",
      "Sample 31 - Lengths Match: True\n",
      "Sample 32 - Lengths Match: True\n",
      "Sample 33 - Lengths Match: True\n",
      "Sample 34 - Lengths Match: True\n",
      "Sample 35 - Lengths Match: True\n",
      "Sample 36 - Lengths Match: True\n",
      "Sample 37 - Lengths Match: True\n",
      "Sample 38 - Lengths Match: True\n",
      "Sample 39 - Lengths Match: True\n",
      "Sample 40 - Lengths Match: True\n",
      "Sample 41 - Lengths Match: True\n",
      "Sample 42 - Lengths Match: True\n",
      "Sample 43 - Lengths Match: True\n",
      "Sample 44 - Lengths Match: True\n",
      "Sample 45 - Lengths Match: True\n",
      "Sample 46 - Lengths Match: True\n",
      "Sample 47 - Lengths Match: True\n",
      "Sample 48 - Lengths Match: True\n",
      "Sample 49 - Lengths Match: True\n",
      "Sample 50 - Lengths Match: True\n",
      "Sample 51 - Lengths Match: True\n",
      "Sample 52 - Lengths Match: True\n",
      "Sample 53 - Lengths Match: True\n",
      "Sample 54 - Lengths Match: True\n",
      "Sample 55 - Lengths Match: True\n",
      "Sample 56 - Lengths Match: True\n",
      "Sample 57 - Lengths Match: True\n",
      "Sample 58 - Lengths Match: True\n",
      "Sample 59 - Lengths Match: True\n",
      "Sample 60 - Lengths Match: True\n",
      "Sample 61 - Lengths Match: True\n",
      "Sample 62 - Lengths Match: True\n",
      "Sample 63 - Lengths Match: True\n",
      "Sample 64 - Lengths Match: True\n",
      "Sample 65 - Lengths Match: True\n",
      "Sample 66 - Lengths Match: True\n",
      "Sample 67 - Lengths Match: True\n",
      "Sample 68 - Lengths Match: True\n",
      "Sample 69 - Lengths Match: True\n",
      "Sample 70 - Lengths Match: True\n",
      "Sample 71 - Lengths Match: True\n",
      "Sample 72 - Lengths Match: True\n",
      "Sample 73 - Lengths Match: True\n",
      "Sample 74 - Lengths Match: True\n",
      "Sample 75 - Lengths Match: True\n",
      "Sample 76 - Lengths Match: True\n",
      "Sample 77 - Lengths Match: True\n",
      "Sample 78 - Lengths Match: True\n",
      "Sample 79 - Lengths Match: True\n",
      "Sample 80 - Lengths Match: True\n",
      "Sample 81 - Lengths Match: True\n",
      "Sample 82 - Lengths Match: True\n",
      "Sample 83 - Lengths Match: True\n",
      "Sample 84 - Lengths Match: True\n",
      "Sample 85 - Lengths Match: True\n",
      "Sample 86 - Lengths Match: True\n",
      "Sample 87 - Lengths Match: True\n",
      "Sample 88 - Lengths Match: True\n",
      "Sample 89 - Lengths Match: True\n",
      "Sample 90 - Lengths Match: True\n",
      "Sample 91 - Lengths Match: True\n",
      "Sample 92 - Lengths Match: True\n",
      "Sample 93 - Lengths Match: True\n",
      "Sample 94 - Lengths Match: True\n",
      "Sample 95 - Lengths Match: True\n",
      "Sample 96 - Lengths Match: True\n",
      "Sample 97 - Lengths Match: True\n",
      "Sample 98 - Lengths Match: True\n",
      "Sample 99 - Lengths Match: True\n",
      "Sample 100 - Lengths Match: True\n",
      "Sample 101 - Lengths Match: True\n",
      "Sample 102 - Lengths Match: True\n",
      "Sample 103 - Lengths Match: True\n",
      "Sample 104 - Lengths Match: True\n",
      "Sample 105 - Lengths Match: True\n",
      "Sample 106 - Lengths Match: True\n",
      "Sample 107 - Lengths Match: True\n",
      "Sample 108 - Lengths Match: True\n",
      "Sample 109 - Lengths Match: True\n",
      "Sample 110 - Lengths Match: True\n",
      "Sample 111 - Lengths Match: True\n",
      "Sample 112 - Lengths Match: True\n",
      "Sample 113 - Lengths Match: True\n",
      "Sample 114 - Lengths Match: True\n",
      "Sample 115 - Lengths Match: True\n",
      "Sample 116 - Lengths Match: True\n",
      "Sample 117 - Lengths Match: True\n",
      "Sample 118 - Lengths Match: True\n",
      "Sample 119 - Lengths Match: True\n",
      "Sample 120 - Lengths Match: True\n",
      "Sample 121 - Lengths Match: True\n",
      "Sample 122 - Lengths Match: True\n",
      "Sample 123 - Lengths Match: True\n",
      "Sample 124 - Lengths Match: True\n",
      "Sample 125 - Lengths Match: True\n",
      "Sample 126 - Lengths Match: True\n",
      "Sample 127 - Lengths Match: True\n",
      "Sample 128 - Lengths Match: True\n",
      "Sample 129 - Lengths Match: True\n",
      "Sample 130 - Lengths Match: True\n",
      "Sample 131 - Lengths Match: True\n",
      "Sample 132 - Lengths Match: True\n",
      "Sample 133 - Lengths Match: True\n",
      "Sample 134 - Lengths Match: True\n",
      "Sample 135 - Lengths Match: True\n",
      "Sample 136 - Lengths Match: True\n",
      "Sample 137 - Lengths Match: True\n",
      "Sample 138 - Lengths Match: True\n",
      "Sample 139 - Lengths Match: True\n",
      "Sample 140 - Lengths Match: True\n",
      "Sample 141 - Lengths Match: True\n",
      "Sample 142 - Lengths Match: True\n",
      "Sample 143 - Lengths Match: True\n",
      "Sample 144 - Lengths Match: True\n",
      "Sample 145 - Lengths Match: True\n",
      "Sample 146 - Lengths Match: True\n",
      "Sample 147 - Lengths Match: True\n",
      "Sample 148 - Lengths Match: True\n",
      "Sample 149 - Lengths Match: True\n",
      "Sample 150 - Lengths Match: True\n",
      "Sample 151 - Lengths Match: True\n",
      "Sample 152 - Lengths Match: True\n",
      "Sample 153 - Lengths Match: True\n",
      "Sample 154 - Lengths Match: True\n",
      "Sample 155 - Lengths Match: True\n",
      "Sample 156 - Lengths Match: True\n",
      "Sample 157 - Lengths Match: True\n",
      "Sample 158 - Lengths Match: True\n",
      "Sample 159 - Lengths Match: True\n",
      "Sample 160 - Lengths Match: True\n",
      "Sample 161 - Lengths Match: True\n",
      "Sample 162 - Lengths Match: True\n",
      "Sample 163 - Lengths Match: True\n",
      "Sample 164 - Lengths Match: True\n",
      "Sample 165 - Lengths Match: True\n",
      "Sample 166 - Lengths Match: True\n",
      "Sample 167 - Lengths Match: True\n",
      "Sample 168 - Lengths Match: True\n",
      "Sample 169 - Lengths Match: True\n",
      "Sample 170 - Lengths Match: True\n",
      "Sample 171 - Lengths Match: True\n",
      "Sample 172 - Lengths Match: True\n",
      "Sample 173 - Lengths Match: True\n",
      "Sample 174 - Lengths Match: True\n",
      "Sample 175 - Lengths Match: True\n",
      "Sample 176 - Lengths Match: True\n",
      "Sample 177 - Lengths Match: True\n",
      "Sample 178 - Lengths Match: True\n",
      "Sample 179 - Lengths Match: True\n",
      "Sample 180 - Lengths Match: True\n",
      "Sample 181 - Lengths Match: True\n",
      "Sample 182 - Lengths Match: True\n",
      "Sample 183 - Lengths Match: True\n",
      "Sample 184 - Lengths Match: True\n",
      "Sample 185 - Lengths Match: True\n",
      "Sample 186 - Lengths Match: True\n",
      "Sample 187 - Lengths Match: True\n",
      "Sample 188 - Lengths Match: True\n",
      "Sample 189 - Lengths Match: True\n",
      "Sample 190 - Lengths Match: True\n",
      "Sample 191 - Lengths Match: True\n",
      "Sample 192 - Lengths Match: True\n",
      "Sample 193 - Lengths Match: True\n",
      "Sample 194 - Lengths Match: True\n",
      "Sample 195 - Lengths Match: True\n",
      "Sample 196 - Lengths Match: True\n",
      "Sample 197 - Lengths Match: True\n",
      "Sample 198 - Lengths Match: True\n",
      "Sample 199 - Lengths Match: True\n",
      "Sample 200 - Lengths Match: True\n",
      "Sample 201 - Lengths Match: True\n",
      "Sample 202 - Lengths Match: True\n",
      "Sample 203 - Lengths Match: True\n",
      "Sample 204 - Lengths Match: True\n",
      "Sample 205 - Lengths Match: True\n",
      "Sample 206 - Lengths Match: True\n",
      "Sample 207 - Lengths Match: True\n",
      "Sample 208 - Lengths Match: True\n",
      "Sample 209 - Lengths Match: True\n",
      "Sample 210 - Lengths Match: True\n",
      "Sample 211 - Lengths Match: True\n",
      "Sample 212 - Lengths Match: True\n",
      "Sample 213 - Lengths Match: True\n",
      "Sample 214 - Lengths Match: True\n",
      "Sample 215 - Lengths Match: True\n",
      "Sample 216 - Lengths Match: True\n",
      "Sample 217 - Lengths Match: True\n",
      "Sample 218 - Lengths Match: True\n",
      "Sample 219 - Lengths Match: True\n",
      "Sample 220 - Lengths Match: True\n",
      "Sample 221 - Lengths Match: True\n",
      "Sample 222 - Lengths Match: True\n",
      "Sample 223 - Lengths Match: True\n",
      "Sample 224 - Lengths Match: True\n",
      "Sample 225 - Lengths Match: True\n",
      "Sample 226 - Lengths Match: True\n",
      "Sample 227 - Lengths Match: True\n",
      "Sample 228 - Lengths Match: True\n",
      "Sample 229 - Lengths Match: True\n",
      "Sample 230 - Lengths Match: True\n",
      "Sample 231 - Lengths Match: True\n",
      "Sample 232 - Lengths Match: True\n",
      "Sample 233 - Lengths Match: True\n",
      "Sample 234 - Lengths Match: True\n",
      "Sample 235 - Lengths Match: True\n",
      "Sample 236 - Lengths Match: True\n",
      "Sample 237 - Lengths Match: True\n",
      "Sample 238 - Lengths Match: True\n",
      "Sample 239 - Lengths Match: True\n",
      "Sample 240 - Lengths Match: True\n",
      "Sample 241 - Lengths Match: True\n",
      "Sample 242 - Lengths Match: True\n",
      "Sample 243 - Lengths Match: True\n",
      "Sample 244 - Lengths Match: True\n",
      "Sample 245 - Lengths Match: True\n",
      "Sample 246 - Lengths Match: True\n",
      "Sample 247 - Lengths Match: True\n",
      "Sample 248 - Lengths Match: True\n",
      "Sample 249 - Lengths Match: True\n",
      "Sample 250 - Lengths Match: True\n",
      "Sample 251 - Lengths Match: True\n",
      "Sample 252 - Lengths Match: True\n",
      "Sample 253 - Lengths Match: True\n",
      "Sample 254 - Lengths Match: True\n",
      "Sample 255 - Lengths Match: True\n",
      "Sample 256 - Lengths Match: True\n",
      "Sample 257 - Lengths Match: True\n",
      "Sample 258 - Lengths Match: True\n",
      "Sample 259 - Lengths Match: True\n",
      "Sample 260 - Lengths Match: True\n",
      "Sample 261 - Lengths Match: True\n",
      "Sample 262 - Lengths Match: True\n",
      "Sample 263 - Lengths Match: True\n",
      "Sample 264 - Lengths Match: True\n",
      "Sample 265 - Lengths Match: True\n",
      "Sample 266 - Lengths Match: True\n",
      "Sample 267 - Lengths Match: True\n",
      "Sample 268 - Lengths Match: True\n",
      "Sample 269 - Lengths Match: True\n",
      "Sample 270 - Lengths Match: True\n",
      "Sample 271 - Lengths Match: True\n",
      "Sample 272 - Lengths Match: True\n",
      "Sample 273 - Lengths Match: True\n",
      "Sample 274 - Lengths Match: True\n",
      "Sample 275 - Lengths Match: True\n",
      "Sample 276 - Lengths Match: True\n",
      "Sample 277 - Lengths Match: True\n",
      "Sample 278 - Lengths Match: True\n",
      "Sample 279 - Lengths Match: True\n",
      "Sample 280 - Lengths Match: True\n",
      "Sample 281 - Lengths Match: True\n",
      "Sample 282 - Lengths Match: True\n",
      "Sample 283 - Lengths Match: True\n",
      "Sample 284 - Lengths Match: True\n",
      "Sample 285 - Lengths Match: True\n",
      "Sample 286 - Lengths Match: True\n",
      "Sample 287 - Lengths Match: True\n",
      "Sample 288 - Lengths Match: True\n",
      "Sample 289 - Lengths Match: True\n",
      "Sample 290 - Lengths Match: True\n",
      "Sample 291 - Lengths Match: True\n",
      "Sample 292 - Lengths Match: True\n",
      "Sample 293 - Lengths Match: True\n",
      "Sample 294 - Lengths Match: True\n",
      "Sample 295 - Lengths Match: True\n",
      "Sample 296 - Lengths Match: True\n",
      "Sample 297 - Lengths Match: True\n",
      "Sample 298 - Lengths Match: True\n",
      "Sample 299 - Lengths Match: True\n",
      "Sample 300 - Lengths Match: True\n",
      "Sample 301 - Lengths Match: True\n",
      "Sample 302 - Lengths Match: True\n",
      "Sample 303 - Lengths Match: True\n",
      "Sample 304 - Lengths Match: True\n",
      "Sample 305 - Lengths Match: True\n",
      "Sample 306 - Lengths Match: True\n",
      "Sample 307 - Lengths Match: True\n",
      "Sample 308 - Lengths Match: True\n",
      "Sample 309 - Lengths Match: True\n",
      "Sample 310 - Lengths Match: True\n",
      "Sample 311 - Lengths Match: True\n",
      "Sample 312 - Lengths Match: True\n",
      "Sample 313 - Lengths Match: True\n",
      "Sample 314 - Lengths Match: True\n",
      "Sample 315 - Lengths Match: True\n",
      "Sample 316 - Lengths Match: True\n",
      "Sample 317 - Lengths Match: True\n",
      "Sample 318 - Lengths Match: True\n",
      "Sample 319 - Lengths Match: True\n",
      "Sample 320 - Lengths Match: True\n",
      "Sample 321 - Lengths Match: True\n",
      "Sample 322 - Lengths Match: True\n",
      "Sample 323 - Lengths Match: True\n",
      "Sample 324 - Lengths Match: True\n",
      "Sample 325 - Lengths Match: True\n",
      "Sample 326 - Lengths Match: True\n",
      "Sample 327 - Lengths Match: True\n",
      "Sample 328 - Lengths Match: True\n",
      "Sample 329 - Lengths Match: True\n",
      "Sample 330 - Lengths Match: True\n",
      "Sample 331 - Lengths Match: True\n",
      "Sample 332 - Lengths Match: True\n",
      "Sample 333 - Lengths Match: True\n",
      "Sample 334 - Lengths Match: True\n",
      "Sample 335 - Lengths Match: True\n",
      "Sample 336 - Lengths Match: True\n",
      "Sample 337 - Lengths Match: True\n",
      "Sample 338 - Lengths Match: True\n",
      "Sample 339 - Lengths Match: True\n",
      "Sample 340 - Lengths Match: True\n",
      "Sample 341 - Lengths Match: True\n",
      "Sample 342 - Lengths Match: True\n",
      "Sample 343 - Lengths Match: True\n",
      "Sample 344 - Lengths Match: True\n",
      "Sample 345 - Lengths Match: True\n",
      "Sample 346 - Lengths Match: True\n",
      "Sample 347 - Lengths Match: True\n",
      "Sample 348 - Lengths Match: True\n",
      "Sample 349 - Lengths Match: True\n",
      "Sample 350 - Lengths Match: True\n",
      "Sample 351 - Lengths Match: True\n",
      "Sample 352 - Lengths Match: True\n",
      "Sample 353 - Lengths Match: True\n",
      "Sample 354 - Lengths Match: True\n",
      "Sample 355 - Lengths Match: True\n",
      "Sample 356 - Lengths Match: True\n",
      "Sample 357 - Lengths Match: True\n",
      "Sample 358 - Lengths Match: True\n",
      "Sample 359 - Lengths Match: True\n",
      "Sample 360 - Lengths Match: True\n",
      "Sample 361 - Lengths Match: True\n",
      "Sample 362 - Lengths Match: True\n",
      "Sample 363 - Lengths Match: True\n",
      "Sample 364 - Lengths Match: True\n",
      "Sample 365 - Lengths Match: True\n",
      "Sample 366 - Lengths Match: True\n",
      "Sample 367 - Lengths Match: True\n",
      "Sample 368 - Lengths Match: True\n",
      "Sample 369 - Lengths Match: True\n",
      "Sample 370 - Lengths Match: True\n",
      "Sample 371 - Lengths Match: True\n",
      "Sample 372 - Lengths Match: True\n",
      "Sample 373 - Lengths Match: True\n",
      "Sample 374 - Lengths Match: True\n",
      "Sample 375 - Lengths Match: True\n",
      "Sample 376 - Lengths Match: True\n",
      "Sample 377 - Lengths Match: True\n",
      "Sample 378 - Lengths Match: True\n",
      "Sample 379 - Lengths Match: True\n",
      "Sample 380 - Lengths Match: True\n",
      "Sample 381 - Lengths Match: True\n",
      "Sample 382 - Lengths Match: True\n",
      "Sample 383 - Lengths Match: True\n",
      "Sample 384 - Lengths Match: True\n",
      "Sample 385 - Lengths Match: True\n",
      "Sample 386 - Lengths Match: True\n",
      "Sample 387 - Lengths Match: True\n",
      "Sample 388 - Lengths Match: True\n",
      "Sample 389 - Lengths Match: True\n",
      "Sample 390 - Lengths Match: True\n",
      "Sample 391 - Lengths Match: True\n",
      "Sample 392 - Lengths Match: True\n",
      "Sample 393 - Lengths Match: True\n",
      "Sample 394 - Lengths Match: True\n",
      "Sample 395 - Lengths Match: True\n",
      "Sample 396 - Lengths Match: True\n",
      "Sample 397 - Lengths Match: True\n",
      "Sample 398 - Lengths Match: True\n",
      "Sample 399 - Lengths Match: True\n",
      "Sample 400 - Lengths Match: True\n",
      "Sample 401 - Lengths Match: True\n",
      "Sample 402 - Lengths Match: True\n",
      "Sample 403 - Lengths Match: True\n",
      "Sample 404 - Lengths Match: True\n",
      "Sample 405 - Lengths Match: True\n",
      "Sample 406 - Lengths Match: True\n",
      "Sample 407 - Lengths Match: True\n",
      "Sample 408 - Lengths Match: True\n",
      "Sample 409 - Lengths Match: True\n",
      "Sample 410 - Lengths Match: True\n",
      "Sample 411 - Lengths Match: True\n",
      "Sample 412 - Lengths Match: True\n",
      "Sample 413 - Lengths Match: True\n",
      "Sample 414 - Lengths Match: True\n",
      "Sample 415 - Lengths Match: True\n",
      "Sample 416 - Lengths Match: True\n",
      "Sample 417 - Lengths Match: True\n",
      "Sample 418 - Lengths Match: True\n",
      "Sample 419 - Lengths Match: True\n",
      "Sample 420 - Lengths Match: True\n",
      "Sample 421 - Lengths Match: True\n",
      "Sample 422 - Lengths Match: True\n",
      "Sample 423 - Lengths Match: True\n",
      "Sample 424 - Lengths Match: True\n",
      "Sample 425 - Lengths Match: True\n",
      "Sample 426 - Lengths Match: True\n",
      "Sample 427 - Lengths Match: True\n",
      "Sample 428 - Lengths Match: True\n",
      "Sample 429 - Lengths Match: True\n",
      "Sample 430 - Lengths Match: True\n",
      "Sample 431 - Lengths Match: True\n",
      "Sample 432 - Lengths Match: True\n",
      "Sample 433 - Lengths Match: True\n",
      "Sample 434 - Lengths Match: True\n",
      "Sample 435 - Lengths Match: True\n",
      "Sample 436 - Lengths Match: True\n",
      "Sample 437 - Lengths Match: True\n",
      "Sample 438 - Lengths Match: True\n",
      "Sample 439 - Lengths Match: True\n",
      "Sample 440 - Lengths Match: True\n",
      "Sample 441 - Lengths Match: True\n",
      "Sample 442 - Lengths Match: True\n",
      "Sample 443 - Lengths Match: True\n",
      "Sample 444 - Lengths Match: True\n",
      "Sample 445 - Lengths Match: True\n",
      "Sample 446 - Lengths Match: True\n",
      "Sample 447 - Lengths Match: True\n",
      "Sample 448 - Lengths Match: True\n",
      "Sample 449 - Lengths Match: True\n",
      "Sample 450 - Lengths Match: True\n",
      "Sample 451 - Lengths Match: True\n",
      "Sample 452 - Lengths Match: True\n",
      "Sample 453 - Lengths Match: True\n",
      "Sample 454 - Lengths Match: True\n",
      "Sample 455 - Lengths Match: True\n",
      "Sample 456 - Lengths Match: True\n",
      "Sample 457 - Lengths Match: True\n",
      "Sample 458 - Lengths Match: True\n",
      "Sample 459 - Lengths Match: True\n",
      "Sample 460 - Lengths Match: True\n",
      "Sample 461 - Lengths Match: True\n",
      "Sample 462 - Lengths Match: True\n",
      "Sample 463 - Lengths Match: True\n",
      "Sample 464 - Lengths Match: True\n",
      "Sample 465 - Lengths Match: True\n",
      "Sample 466 - Lengths Match: True\n",
      "Sample 467 - Lengths Match: True\n",
      "Sample 468 - Lengths Match: True\n",
      "Sample 469 - Lengths Match: True\n",
      "Sample 470 - Lengths Match: True\n",
      "Sample 471 - Lengths Match: True\n",
      "Sample 472 - Lengths Match: True\n",
      "Sample 473 - Lengths Match: True\n",
      "Sample 474 - Lengths Match: True\n",
      "Sample 475 - Lengths Match: True\n",
      "Sample 476 - Lengths Match: True\n",
      "Sample 477 - Lengths Match: True\n",
      "Sample 478 - Lengths Match: True\n",
      "Sample 479 - Lengths Match: True\n",
      "Sample 480 - Lengths Match: True\n",
      "Sample 481 - Lengths Match: True\n",
      "Sample 482 - Lengths Match: True\n",
      "Sample 483 - Lengths Match: True\n",
      "Sample 484 - Lengths Match: True\n",
      "Sample 485 - Lengths Match: True\n",
      "Sample 486 - Lengths Match: True\n",
      "Sample 487 - Lengths Match: True\n",
      "Sample 488 - Lengths Match: True\n",
      "Sample 489 - Lengths Match: True\n",
      "Sample 490 - Lengths Match: True\n",
      "Sample 491 - Lengths Match: True\n",
      "Sample 492 - Lengths Match: True\n",
      "Sample 493 - Lengths Match: True\n",
      "Sample 494 - Lengths Match: True\n",
      "Sample 495 - Lengths Match: True\n",
      "Sample 496 - Lengths Match: True\n",
      "Sample 497 - Lengths Match: True\n",
      "Sample 498 - Lengths Match: True\n",
      "Sample 499 - Lengths Match: True\n",
      "Sample 500 - Lengths Match: True\n",
      "Sample 501 - Lengths Match: True\n",
      "Sample 502 - Lengths Match: True\n",
      "Sample 503 - Lengths Match: True\n",
      "Sample 504 - Lengths Match: True\n",
      "Sample 505 - Lengths Match: True\n",
      "Sample 506 - Lengths Match: True\n",
      "Sample 507 - Lengths Match: True\n",
      "Sample 508 - Lengths Match: True\n",
      "Sample 509 - Lengths Match: True\n",
      "Sample 510 - Lengths Match: True\n",
      "Sample 511 - Lengths Match: True\n",
      "Sample 512 - Lengths Match: True\n",
      "Sample 513 - Lengths Match: True\n",
      "Sample 514 - Lengths Match: True\n",
      "Sample 515 - Lengths Match: True\n",
      "Sample 516 - Lengths Match: True\n",
      "Sample 517 - Lengths Match: True\n",
      "Sample 518 - Lengths Match: True\n",
      "Sample 519 - Lengths Match: True\n",
      "Sample 520 - Lengths Match: True\n",
      "Sample 521 - Lengths Match: True\n",
      "Sample 522 - Lengths Match: True\n",
      "Sample 523 - Lengths Match: True\n",
      "Sample 524 - Lengths Match: True\n",
      "Sample 525 - Lengths Match: True\n",
      "Sample 526 - Lengths Match: True\n",
      "Sample 527 - Lengths Match: True\n",
      "Sample 528 - Lengths Match: True\n",
      "Sample 529 - Lengths Match: True\n",
      "Sample 530 - Lengths Match: True\n",
      "Sample 531 - Lengths Match: True\n",
      "Sample 532 - Lengths Match: True\n",
      "Sample 533 - Lengths Match: True\n",
      "Sample 534 - Lengths Match: True\n",
      "Sample 535 - Lengths Match: True\n",
      "Sample 536 - Lengths Match: True\n",
      "Sample 537 - Lengths Match: True\n",
      "Sample 538 - Lengths Match: True\n",
      "Sample 539 - Lengths Match: True\n",
      "Sample 540 - Lengths Match: True\n",
      "Sample 541 - Lengths Match: True\n",
      "Sample 542 - Lengths Match: True\n",
      "Sample 543 - Lengths Match: True\n",
      "Sample 544 - Lengths Match: True\n",
      "Sample 545 - Lengths Match: True\n",
      "Sample 546 - Lengths Match: True\n",
      "Sample 547 - Lengths Match: True\n",
      "Sample 548 - Lengths Match: True\n",
      "Sample 549 - Lengths Match: True\n",
      "Sample 550 - Lengths Match: True\n",
      "Sample 551 - Lengths Match: True\n",
      "Sample 552 - Lengths Match: True\n",
      "Sample 553 - Lengths Match: True\n",
      "Sample 554 - Lengths Match: True\n",
      "Sample 555 - Lengths Match: True\n",
      "Sample 556 - Lengths Match: True\n",
      "Sample 557 - Lengths Match: True\n",
      "Sample 558 - Lengths Match: True\n",
      "Sample 559 - Lengths Match: True\n",
      "Sample 560 - Lengths Match: True\n",
      "Sample 561 - Lengths Match: True\n",
      "Sample 562 - Lengths Match: True\n",
      "Sample 563 - Lengths Match: True\n",
      "Sample 564 - Lengths Match: True\n",
      "Sample 565 - Lengths Match: True\n",
      "Sample 566 - Lengths Match: True\n",
      "Sample 567 - Lengths Match: True\n",
      "Sample 568 - Lengths Match: True\n",
      "Sample 569 - Lengths Match: True\n",
      "Sample 570 - Lengths Match: True\n",
      "Sample 571 - Lengths Match: True\n",
      "Sample 572 - Lengths Match: True\n",
      "Sample 573 - Lengths Match: True\n",
      "Sample 574 - Lengths Match: True\n",
      "Sample 575 - Lengths Match: True\n",
      "Sample 576 - Lengths Match: True\n",
      "Sample 577 - Lengths Match: True\n",
      "Sample 578 - Lengths Match: True\n",
      "Sample 579 - Lengths Match: True\n",
      "Sample 580 - Lengths Match: True\n",
      "Sample 581 - Lengths Match: True\n",
      "Sample 582 - Lengths Match: True\n",
      "Sample 583 - Lengths Match: True\n",
      "Sample 584 - Lengths Match: True\n",
      "Sample 585 - Lengths Match: True\n",
      "Sample 586 - Lengths Match: True\n",
      "Sample 587 - Lengths Match: True\n",
      "Sample 588 - Lengths Match: True\n",
      "Sample 589 - Lengths Match: True\n",
      "Sample 590 - Lengths Match: True\n",
      "Sample 591 - Lengths Match: True\n",
      "Sample 592 - Lengths Match: True\n",
      "Sample 593 - Lengths Match: True\n",
      "Sample 594 - Lengths Match: True\n",
      "Sample 595 - Lengths Match: True\n",
      "Sample 596 - Lengths Match: True\n",
      "Sample 597 - Lengths Match: True\n",
      "Sample 598 - Lengths Match: True\n",
      "Sample 599 - Lengths Match: True\n",
      "Sample 600 - Lengths Match: True\n",
      "Sample 601 - Lengths Match: True\n",
      "Sample 602 - Lengths Match: True\n",
      "Sample 603 - Lengths Match: True\n",
      "Sample 604 - Lengths Match: True\n",
      "Sample 605 - Lengths Match: True\n",
      "Sample 606 - Lengths Match: True\n",
      "Sample 607 - Lengths Match: True\n",
      "Sample 608 - Lengths Match: True\n",
      "Sample 609 - Lengths Match: True\n",
      "Sample 610 - Lengths Match: True\n",
      "Sample 611 - Lengths Match: True\n",
      "Sample 612 - Lengths Match: True\n",
      "Sample 613 - Lengths Match: True\n",
      "Sample 614 - Lengths Match: True\n",
      "Sample 615 - Lengths Match: True\n",
      "Sample 616 - Lengths Match: True\n",
      "Sample 617 - Lengths Match: True\n",
      "Sample 618 - Lengths Match: True\n",
      "Sample 619 - Lengths Match: True\n",
      "Sample 620 - Lengths Match: True\n",
      "Sample 621 - Lengths Match: True\n",
      "Sample 622 - Lengths Match: True\n",
      "Sample 623 - Lengths Match: True\n",
      "Sample 624 - Lengths Match: True\n",
      "Sample 625 - Lengths Match: True\n",
      "Sample 626 - Lengths Match: True\n",
      "Sample 627 - Lengths Match: True\n",
      "Sample 628 - Lengths Match: True\n",
      "Sample 629 - Lengths Match: True\n",
      "Sample 630 - Lengths Match: True\n",
      "Sample 631 - Lengths Match: True\n",
      "Sample 632 - Lengths Match: True\n",
      "Sample 633 - Lengths Match: True\n",
      "Sample 634 - Lengths Match: True\n",
      "Sample 635 - Lengths Match: True\n",
      "Sample 636 - Lengths Match: True\n",
      "Sample 637 - Lengths Match: True\n",
      "Sample 638 - Lengths Match: True\n",
      "Sample 639 - Lengths Match: True\n",
      "Sample 640 - Lengths Match: True\n",
      "Sample 641 - Lengths Match: True\n",
      "Sample 642 - Lengths Match: True\n",
      "Sample 643 - Lengths Match: True\n",
      "Sample 644 - Lengths Match: True\n",
      "Sample 645 - Lengths Match: True\n",
      "Sample 646 - Lengths Match: True\n",
      "Sample 647 - Lengths Match: True\n",
      "Sample 648 - Lengths Match: True\n",
      "Sample 649 - Lengths Match: True\n",
      "Sample 650 - Lengths Match: True\n",
      "Sample 651 - Lengths Match: True\n",
      "Sample 652 - Lengths Match: True\n",
      "Sample 653 - Lengths Match: True\n",
      "Sample 654 - Lengths Match: True\n",
      "Sample 655 - Lengths Match: True\n",
      "Sample 656 - Lengths Match: True\n",
      "Sample 657 - Lengths Match: True\n",
      "Sample 658 - Lengths Match: True\n",
      "Sample 659 - Lengths Match: True\n",
      "Sample 660 - Lengths Match: True\n",
      "Sample 661 - Lengths Match: True\n",
      "Sample 662 - Lengths Match: True\n",
      "Sample 663 - Lengths Match: True\n",
      "Sample 664 - Lengths Match: True\n",
      "Sample 665 - Lengths Match: True\n",
      "Sample 666 - Lengths Match: True\n",
      "Sample 667 - Lengths Match: True\n",
      "Sample 668 - Lengths Match: True\n",
      "Sample 669 - Lengths Match: True\n",
      "Sample 670 - Lengths Match: True\n",
      "Sample 671 - Lengths Match: True\n",
      "Sample 672 - Lengths Match: True\n",
      "Sample 673 - Lengths Match: True\n",
      "Sample 674 - Lengths Match: True\n",
      "Sample 675 - Lengths Match: True\n",
      "Sample 676 - Lengths Match: True\n",
      "Sample 677 - Lengths Match: True\n",
      "Sample 678 - Lengths Match: True\n",
      "Sample 679 - Lengths Match: True\n",
      "Sample 680 - Lengths Match: True\n",
      "Sample 681 - Lengths Match: True\n",
      "Sample 682 - Lengths Match: True\n",
      "Sample 683 - Lengths Match: True\n",
      "Sample 684 - Lengths Match: True\n",
      "Sample 685 - Lengths Match: True\n",
      "Sample 686 - Lengths Match: True\n",
      "Sample 687 - Lengths Match: True\n",
      "Sample 688 - Lengths Match: True\n",
      "Sample 689 - Lengths Match: True\n",
      "Sample 690 - Lengths Match: True\n",
      "Sample 691 - Lengths Match: True\n",
      "Sample 692 - Lengths Match: True\n",
      "Sample 693 - Lengths Match: True\n",
      "Sample 694 - Lengths Match: True\n",
      "Sample 695 - Lengths Match: True\n",
      "Sample 696 - Lengths Match: True\n",
      "Sample 697 - Lengths Match: True\n",
      "Sample 698 - Lengths Match: True\n",
      "Sample 699 - Lengths Match: True\n",
      "Sample 700 - Lengths Match: True\n",
      "Sample 701 - Lengths Match: True\n",
      "Sample 702 - Lengths Match: True\n",
      "Sample 703 - Lengths Match: True\n",
      "Sample 704 - Lengths Match: True\n",
      "Sample 705 - Lengths Match: True\n",
      "Sample 706 - Lengths Match: True\n",
      "Sample 707 - Lengths Match: True\n",
      "Sample 708 - Lengths Match: True\n",
      "Sample 709 - Lengths Match: True\n",
      "Sample 710 - Lengths Match: True\n",
      "Sample 711 - Lengths Match: True\n",
      "Sample 712 - Lengths Match: True\n",
      "Sample 713 - Lengths Match: True\n",
      "Sample 714 - Lengths Match: True\n",
      "Sample 715 - Lengths Match: True\n",
      "Sample 716 - Lengths Match: True\n",
      "Sample 717 - Lengths Match: True\n",
      "Sample 718 - Lengths Match: True\n",
      "Sample 719 - Lengths Match: True\n",
      "Sample 720 - Lengths Match: True\n",
      "Sample 721 - Lengths Match: True\n",
      "Sample 722 - Lengths Match: True\n",
      "Sample 723 - Lengths Match: True\n",
      "Sample 724 - Lengths Match: True\n",
      "Sample 725 - Lengths Match: True\n",
      "Sample 726 - Lengths Match: True\n",
      "Sample 727 - Lengths Match: True\n",
      "Sample 728 - Lengths Match: True\n",
      "Sample 729 - Lengths Match: True\n",
      "Sample 730 - Lengths Match: True\n",
      "Sample 731 - Lengths Match: True\n",
      "Sample 732 - Lengths Match: True\n",
      "Sample 733 - Lengths Match: True\n",
      "Sample 734 - Lengths Match: True\n",
      "Sample 735 - Lengths Match: True\n",
      "Sample 736 - Lengths Match: True\n",
      "Sample 737 - Lengths Match: True\n",
      "Sample 738 - Lengths Match: True\n",
      "Sample 739 - Lengths Match: True\n",
      "Sample 740 - Lengths Match: True\n",
      "Sample 741 - Lengths Match: True\n",
      "Sample 742 - Lengths Match: True\n",
      "Sample 743 - Lengths Match: True\n",
      "Sample 744 - Lengths Match: True\n",
      "Sample 745 - Lengths Match: True\n",
      "Sample 746 - Lengths Match: True\n",
      "Sample 747 - Lengths Match: True\n",
      "Sample 748 - Lengths Match: True\n",
      "Sample 749 - Lengths Match: True\n",
      "Sample 750 - Lengths Match: True\n",
      "Sample 751 - Lengths Match: True\n",
      "Sample 752 - Lengths Match: True\n",
      "Sample 753 - Lengths Match: True\n",
      "Sample 754 - Lengths Match: True\n",
      "Sample 755 - Lengths Match: True\n",
      "Sample 756 - Lengths Match: True\n",
      "Sample 757 - Lengths Match: True\n",
      "Sample 758 - Lengths Match: True\n",
      "Sample 759 - Lengths Match: True\n",
      "Sample 760 - Lengths Match: True\n",
      "Sample 761 - Lengths Match: True\n",
      "Sample 762 - Lengths Match: True\n",
      "Sample 763 - Lengths Match: True\n",
      "Sample 764 - Lengths Match: True\n",
      "Sample 765 - Lengths Match: True\n",
      "Sample 766 - Lengths Match: True\n",
      "Sample 767 - Lengths Match: True\n",
      "Sample 768 - Lengths Match: True\n",
      "Sample 769 - Lengths Match: True\n",
      "Sample 770 - Lengths Match: True\n",
      "Sample 771 - Lengths Match: True\n",
      "Sample 772 - Lengths Match: True\n",
      "Sample 773 - Lengths Match: True\n",
      "Sample 774 - Lengths Match: True\n",
      "Sample 775 - Lengths Match: True\n",
      "Sample 776 - Lengths Match: True\n",
      "Sample 777 - Lengths Match: True\n",
      "Sample 778 - Lengths Match: True\n",
      "Sample 779 - Lengths Match: True\n",
      "Sample 780 - Lengths Match: True\n",
      "Sample 781 - Lengths Match: True\n",
      "Sample 782 - Lengths Match: True\n",
      "Sample 783 - Lengths Match: True\n",
      "Sample 784 - Lengths Match: True\n",
      "Sample 785 - Lengths Match: True\n",
      "Sample 786 - Lengths Match: True\n",
      "Sample 787 - Lengths Match: True\n",
      "Sample 788 - Lengths Match: True\n",
      "Sample 789 - Lengths Match: True\n",
      "Sample 790 - Lengths Match: True\n",
      "Sample 791 - Lengths Match: True\n",
      "Sample 792 - Lengths Match: True\n",
      "Sample 793 - Lengths Match: True\n",
      "Sample 794 - Lengths Match: True\n",
      "Sample 795 - Lengths Match: True\n",
      "Sample 796 - Lengths Match: True\n",
      "Sample 797 - Lengths Match: True\n",
      "Sample 798 - Lengths Match: True\n",
      "Sample 799 - Lengths Match: True\n",
      "Sample 800 - Lengths Match: True\n",
      "Sample 801 - Lengths Match: True\n",
      "Sample 802 - Lengths Match: True\n",
      "Sample 803 - Lengths Match: True\n",
      "Sample 804 - Lengths Match: True\n",
      "Sample 805 - Lengths Match: True\n",
      "Sample 806 - Lengths Match: True\n",
      "Sample 807 - Lengths Match: True\n",
      "Sample 808 - Lengths Match: True\n",
      "Sample 809 - Lengths Match: True\n",
      "Sample 810 - Lengths Match: True\n",
      "Sample 811 - Lengths Match: True\n",
      "Sample 812 - Lengths Match: True\n",
      "Sample 813 - Lengths Match: True\n",
      "Sample 814 - Lengths Match: True\n",
      "Sample 815 - Lengths Match: True\n",
      "Sample 816 - Lengths Match: True\n",
      "Sample 817 - Lengths Match: True\n",
      "Sample 818 - Lengths Match: True\n",
      "Sample 819 - Lengths Match: True\n",
      "Sample 820 - Lengths Match: True\n",
      "Sample 821 - Lengths Match: True\n",
      "Sample 822 - Lengths Match: True\n",
      "Sample 823 - Lengths Match: True\n",
      "Sample 824 - Lengths Match: True\n",
      "Sample 825 - Lengths Match: True\n",
      "Sample 826 - Lengths Match: True\n",
      "Sample 827 - Lengths Match: True\n",
      "Sample 828 - Lengths Match: True\n",
      "Sample 829 - Lengths Match: True\n",
      "Sample 830 - Lengths Match: True\n",
      "Sample 831 - Lengths Match: True\n",
      "Sample 832 - Lengths Match: True\n",
      "Sample 833 - Lengths Match: True\n",
      "Sample 834 - Lengths Match: True\n",
      "Sample 835 - Lengths Match: True\n",
      "Sample 836 - Lengths Match: True\n",
      "Sample 837 - Lengths Match: True\n",
      "Sample 838 - Lengths Match: True\n",
      "Sample 839 - Lengths Match: True\n",
      "Sample 840 - Lengths Match: True\n",
      "Sample 841 - Lengths Match: True\n",
      "Sample 842 - Lengths Match: True\n",
      "Sample 843 - Lengths Match: True\n",
      "Sample 844 - Lengths Match: True\n",
      "Sample 845 - Lengths Match: True\n",
      "Sample 846 - Lengths Match: True\n",
      "Sample 847 - Lengths Match: True\n",
      "Sample 848 - Lengths Match: True\n",
      "Sample 849 - Lengths Match: True\n",
      "Sample 850 - Lengths Match: True\n",
      "Sample 851 - Lengths Match: True\n",
      "Sample 852 - Lengths Match: True\n",
      "Sample 853 - Lengths Match: True\n",
      "Sample 854 - Lengths Match: True\n",
      "Sample 855 - Lengths Match: True\n",
      "Sample 856 - Lengths Match: True\n",
      "Sample 857 - Lengths Match: True\n",
      "Sample 858 - Lengths Match: True\n",
      "Sample 859 - Lengths Match: True\n",
      "Sample 860 - Lengths Match: True\n",
      "Sample 861 - Lengths Match: True\n",
      "Sample 862 - Lengths Match: True\n",
      "Sample 863 - Lengths Match: True\n",
      "Sample 864 - Lengths Match: True\n",
      "Sample 865 - Lengths Match: True\n",
      "Sample 866 - Lengths Match: True\n",
      "Sample 867 - Lengths Match: True\n",
      "Sample 868 - Lengths Match: True\n",
      "Sample 869 - Lengths Match: True\n",
      "Sample 870 - Lengths Match: True\n",
      "Sample 871 - Lengths Match: True\n",
      "Sample 872 - Lengths Match: True\n",
      "Sample 873 - Lengths Match: True\n",
      "Sample 874 - Lengths Match: True\n",
      "Sample 875 - Lengths Match: True\n",
      "Sample 876 - Lengths Match: True\n",
      "Sample 877 - Lengths Match: True\n",
      "Sample 878 - Lengths Match: True\n",
      "Sample 879 - Lengths Match: True\n",
      "Sample 880 - Lengths Match: True\n",
      "Sample 881 - Lengths Match: True\n",
      "Sample 882 - Lengths Match: True\n",
      "Sample 883 - Lengths Match: True\n",
      "Sample 884 - Lengths Match: True\n",
      "Sample 885 - Lengths Match: True\n",
      "Sample 886 - Lengths Match: True\n",
      "Sample 887 - Lengths Match: True\n",
      "Sample 888 - Lengths Match: True\n",
      "Sample 889 - Lengths Match: True\n",
      "Sample 890 - Lengths Match: True\n",
      "Sample 891 - Lengths Match: True\n",
      "Sample 892 - Lengths Match: True\n",
      "Sample 893 - Lengths Match: True\n",
      "Sample 894 - Lengths Match: True\n",
      "Sample 895 - Lengths Match: True\n",
      "Sample 896 - Lengths Match: True\n",
      "Sample 897 - Lengths Match: True\n",
      "Sample 898 - Lengths Match: True\n",
      "Sample 899 - Lengths Match: True\n",
      "Sample 900 - Lengths Match: True\n",
      "Sample 901 - Lengths Match: True\n",
      "Sample 902 - Lengths Match: True\n",
      "Sample 903 - Lengths Match: True\n",
      "Sample 904 - Lengths Match: True\n",
      "Sample 905 - Lengths Match: True\n",
      "Sample 906 - Lengths Match: True\n",
      "Sample 907 - Lengths Match: True\n",
      "Sample 908 - Lengths Match: True\n",
      "Sample 909 - Lengths Match: True\n",
      "Sample 910 - Lengths Match: True\n",
      "Sample 911 - Lengths Match: True\n",
      "Sample 912 - Lengths Match: True\n",
      "Sample 913 - Lengths Match: True\n",
      "Sample 914 - Lengths Match: True\n",
      "Sample 915 - Lengths Match: True\n",
      "Sample 916 - Lengths Match: True\n",
      "Sample 917 - Lengths Match: True\n",
      "Sample 918 - Lengths Match: True\n",
      "Sample 919 - Lengths Match: True\n",
      "Sample 920 - Lengths Match: True\n",
      "Sample 921 - Lengths Match: True\n",
      "Sample 922 - Lengths Match: True\n",
      "Sample 923 - Lengths Match: True\n",
      "Sample 924 - Lengths Match: True\n",
      "Sample 925 - Lengths Match: True\n",
      "Sample 926 - Lengths Match: True\n",
      "Sample 927 - Lengths Match: True\n",
      "Sample 928 - Lengths Match: True\n",
      "Sample 929 - Lengths Match: True\n",
      "Sample 930 - Lengths Match: True\n",
      "Sample 931 - Lengths Match: True\n",
      "Sample 932 - Lengths Match: True\n",
      "Sample 933 - Lengths Match: True\n",
      "Sample 934 - Lengths Match: True\n",
      "Sample 935 - Lengths Match: True\n",
      "Sample 936 - Lengths Match: True\n",
      "Sample 937 - Lengths Match: True\n",
      "Sample 938 - Lengths Match: True\n",
      "Sample 939 - Lengths Match: True\n",
      "Sample 940 - Lengths Match: True\n",
      "Sample 941 - Lengths Match: True\n",
      "Sample 942 - Lengths Match: True\n",
      "Sample 943 - Lengths Match: True\n",
      "Sample 944 - Lengths Match: True\n",
      "Sample 945 - Lengths Match: True\n",
      "Sample 946 - Lengths Match: True\n",
      "Sample 947 - Lengths Match: True\n",
      "Sample 948 - Lengths Match: True\n",
      "Sample 949 - Lengths Match: True\n",
      "Sample 950 - Lengths Match: True\n",
      "Sample 951 - Lengths Match: True\n",
      "Sample 952 - Lengths Match: True\n",
      "Sample 953 - Lengths Match: True\n",
      "Sample 954 - Lengths Match: True\n",
      "Sample 955 - Lengths Match: True\n",
      "Sample 956 - Lengths Match: True\n",
      "Sample 957 - Lengths Match: True\n",
      "Sample 958 - Lengths Match: True\n",
      "Sample 959 - Lengths Match: True\n",
      "Sample 960 - Lengths Match: True\n",
      "Sample 961 - Lengths Match: True\n",
      "Sample 962 - Lengths Match: True\n",
      "Sample 963 - Lengths Match: True\n",
      "Sample 964 - Lengths Match: True\n",
      "Sample 965 - Lengths Match: True\n",
      "Sample 966 - Lengths Match: True\n",
      "Sample 967 - Lengths Match: True\n",
      "Sample 968 - Lengths Match: True\n",
      "Sample 969 - Lengths Match: True\n",
      "Sample 970 - Lengths Match: True\n",
      "Sample 971 - Lengths Match: True\n",
      "Sample 972 - Lengths Match: True\n",
      "Sample 973 - Lengths Match: True\n",
      "Sample 974 - Lengths Match: True\n",
      "Sample 975 - Lengths Match: True\n",
      "Sample 976 - Lengths Match: True\n",
      "Sample 977 - Lengths Match: True\n",
      "Sample 978 - Lengths Match: True\n",
      "Sample 979 - Lengths Match: True\n",
      "Sample 980 - Lengths Match: True\n",
      "Sample 981 - Lengths Match: True\n",
      "Sample 982 - Lengths Match: True\n",
      "Sample 983 - Lengths Match: True\n",
      "Sample 984 - Lengths Match: True\n",
      "Sample 985 - Lengths Match: True\n",
      "Sample 986 - Lengths Match: True\n",
      "Sample 987 - Lengths Match: True\n",
      "Sample 988 - Lengths Match: True\n",
      "Sample 989 - Lengths Match: True\n",
      "Sample 990 - Lengths Match: True\n",
      "Sample 991 - Lengths Match: True\n",
      "Sample 992 - Lengths Match: True\n",
      "Sample 993 - Lengths Match: True\n",
      "Sample 994 - Lengths Match: True\n",
      "Sample 995 - Lengths Match: True\n",
      "Sample 996 - Lengths Match: True\n",
      "Sample 997 - Lengths Match: True\n",
      "Sample 998 - Lengths Match: True\n",
      "Sample 999 - Lengths Match: True\n",
      "Sample 1000 - Lengths Match: True\n",
      "Sample 1001 - Lengths Match: True\n",
      "Sample 1002 - Lengths Match: True\n",
      "Sample 1003 - Lengths Match: True\n",
      "Sample 1004 - Lengths Match: True\n",
      "Sample 1005 - Lengths Match: True\n",
      "Sample 1006 - Lengths Match: True\n",
      "Sample 1007 - Lengths Match: True\n",
      "Sample 1008 - Lengths Match: True\n",
      "Sample 1009 - Lengths Match: True\n",
      "Sample 1010 - Lengths Match: True\n",
      "Sample 1011 - Lengths Match: True\n",
      "Sample 1012 - Lengths Match: True\n",
      "Sample 1013 - Lengths Match: True\n",
      "Sample 1014 - Lengths Match: True\n",
      "Sample 1015 - Lengths Match: True\n",
      "Sample 1016 - Lengths Match: True\n",
      "Sample 1017 - Lengths Match: True\n",
      "Sample 1018 - Lengths Match: True\n",
      "Sample 1019 - Lengths Match: True\n",
      "Sample 1020 - Lengths Match: True\n",
      "Sample 1021 - Lengths Match: True\n",
      "Sample 1022 - Lengths Match: True\n",
      "Sample 1023 - Lengths Match: True\n",
      "Sample 1024 - Lengths Match: True\n",
      "Sample 1025 - Lengths Match: True\n",
      "Sample 1026 - Lengths Match: True\n",
      "Sample 1027 - Lengths Match: True\n",
      "Sample 1028 - Lengths Match: True\n",
      "Sample 1029 - Lengths Match: True\n",
      "Sample 1030 - Lengths Match: True\n",
      "Sample 1031 - Lengths Match: True\n",
      "Sample 1032 - Lengths Match: True\n",
      "Sample 1033 - Lengths Match: True\n",
      "Sample 1034 - Lengths Match: True\n",
      "Sample 1035 - Lengths Match: True\n",
      "Sample 1036 - Lengths Match: True\n",
      "Sample 1037 - Lengths Match: True\n",
      "Sample 1038 - Lengths Match: True\n",
      "Sample 1039 - Lengths Match: True\n",
      "Sample 1040 - Lengths Match: True\n",
      "Sample 1041 - Lengths Match: True\n",
      "Sample 1042 - Lengths Match: True\n",
      "Sample 1043 - Lengths Match: True\n",
      "Sample 1044 - Lengths Match: True\n",
      "Sample 1045 - Lengths Match: True\n",
      "Sample 1046 - Lengths Match: True\n",
      "Sample 1047 - Lengths Match: True\n",
      "Sample 1048 - Lengths Match: True\n",
      "Sample 1049 - Lengths Match: True\n",
      "Sample 1050 - Lengths Match: True\n",
      "Sample 1051 - Lengths Match: True\n",
      "Sample 1052 - Lengths Match: True\n",
      "Sample 1053 - Lengths Match: True\n",
      "Sample 1054 - Lengths Match: True\n",
      "Sample 1055 - Lengths Match: True\n",
      "Sample 1056 - Lengths Match: True\n",
      "Sample 1057 - Lengths Match: True\n",
      "Sample 1058 - Lengths Match: True\n",
      "Sample 1059 - Lengths Match: True\n",
      "Sample 1060 - Lengths Match: True\n",
      "Sample 1061 - Lengths Match: True\n",
      "Sample 1062 - Lengths Match: True\n",
      "Sample 1063 - Lengths Match: True\n",
      "Sample 1064 - Lengths Match: True\n",
      "Sample 1065 - Lengths Match: True\n",
      "Sample 1066 - Lengths Match: True\n",
      "Sample 1067 - Lengths Match: True\n",
      "Sample 1068 - Lengths Match: True\n",
      "Sample 1069 - Lengths Match: True\n",
      "Sample 1070 - Lengths Match: True\n",
      "Sample 1071 - Lengths Match: True\n",
      "Sample 1072 - Lengths Match: True\n",
      "Sample 1073 - Lengths Match: True\n",
      "Sample 1074 - Lengths Match: True\n",
      "Sample 1075 - Lengths Match: True\n",
      "Sample 1076 - Lengths Match: True\n",
      "Sample 1077 - Lengths Match: True\n",
      "Sample 1078 - Lengths Match: True\n",
      "Sample 1079 - Lengths Match: True\n",
      "Sample 1080 - Lengths Match: True\n",
      "Sample 1081 - Lengths Match: True\n",
      "Sample 1082 - Lengths Match: True\n",
      "Sample 1083 - Lengths Match: True\n",
      "Sample 1084 - Lengths Match: True\n",
      "Sample 1085 - Lengths Match: True\n",
      "Sample 1086 - Lengths Match: True\n",
      "Sample 1087 - Lengths Match: True\n",
      "Sample 1088 - Lengths Match: True\n",
      "Sample 1089 - Lengths Match: True\n",
      "Sample 1090 - Lengths Match: True\n",
      "Sample 1091 - Lengths Match: True\n",
      "Sample 1092 - Lengths Match: True\n",
      "Sample 1093 - Lengths Match: True\n",
      "Sample 1094 - Lengths Match: True\n",
      "Sample 1095 - Lengths Match: True\n",
      "Sample 1096 - Lengths Match: True\n",
      "Sample 1097 - Lengths Match: True\n",
      "Sample 1098 - Lengths Match: True\n",
      "Sample 1099 - Lengths Match: True\n",
      "Sample 1100 - Lengths Match: True\n",
      "Sample 1101 - Lengths Match: True\n",
      "Sample 1102 - Lengths Match: True\n",
      "Sample 1103 - Lengths Match: True\n",
      "Sample 1104 - Lengths Match: True\n",
      "Sample 1105 - Lengths Match: True\n",
      "Sample 1106 - Lengths Match: True\n",
      "Sample 1107 - Lengths Match: True\n",
      "Sample 1108 - Lengths Match: True\n",
      "Sample 1109 - Lengths Match: True\n",
      "Sample 1110 - Lengths Match: True\n",
      "Sample 1111 - Lengths Match: True\n",
      "Sample 1112 - Lengths Match: True\n",
      "Sample 1113 - Lengths Match: True\n",
      "Sample 1114 - Lengths Match: True\n",
      "Sample 1115 - Lengths Match: True\n",
      "Sample 1116 - Lengths Match: True\n",
      "Sample 1117 - Lengths Match: True\n",
      "Sample 1118 - Lengths Match: True\n",
      "Sample 1119 - Lengths Match: True\n",
      "Sample 1120 - Lengths Match: True\n",
      "Sample 1121 - Lengths Match: True\n",
      "Sample 1122 - Lengths Match: True\n",
      "Sample 1123 - Lengths Match: True\n",
      "Sample 1124 - Lengths Match: True\n",
      "Sample 1125 - Lengths Match: True\n",
      "Sample 1126 - Lengths Match: True\n",
      "Sample 1127 - Lengths Match: True\n",
      "Sample 1128 - Lengths Match: True\n",
      "Sample 1129 - Lengths Match: True\n",
      "Sample 1130 - Lengths Match: True\n",
      "Sample 1131 - Lengths Match: True\n",
      "Sample 1132 - Lengths Match: True\n",
      "Sample 1133 - Lengths Match: True\n",
      "Sample 1134 - Lengths Match: True\n",
      "Sample 1135 - Lengths Match: True\n",
      "Sample 1136 - Lengths Match: True\n",
      "Sample 1137 - Lengths Match: True\n",
      "Sample 1138 - Lengths Match: True\n",
      "Sample 1139 - Lengths Match: True\n",
      "Sample 1140 - Lengths Match: True\n",
      "Sample 1141 - Lengths Match: True\n",
      "Sample 1142 - Lengths Match: True\n",
      "Sample 1143 - Lengths Match: True\n",
      "Sample 1144 - Lengths Match: True\n",
      "Sample 1145 - Lengths Match: True\n",
      "Sample 1146 - Lengths Match: True\n",
      "Sample 1147 - Lengths Match: True\n",
      "Sample 1148 - Lengths Match: True\n",
      "Sample 1149 - Lengths Match: True\n",
      "Sample 1150 - Lengths Match: True\n",
      "Sample 1151 - Lengths Match: True\n",
      "Sample 1152 - Lengths Match: True\n",
      "Sample 1153 - Lengths Match: True\n",
      "Sample 1154 - Lengths Match: True\n",
      "Sample 1155 - Lengths Match: True\n",
      "Sample 1156 - Lengths Match: True\n",
      "Sample 1157 - Lengths Match: True\n",
      "Sample 1158 - Lengths Match: True\n",
      "Sample 1159 - Lengths Match: True\n",
      "Sample 1160 - Lengths Match: True\n",
      "Sample 1161 - Lengths Match: True\n",
      "Sample 1162 - Lengths Match: True\n",
      "Sample 1163 - Lengths Match: True\n",
      "Sample 1164 - Lengths Match: True\n",
      "Sample 1165 - Lengths Match: True\n",
      "Sample 1166 - Lengths Match: True\n",
      "Sample 1167 - Lengths Match: True\n",
      "Sample 1168 - Lengths Match: True\n",
      "Sample 1169 - Lengths Match: True\n",
      "Sample 1170 - Lengths Match: True\n",
      "Sample 1171 - Lengths Match: True\n",
      "Sample 1172 - Lengths Match: True\n",
      "Sample 1173 - Lengths Match: True\n",
      "Sample 1174 - Lengths Match: True\n",
      "Sample 1175 - Lengths Match: True\n",
      "Sample 1176 - Lengths Match: True\n",
      "Sample 1177 - Lengths Match: True\n",
      "Sample 1178 - Lengths Match: True\n",
      "Sample 1179 - Lengths Match: True\n",
      "Sample 1180 - Lengths Match: True\n",
      "Sample 1181 - Lengths Match: True\n",
      "Sample 1182 - Lengths Match: True\n",
      "Sample 1183 - Lengths Match: True\n",
      "Sample 1184 - Lengths Match: True\n",
      "Sample 1185 - Lengths Match: True\n",
      "Sample 1186 - Lengths Match: True\n",
      "Sample 1187 - Lengths Match: True\n",
      "Sample 1188 - Lengths Match: True\n",
      "Sample 1189 - Lengths Match: True\n",
      "Sample 1190 - Lengths Match: True\n",
      "Sample 1191 - Lengths Match: True\n",
      "Sample 1192 - Lengths Match: True\n",
      "Sample 1193 - Lengths Match: True\n",
      "Sample 1194 - Lengths Match: True\n",
      "Sample 1195 - Lengths Match: True\n",
      "Sample 1196 - Lengths Match: True\n",
      "Sample 1197 - Lengths Match: True\n",
      "Sample 1198 - Lengths Match: True\n",
      "Sample 1199 - Lengths Match: True\n",
      "Sample 1200 - Lengths Match: True\n",
      "Sample 1201 - Lengths Match: True\n",
      "Sample 1202 - Lengths Match: True\n",
      "Sample 1203 - Lengths Match: True\n",
      "Sample 1204 - Lengths Match: True\n",
      "Sample 1205 - Lengths Match: True\n",
      "Sample 1206 - Lengths Match: True\n",
      "Sample 1207 - Lengths Match: True\n",
      "Sample 1208 - Lengths Match: True\n",
      "Sample 1209 - Lengths Match: True\n",
      "Sample 1210 - Lengths Match: True\n",
      "Sample 1211 - Lengths Match: True\n",
      "Sample 1212 - Lengths Match: True\n",
      "Sample 1213 - Lengths Match: True\n",
      "Sample 1214 - Lengths Match: True\n",
      "Sample 1215 - Lengths Match: True\n",
      "Sample 1216 - Lengths Match: True\n",
      "Sample 1217 - Lengths Match: True\n",
      "Sample 1218 - Lengths Match: True\n",
      "Sample 1219 - Lengths Match: True\n",
      "Sample 1220 - Lengths Match: True\n",
      "Sample 1221 - Lengths Match: True\n",
      "Sample 1222 - Lengths Match: True\n",
      "Sample 1223 - Lengths Match: True\n",
      "Sample 1224 - Lengths Match: True\n",
      "Sample 1225 - Lengths Match: True\n",
      "Sample 1226 - Lengths Match: True\n",
      "Sample 1227 - Lengths Match: True\n",
      "Sample 1228 - Lengths Match: True\n",
      "Sample 1229 - Lengths Match: True\n",
      "Sample 1230 - Lengths Match: True\n",
      "Sample 1231 - Lengths Match: True\n",
      "Sample 1232 - Lengths Match: True\n",
      "Sample 1233 - Lengths Match: True\n",
      "Sample 1234 - Lengths Match: True\n",
      "Sample 1235 - Lengths Match: True\n",
      "Sample 1236 - Lengths Match: True\n",
      "Sample 1237 - Lengths Match: True\n",
      "Sample 1238 - Lengths Match: True\n",
      "Sample 1239 - Lengths Match: True\n",
      "Sample 1240 - Lengths Match: True\n",
      "Sample 1241 - Lengths Match: True\n",
      "Sample 1242 - Lengths Match: True\n",
      "Sample 1243 - Lengths Match: True\n",
      "Sample 1244 - Lengths Match: True\n",
      "Sample 1245 - Lengths Match: True\n",
      "Sample 1246 - Lengths Match: True\n",
      "Sample 1247 - Lengths Match: True\n",
      "Sample 1248 - Lengths Match: True\n",
      "Sample 1249 - Lengths Match: True\n",
      "Sample 1250 - Lengths Match: True\n",
      "Sample 1251 - Lengths Match: True\n",
      "Sample 1252 - Lengths Match: True\n",
      "Sample 1253 - Lengths Match: True\n",
      "Sample 1254 - Lengths Match: True\n",
      "Sample 1255 - Lengths Match: True\n",
      "Sample 1256 - Lengths Match: True\n",
      "Sample 1257 - Lengths Match: True\n",
      "Sample 1258 - Lengths Match: True\n",
      "Sample 1259 - Lengths Match: True\n",
      "Sample 1260 - Lengths Match: True\n",
      "Sample 1261 - Lengths Match: True\n",
      "Sample 1262 - Lengths Match: True\n",
      "Sample 1263 - Lengths Match: True\n",
      "Sample 1264 - Lengths Match: True\n",
      "Sample 1265 - Lengths Match: True\n",
      "Sample 1266 - Lengths Match: True\n",
      "Sample 1267 - Lengths Match: True\n",
      "Sample 1268 - Lengths Match: True\n",
      "Sample 1269 - Lengths Match: True\n",
      "Sample 1270 - Lengths Match: True\n",
      "Sample 1271 - Lengths Match: True\n",
      "Sample 1272 - Lengths Match: True\n",
      "Sample 1273 - Lengths Match: True\n",
      "Sample 1274 - Lengths Match: True\n",
      "Sample 1275 - Lengths Match: True\n",
      "Sample 1276 - Lengths Match: True\n",
      "Sample 1277 - Lengths Match: True\n",
      "Sample 1278 - Lengths Match: True\n",
      "Sample 1279 - Lengths Match: True\n",
      "Sample 1280 - Lengths Match: True\n",
      "Sample 1281 - Lengths Match: True\n",
      "Sample 1282 - Lengths Match: True\n",
      "Sample 1283 - Lengths Match: True\n",
      "Sample 1284 - Lengths Match: True\n",
      "Sample 1285 - Lengths Match: True\n",
      "Sample 1286 - Lengths Match: True\n",
      "Sample 1287 - Lengths Match: True\n",
      "Sample 1288 - Lengths Match: True\n",
      "Sample 1289 - Lengths Match: True\n",
      "Sample 1290 - Lengths Match: True\n",
      "Sample 1291 - Lengths Match: True\n",
      "Sample 1292 - Lengths Match: True\n",
      "Sample 1293 - Lengths Match: True\n",
      "Sample 1294 - Lengths Match: True\n",
      "Sample 1295 - Lengths Match: True\n",
      "Sample 1296 - Lengths Match: True\n",
      "Sample 1297 - Lengths Match: True\n",
      "Sample 1298 - Lengths Match: True\n",
      "Sample 1299 - Lengths Match: True\n",
      "Sample 1300 - Lengths Match: True\n",
      "Sample 1301 - Lengths Match: True\n",
      "Sample 1302 - Lengths Match: True\n",
      "Sample 1303 - Lengths Match: True\n",
      "Sample 1304 - Lengths Match: True\n",
      "Sample 1305 - Lengths Match: True\n",
      "Sample 1306 - Lengths Match: True\n",
      "Sample 1307 - Lengths Match: True\n",
      "Sample 1308 - Lengths Match: True\n",
      "Sample 1309 - Lengths Match: True\n",
      "Sample 1310 - Lengths Match: True\n",
      "Sample 1311 - Lengths Match: True\n",
      "Sample 1312 - Lengths Match: True\n",
      "Sample 1313 - Lengths Match: True\n",
      "Sample 1314 - Lengths Match: True\n",
      "Sample 1315 - Lengths Match: True\n",
      "Sample 1316 - Lengths Match: True\n",
      "Sample 1317 - Lengths Match: True\n",
      "Sample 1318 - Lengths Match: True\n",
      "Sample 1319 - Lengths Match: True\n",
      "Sample 1320 - Lengths Match: True\n",
      "Sample 1321 - Lengths Match: True\n",
      "Sample 1322 - Lengths Match: True\n",
      "Sample 1323 - Lengths Match: True\n",
      "Sample 1324 - Lengths Match: True\n",
      "Sample 1325 - Lengths Match: True\n",
      "Sample 1326 - Lengths Match: True\n",
      "Sample 1327 - Lengths Match: True\n",
      "Sample 1328 - Lengths Match: True\n",
      "Sample 1329 - Lengths Match: True\n",
      "Sample 1330 - Lengths Match: True\n",
      "Sample 1331 - Lengths Match: True\n",
      "Sample 1332 - Lengths Match: True\n",
      "Sample 1333 - Lengths Match: True\n",
      "Sample 1334 - Lengths Match: True\n",
      "Sample 1335 - Lengths Match: True\n",
      "Sample 1336 - Lengths Match: True\n",
      "Sample 1337 - Lengths Match: True\n",
      "Sample 1338 - Lengths Match: True\n",
      "Sample 1339 - Lengths Match: True\n",
      "Sample 1340 - Lengths Match: True\n",
      "Sample 1341 - Lengths Match: True\n",
      "Sample 1342 - Lengths Match: True\n",
      "Sample 1343 - Lengths Match: True\n",
      "Sample 1344 - Lengths Match: True\n",
      "Sample 1345 - Lengths Match: True\n",
      "Sample 1346 - Lengths Match: True\n",
      "Sample 1347 - Lengths Match: True\n",
      "Sample 1348 - Lengths Match: True\n",
      "Sample 1349 - Lengths Match: True\n",
      "Sample 1350 - Lengths Match: True\n",
      "Sample 1351 - Lengths Match: True\n",
      "Sample 1352 - Lengths Match: True\n",
      "Sample 1353 - Lengths Match: True\n",
      "Sample 1354 - Lengths Match: True\n",
      "Sample 1355 - Lengths Match: True\n",
      "Sample 1356 - Lengths Match: True\n",
      "Sample 1357 - Lengths Match: True\n",
      "Sample 1358 - Lengths Match: True\n",
      "Sample 1359 - Lengths Match: True\n",
      "Sample 1360 - Lengths Match: True\n",
      "Sample 1361 - Lengths Match: True\n",
      "Sample 1362 - Lengths Match: True\n",
      "Sample 1363 - Lengths Match: True\n",
      "Sample 1364 - Lengths Match: True\n",
      "Sample 1365 - Lengths Match: True\n",
      "Sample 1366 - Lengths Match: True\n",
      "Sample 1367 - Lengths Match: True\n",
      "Sample 1368 - Lengths Match: True\n",
      "Sample 1369 - Lengths Match: True\n",
      "Sample 1370 - Lengths Match: True\n",
      "Sample 1371 - Lengths Match: True\n",
      "Sample 1372 - Lengths Match: True\n",
      "Sample 1373 - Lengths Match: True\n",
      "Sample 1374 - Lengths Match: True\n",
      "Sample 1375 - Lengths Match: True\n",
      "Sample 1376 - Lengths Match: True\n",
      "Sample 1377 - Lengths Match: True\n",
      "Sample 1378 - Lengths Match: True\n",
      "Sample 1379 - Lengths Match: True\n",
      "Sample 1380 - Lengths Match: True\n",
      "Sample 1381 - Lengths Match: True\n",
      "Sample 1382 - Lengths Match: True\n",
      "Sample 1383 - Lengths Match: True\n",
      "Sample 1384 - Lengths Match: True\n",
      "Sample 1385 - Lengths Match: True\n",
      "Sample 1386 - Lengths Match: True\n",
      "Sample 1387 - Lengths Match: True\n",
      "Sample 1388 - Lengths Match: True\n",
      "Sample 1389 - Lengths Match: True\n",
      "Sample 1390 - Lengths Match: True\n",
      "Sample 1391 - Lengths Match: True\n",
      "Sample 1392 - Lengths Match: True\n",
      "Sample 1393 - Lengths Match: True\n",
      "Sample 1394 - Lengths Match: True\n",
      "Sample 1395 - Lengths Match: True\n",
      "Sample 1396 - Lengths Match: True\n",
      "Sample 1397 - Lengths Match: True\n",
      "Sample 1398 - Lengths Match: True\n",
      "Sample 1399 - Lengths Match: True\n",
      "Sample 1400 - Lengths Match: True\n",
      "Sample 1401 - Lengths Match: True\n",
      "Sample 1402 - Lengths Match: True\n",
      "Sample 1403 - Lengths Match: True\n",
      "Sample 1404 - Lengths Match: True\n",
      "Sample 1405 - Lengths Match: True\n",
      "Sample 1406 - Lengths Match: True\n",
      "Sample 1407 - Lengths Match: True\n",
      "Sample 1408 - Lengths Match: True\n",
      "Sample 1409 - Lengths Match: True\n",
      "Sample 1410 - Lengths Match: True\n",
      "Sample 1411 - Lengths Match: True\n",
      "Sample 1412 - Lengths Match: True\n",
      "Sample 1413 - Lengths Match: True\n",
      "Sample 1414 - Lengths Match: True\n",
      "Sample 1415 - Lengths Match: True\n",
      "Sample 1416 - Lengths Match: True\n",
      "Sample 1417 - Lengths Match: True\n",
      "Sample 1418 - Lengths Match: True\n",
      "Sample 1419 - Lengths Match: True\n",
      "Sample 1420 - Lengths Match: True\n",
      "Sample 1421 - Lengths Match: True\n",
      "Sample 1422 - Lengths Match: True\n",
      "Sample 1423 - Lengths Match: True\n",
      "Sample 1424 - Lengths Match: True\n",
      "Sample 1425 - Lengths Match: True\n",
      "Sample 1426 - Lengths Match: True\n",
      "Sample 1427 - Lengths Match: True\n",
      "Sample 1428 - Lengths Match: True\n",
      "Sample 1429 - Lengths Match: True\n",
      "Sample 1430 - Lengths Match: True\n",
      "Sample 1431 - Lengths Match: True\n",
      "Sample 1432 - Lengths Match: True\n",
      "Sample 1433 - Lengths Match: True\n",
      "Sample 1434 - Lengths Match: True\n",
      "Sample 1435 - Lengths Match: True\n",
      "Sample 1436 - Lengths Match: True\n",
      "Sample 1437 - Lengths Match: True\n",
      "Sample 1438 - Lengths Match: True\n",
      "Sample 1439 - Lengths Match: True\n",
      "Sample 1440 - Lengths Match: True\n",
      "Sample 1441 - Lengths Match: True\n",
      "Sample 1442 - Lengths Match: True\n",
      "Sample 1443 - Lengths Match: True\n",
      "Sample 1444 - Lengths Match: True\n",
      "Sample 1445 - Lengths Match: True\n",
      "Sample 1446 - Lengths Match: True\n",
      "Sample 1447 - Lengths Match: True\n",
      "Sample 1448 - Lengths Match: True\n",
      "Sample 1449 - Lengths Match: True\n",
      "Sample 1450 - Lengths Match: True\n",
      "Sample 1451 - Lengths Match: True\n",
      "Sample 1452 - Lengths Match: True\n",
      "Sample 1453 - Lengths Match: True\n",
      "Sample 1454 - Lengths Match: True\n",
      "Sample 1455 - Lengths Match: True\n",
      "Sample 1456 - Lengths Match: True\n",
      "Sample 1457 - Lengths Match: True\n",
      "Sample 1458 - Lengths Match: True\n",
      "Sample 1459 - Lengths Match: True\n",
      "Sample 1460 - Lengths Match: True\n",
      "Sample 1461 - Lengths Match: True\n",
      "Sample 1462 - Lengths Match: True\n",
      "Sample 1463 - Lengths Match: True\n",
      "Sample 1464 - Lengths Match: True\n",
      "Sample 1465 - Lengths Match: True\n",
      "Sample 1466 - Lengths Match: True\n",
      "Sample 1467 - Lengths Match: True\n",
      "Sample 1468 - Lengths Match: True\n",
      "Sample 1469 - Lengths Match: True\n",
      "Sample 1470 - Lengths Match: True\n",
      "Sample 1471 - Lengths Match: True\n",
      "Sample 1472 - Lengths Match: True\n",
      "Sample 1473 - Lengths Match: True\n",
      "Sample 1474 - Lengths Match: True\n",
      "Sample 1475 - Lengths Match: True\n",
      "Sample 1476 - Lengths Match: True\n",
      "Sample 1477 - Lengths Match: True\n",
      "Sample 1478 - Lengths Match: True\n",
      "Sample 1479 - Lengths Match: True\n",
      "Sample 1480 - Lengths Match: True\n",
      "Sample 1481 - Lengths Match: True\n",
      "Sample 1482 - Lengths Match: True\n",
      "Sample 1483 - Lengths Match: True\n",
      "Sample 1484 - Lengths Match: True\n",
      "Sample 1485 - Lengths Match: True\n",
      "Sample 1486 - Lengths Match: True\n",
      "Sample 1487 - Lengths Match: True\n",
      "Sample 1488 - Lengths Match: True\n",
      "Sample 1489 - Lengths Match: True\n",
      "Sample 1490 - Lengths Match: True\n",
      "Sample 1491 - Lengths Match: True\n",
      "Sample 1492 - Lengths Match: True\n",
      "Sample 1493 - Lengths Match: True\n",
      "Sample 1494 - Lengths Match: True\n",
      "Sample 1495 - Lengths Match: True\n",
      "Sample 1496 - Lengths Match: True\n",
      "Sample 1497 - Lengths Match: True\n",
      "Sample 1498 - Lengths Match: True\n",
      "Sample 1499 - Lengths Match: True\n",
      "Sample 1500 - Lengths Match: True\n",
      "Sample 1501 - Lengths Match: True\n",
      "Sample 1502 - Lengths Match: True\n",
      "Sample 1503 - Lengths Match: True\n",
      "Sample 1504 - Lengths Match: True\n",
      "Sample 1505 - Lengths Match: True\n",
      "Sample 1506 - Lengths Match: True\n",
      "Sample 1507 - Lengths Match: True\n",
      "Sample 1508 - Lengths Match: True\n",
      "Sample 1509 - Lengths Match: True\n",
      "Sample 1510 - Lengths Match: True\n",
      "Sample 1511 - Lengths Match: True\n",
      "Sample 1512 - Lengths Match: True\n",
      "Sample 1513 - Lengths Match: True\n",
      "Sample 1514 - Lengths Match: True\n",
      "Sample 1515 - Lengths Match: True\n",
      "Sample 1516 - Lengths Match: True\n",
      "Sample 1517 - Lengths Match: True\n",
      "Sample 1518 - Lengths Match: True\n",
      "Sample 1519 - Lengths Match: True\n",
      "Sample 1520 - Lengths Match: True\n",
      "Sample 1521 - Lengths Match: True\n",
      "Sample 1522 - Lengths Match: True\n",
      "Sample 1523 - Lengths Match: True\n",
      "Sample 1524 - Lengths Match: True\n",
      "Sample 1525 - Lengths Match: True\n",
      "Sample 1526 - Lengths Match: True\n",
      "Sample 1527 - Lengths Match: True\n",
      "Sample 1528 - Lengths Match: True\n",
      "Sample 1529 - Lengths Match: True\n",
      "Sample 1530 - Lengths Match: True\n",
      "Sample 1531 - Lengths Match: True\n",
      "Sample 1532 - Lengths Match: True\n",
      "Sample 1533 - Lengths Match: True\n",
      "Sample 1534 - Lengths Match: True\n",
      "Sample 1535 - Lengths Match: True\n",
      "Sample 1536 - Lengths Match: True\n",
      "Sample 1537 - Lengths Match: True\n",
      "Sample 1538 - Lengths Match: True\n",
      "Sample 1539 - Lengths Match: True\n",
      "Sample 1540 - Lengths Match: True\n",
      "Sample 1541 - Lengths Match: True\n",
      "Sample 1542 - Lengths Match: True\n",
      "Sample 1543 - Lengths Match: True\n",
      "Sample 1544 - Lengths Match: True\n",
      "Sample 1545 - Lengths Match: True\n",
      "Sample 1546 - Lengths Match: True\n",
      "Sample 1547 - Lengths Match: True\n",
      "Sample 1548 - Lengths Match: True\n",
      "Sample 1549 - Lengths Match: True\n",
      "Sample 1550 - Lengths Match: True\n",
      "Sample 1551 - Lengths Match: True\n",
      "Sample 1552 - Lengths Match: True\n",
      "Sample 1553 - Lengths Match: True\n",
      "Sample 1554 - Lengths Match: True\n",
      "Sample 1555 - Lengths Match: True\n",
      "Sample 1556 - Lengths Match: True\n",
      "Sample 1557 - Lengths Match: True\n",
      "Sample 1558 - Lengths Match: True\n",
      "Sample 1559 - Lengths Match: True\n",
      "Sample 1560 - Lengths Match: True\n",
      "Sample 1561 - Lengths Match: True\n",
      "Sample 1562 - Lengths Match: True\n",
      "Sample 1563 - Lengths Match: True\n",
      "Sample 1564 - Lengths Match: True\n",
      "Sample 1565 - Lengths Match: True\n",
      "Sample 1566 - Lengths Match: True\n",
      "Sample 1567 - Lengths Match: True\n",
      "Sample 1568 - Lengths Match: True\n",
      "Sample 1569 - Lengths Match: True\n",
      "Sample 1570 - Lengths Match: True\n",
      "Sample 1571 - Lengths Match: True\n",
      "Sample 1572 - Lengths Match: True\n",
      "Sample 1573 - Lengths Match: True\n",
      "Sample 1574 - Lengths Match: True\n",
      "Sample 1575 - Lengths Match: True\n",
      "Sample 1576 - Lengths Match: True\n",
      "Sample 1577 - Lengths Match: True\n",
      "Sample 1578 - Lengths Match: True\n",
      "Sample 1579 - Lengths Match: True\n",
      "Sample 1580 - Lengths Match: True\n",
      "Sample 1581 - Lengths Match: True\n",
      "Sample 1582 - Lengths Match: True\n",
      "Sample 1583 - Lengths Match: True\n",
      "Sample 1584 - Lengths Match: True\n",
      "Sample 1585 - Lengths Match: True\n",
      "Sample 1586 - Lengths Match: True\n",
      "Sample 1587 - Lengths Match: True\n",
      "Sample 1588 - Lengths Match: True\n",
      "Sample 1589 - Lengths Match: True\n",
      "Sample 1590 - Lengths Match: True\n",
      "Sample 1591 - Lengths Match: True\n",
      "Sample 1592 - Lengths Match: True\n",
      "Sample 1593 - Lengths Match: True\n",
      "Sample 1594 - Lengths Match: True\n",
      "Sample 1595 - Lengths Match: True\n",
      "Sample 1596 - Lengths Match: True\n",
      "Sample 1597 - Lengths Match: True\n",
      "Sample 1598 - Lengths Match: True\n",
      "Sample 1599 - Lengths Match: True\n",
      "Sample 1600 - Lengths Match: True\n",
      "Sample 1601 - Lengths Match: True\n",
      "Sample 1602 - Lengths Match: True\n",
      "Sample 1603 - Lengths Match: True\n",
      "Sample 1604 - Lengths Match: True\n",
      "Sample 1605 - Lengths Match: True\n",
      "Sample 1606 - Lengths Match: True\n",
      "Sample 1607 - Lengths Match: True\n",
      "Sample 1608 - Lengths Match: True\n",
      "Sample 1609 - Lengths Match: True\n",
      "Sample 1610 - Lengths Match: True\n",
      "Sample 1611 - Lengths Match: True\n",
      "Sample 1612 - Lengths Match: True\n",
      "Sample 1613 - Lengths Match: True\n",
      "Sample 1614 - Lengths Match: True\n",
      "Sample 1615 - Lengths Match: True\n",
      "Sample 1616 - Lengths Match: True\n",
      "Sample 1617 - Lengths Match: True\n",
      "Sample 1618 - Lengths Match: True\n",
      "Sample 1619 - Lengths Match: True\n",
      "Sample 1620 - Lengths Match: True\n",
      "Sample 1621 - Lengths Match: True\n",
      "Sample 1622 - Lengths Match: True\n",
      "Sample 1623 - Lengths Match: True\n",
      "Sample 1624 - Lengths Match: True\n",
      "Sample 1625 - Lengths Match: True\n",
      "Sample 1626 - Lengths Match: True\n",
      "Sample 1627 - Lengths Match: True\n",
      "Sample 1628 - Lengths Match: True\n",
      "Sample 1629 - Lengths Match: True\n",
      "Sample 1630 - Lengths Match: True\n",
      "Sample 1631 - Lengths Match: True\n",
      "Sample 1632 - Lengths Match: True\n",
      "Sample 1633 - Lengths Match: True\n",
      "Sample 1634 - Lengths Match: True\n",
      "Sample 1635 - Lengths Match: True\n",
      "Sample 1636 - Lengths Match: True\n",
      "Sample 1637 - Lengths Match: True\n",
      "Sample 1638 - Lengths Match: True\n",
      "Sample 1639 - Lengths Match: True\n",
      "Sample 1640 - Lengths Match: True\n",
      "Sample 1641 - Lengths Match: True\n",
      "Sample 1642 - Lengths Match: True\n",
      "Sample 1643 - Lengths Match: True\n",
      "Sample 1644 - Lengths Match: True\n",
      "Sample 1645 - Lengths Match: True\n",
      "Sample 1646 - Lengths Match: True\n",
      "Sample 1647 - Lengths Match: True\n",
      "Sample 1648 - Lengths Match: True\n",
      "Sample 1649 - Lengths Match: True\n",
      "Sample 1650 - Lengths Match: True\n",
      "Sample 1651 - Lengths Match: True\n",
      "Sample 1652 - Lengths Match: True\n",
      "Sample 1653 - Lengths Match: True\n",
      "Sample 1654 - Lengths Match: True\n",
      "Sample 1655 - Lengths Match: True\n",
      "Sample 1656 - Lengths Match: True\n",
      "Sample 1657 - Lengths Match: True\n",
      "Sample 1658 - Lengths Match: True\n",
      "Sample 1659 - Lengths Match: True\n",
      "Sample 1660 - Lengths Match: True\n",
      "Sample 1661 - Lengths Match: True\n",
      "Sample 1662 - Lengths Match: True\n",
      "Sample 1663 - Lengths Match: True\n",
      "Sample 1664 - Lengths Match: True\n",
      "Sample 1665 - Lengths Match: True\n",
      "Sample 1666 - Lengths Match: True\n",
      "Sample 1667 - Lengths Match: True\n",
      "Sample 1668 - Lengths Match: True\n",
      "Sample 1669 - Lengths Match: True\n",
      "Sample 1670 - Lengths Match: True\n",
      "Sample 1671 - Lengths Match: True\n",
      "Sample 1672 - Lengths Match: True\n",
      "Sample 1673 - Lengths Match: True\n",
      "Sample 1674 - Lengths Match: True\n",
      "Sample 1675 - Lengths Match: True\n",
      "Sample 1676 - Lengths Match: True\n",
      "Sample 1677 - Lengths Match: True\n",
      "Sample 1678 - Lengths Match: True\n",
      "Sample 1679 - Lengths Match: True\n",
      "Sample 1680 - Lengths Match: True\n",
      "Sample 1681 - Lengths Match: True\n",
      "Sample 1682 - Lengths Match: True\n",
      "Sample 1683 - Lengths Match: True\n",
      "Sample 1684 - Lengths Match: True\n",
      "Sample 1685 - Lengths Match: True\n",
      "Sample 1686 - Lengths Match: True\n",
      "Sample 1687 - Lengths Match: True\n",
      "Sample 1688 - Lengths Match: True\n",
      "Sample 1689 - Lengths Match: True\n",
      "Sample 1690 - Lengths Match: True\n",
      "Sample 1691 - Lengths Match: True\n",
      "Sample 1692 - Lengths Match: True\n",
      "Sample 1693 - Lengths Match: True\n",
      "Sample 1694 - Lengths Match: True\n",
      "Sample 1695 - Lengths Match: True\n",
      "Sample 1696 - Lengths Match: True\n",
      "Sample 1697 - Lengths Match: True\n",
      "Sample 1698 - Lengths Match: True\n",
      "Sample 1699 - Lengths Match: True\n",
      "Sample 1700 - Lengths Match: True\n",
      "Sample 1701 - Lengths Match: True\n",
      "Sample 1702 - Lengths Match: True\n",
      "Sample 1703 - Lengths Match: True\n",
      "Sample 1704 - Lengths Match: True\n",
      "Sample 1705 - Lengths Match: True\n",
      "Sample 1706 - Lengths Match: True\n",
      "Sample 1707 - Lengths Match: True\n",
      "Sample 1708 - Lengths Match: True\n",
      "Sample 1709 - Lengths Match: True\n",
      "Sample 1710 - Lengths Match: True\n",
      "Sample 1711 - Lengths Match: True\n",
      "Sample 1712 - Lengths Match: True\n",
      "Sample 1713 - Lengths Match: True\n",
      "Sample 1714 - Lengths Match: True\n",
      "Sample 1715 - Lengths Match: True\n",
      "Sample 1716 - Lengths Match: True\n",
      "Sample 1717 - Lengths Match: True\n",
      "Sample 1718 - Lengths Match: True\n",
      "Sample 1719 - Lengths Match: True\n",
      "Sample 1720 - Lengths Match: True\n",
      "Sample 1721 - Lengths Match: True\n",
      "Sample 1722 - Lengths Match: True\n",
      "Sample 1723 - Lengths Match: True\n",
      "Sample 1724 - Lengths Match: True\n",
      "Sample 1725 - Lengths Match: True\n",
      "Sample 1726 - Lengths Match: True\n",
      "Sample 1727 - Lengths Match: True\n",
      "Sample 1728 - Lengths Match: True\n",
      "Sample 1729 - Lengths Match: True\n",
      "Sample 1730 - Lengths Match: True\n",
      "Sample 1731 - Lengths Match: True\n",
      "Sample 1732 - Lengths Match: True\n",
      "Sample 1733 - Lengths Match: True\n",
      "Sample 1734 - Lengths Match: True\n",
      "Sample 1735 - Lengths Match: True\n",
      "Sample 1736 - Lengths Match: True\n",
      "Sample 1737 - Lengths Match: True\n",
      "Sample 1738 - Lengths Match: True\n",
      "Sample 1739 - Lengths Match: True\n",
      "Sample 1740 - Lengths Match: True\n",
      "Sample 1741 - Lengths Match: True\n",
      "Sample 1742 - Lengths Match: True\n",
      "Sample 1743 - Lengths Match: True\n",
      "Sample 1744 - Lengths Match: True\n",
      "Sample 1745 - Lengths Match: True\n",
      "Sample 1746 - Lengths Match: True\n",
      "Sample 1747 - Lengths Match: True\n",
      "Sample 1748 - Lengths Match: True\n",
      "Sample 1749 - Lengths Match: True\n",
      "Sample 1750 - Lengths Match: True\n",
      "Sample 1751 - Lengths Match: True\n",
      "Sample 1752 - Lengths Match: True\n",
      "Sample 1753 - Lengths Match: True\n",
      "Sample 1754 - Lengths Match: True\n",
      "Sample 1755 - Lengths Match: True\n",
      "Sample 1756 - Lengths Match: True\n",
      "Sample 1757 - Lengths Match: True\n",
      "Sample 1758 - Lengths Match: True\n",
      "Sample 1759 - Lengths Match: True\n",
      "Sample 1760 - Lengths Match: True\n",
      "Sample 1761 - Lengths Match: True\n",
      "Sample 1762 - Lengths Match: True\n",
      "Sample 1763 - Lengths Match: True\n",
      "Sample 1764 - Lengths Match: True\n",
      "Sample 1765 - Lengths Match: True\n",
      "Sample 1766 - Lengths Match: True\n",
      "Sample 1767 - Lengths Match: True\n",
      "Sample 1768 - Lengths Match: True\n",
      "Sample 1769 - Lengths Match: True\n",
      "Sample 1770 - Lengths Match: True\n",
      "Sample 1771 - Lengths Match: True\n",
      "Sample 1772 - Lengths Match: True\n",
      "Sample 1773 - Lengths Match: True\n",
      "Sample 1774 - Lengths Match: True\n",
      "Sample 1775 - Lengths Match: True\n",
      "Sample 1776 - Lengths Match: True\n",
      "Sample 1777 - Lengths Match: True\n",
      "Sample 1778 - Lengths Match: True\n",
      "Sample 1779 - Lengths Match: True\n",
      "Sample 1780 - Lengths Match: True\n",
      "Sample 1781 - Lengths Match: True\n",
      "Sample 1782 - Lengths Match: True\n",
      "Sample 1783 - Lengths Match: True\n",
      "Sample 1784 - Lengths Match: True\n",
      "Sample 1785 - Lengths Match: True\n",
      "Sample 1786 - Lengths Match: True\n",
      "Sample 1787 - Lengths Match: True\n",
      "Sample 1788 - Lengths Match: True\n",
      "Sample 1789 - Lengths Match: True\n",
      "Sample 1790 - Lengths Match: True\n",
      "Sample 1791 - Lengths Match: True\n",
      "Sample 1792 - Lengths Match: True\n",
      "Sample 1793 - Lengths Match: True\n",
      "Sample 1794 - Lengths Match: True\n",
      "Sample 1795 - Lengths Match: True\n",
      "Sample 1796 - Lengths Match: True\n",
      "Sample 1797 - Lengths Match: True\n",
      "Sample 1798 - Lengths Match: True\n",
      "Sample 1799 - Lengths Match: True\n",
      "Sample 1800 - Lengths Match: True\n",
      "Sample 1801 - Lengths Match: True\n",
      "Sample 1802 - Lengths Match: True\n",
      "Sample 1803 - Lengths Match: True\n",
      "Sample 1804 - Lengths Match: True\n",
      "Sample 1805 - Lengths Match: True\n",
      "Sample 1806 - Lengths Match: True\n",
      "Sample 1807 - Lengths Match: True\n",
      "Sample 1808 - Lengths Match: True\n",
      "Sample 1809 - Lengths Match: True\n",
      "Sample 1810 - Lengths Match: True\n",
      "Sample 1811 - Lengths Match: True\n",
      "Sample 1812 - Lengths Match: True\n",
      "Sample 1813 - Lengths Match: True\n",
      "Sample 1814 - Lengths Match: True\n",
      "Sample 1815 - Lengths Match: True\n",
      "Sample 1816 - Lengths Match: True\n",
      "Sample 1817 - Lengths Match: True\n",
      "Sample 1818 - Lengths Match: True\n",
      "Sample 1819 - Lengths Match: True\n",
      "Sample 1820 - Lengths Match: True\n",
      "Sample 1821 - Lengths Match: True\n",
      "Sample 1822 - Lengths Match: True\n",
      "Sample 1823 - Lengths Match: True\n",
      "Sample 1824 - Lengths Match: True\n",
      "Sample 1825 - Lengths Match: True\n",
      "Sample 1826 - Lengths Match: True\n",
      "Sample 1827 - Lengths Match: True\n",
      "Sample 1828 - Lengths Match: True\n",
      "Sample 1829 - Lengths Match: True\n",
      "Sample 1830 - Lengths Match: True\n",
      "Sample 1831 - Lengths Match: True\n",
      "Sample 1832 - Lengths Match: True\n",
      "Sample 1833 - Lengths Match: True\n",
      "Sample 1834 - Lengths Match: True\n",
      "Sample 1835 - Lengths Match: True\n",
      "Sample 1836 - Lengths Match: True\n",
      "Sample 1837 - Lengths Match: True\n",
      "Sample 1838 - Lengths Match: True\n",
      "Sample 1839 - Lengths Match: True\n",
      "Sample 1840 - Lengths Match: True\n",
      "Sample 1841 - Lengths Match: True\n",
      "Sample 1842 - Lengths Match: True\n",
      "Sample 1843 - Lengths Match: True\n",
      "Sample 1844 - Lengths Match: True\n",
      "Sample 1845 - Lengths Match: True\n",
      "Sample 1846 - Lengths Match: True\n",
      "Sample 1847 - Lengths Match: True\n",
      "Sample 1848 - Lengths Match: True\n",
      "Sample 1849 - Lengths Match: True\n",
      "Sample 1850 - Lengths Match: True\n",
      "Sample 1851 - Lengths Match: True\n",
      "Sample 1852 - Lengths Match: True\n",
      "Sample 1853 - Lengths Match: True\n",
      "Sample 1854 - Lengths Match: True\n",
      "Sample 1855 - Lengths Match: True\n",
      "Sample 1856 - Lengths Match: True\n",
      "Sample 1857 - Lengths Match: True\n",
      "Sample 1858 - Lengths Match: True\n",
      "Sample 1859 - Lengths Match: True\n",
      "Sample 1860 - Lengths Match: True\n",
      "Sample 1861 - Lengths Match: True\n",
      "Sample 1862 - Lengths Match: True\n",
      "Sample 1863 - Lengths Match: True\n",
      "Sample 1864 - Lengths Match: True\n",
      "Sample 1865 - Lengths Match: True\n",
      "Sample 1866 - Lengths Match: True\n",
      "Sample 1867 - Lengths Match: True\n",
      "Sample 1868 - Lengths Match: True\n",
      "Sample 1869 - Lengths Match: True\n",
      "Sample 1870 - Lengths Match: True\n",
      "Sample 1871 - Lengths Match: True\n",
      "Sample 1872 - Lengths Match: True\n",
      "Sample 1873 - Lengths Match: True\n",
      "Sample 1874 - Lengths Match: True\n",
      "Sample 1875 - Lengths Match: True\n",
      "Sample 1876 - Lengths Match: True\n",
      "Sample 1877 - Lengths Match: True\n",
      "Sample 1878 - Lengths Match: True\n",
      "Sample 1879 - Lengths Match: True\n",
      "Sample 1880 - Lengths Match: True\n",
      "Sample 1881 - Lengths Match: True\n",
      "Sample 1882 - Lengths Match: True\n",
      "Sample 1883 - Lengths Match: True\n",
      "Sample 1884 - Lengths Match: True\n",
      "Sample 1885 - Lengths Match: True\n",
      "Sample 1886 - Lengths Match: True\n",
      "Sample 1887 - Lengths Match: True\n",
      "Sample 1888 - Lengths Match: True\n",
      "Sample 1889 - Lengths Match: True\n",
      "Sample 1890 - Lengths Match: True\n",
      "Sample 1891 - Lengths Match: True\n",
      "Sample 1892 - Lengths Match: True\n",
      "Sample 1893 - Lengths Match: True\n",
      "Sample 1894 - Lengths Match: True\n",
      "Sample 1895 - Lengths Match: True\n",
      "Sample 1896 - Lengths Match: True\n",
      "Sample 1897 - Lengths Match: True\n",
      "Sample 1898 - Lengths Match: True\n",
      "Sample 1899 - Lengths Match: True\n",
      "Sample 1900 - Lengths Match: True\n",
      "Sample 1901 - Lengths Match: True\n",
      "Sample 1902 - Lengths Match: True\n",
      "Sample 1903 - Lengths Match: True\n",
      "Sample 1904 - Lengths Match: True\n",
      "Sample 1905 - Lengths Match: True\n",
      "Sample 1906 - Lengths Match: True\n",
      "Sample 1907 - Lengths Match: True\n",
      "Sample 1908 - Lengths Match: True\n",
      "Sample 1909 - Lengths Match: True\n",
      "Sample 1910 - Lengths Match: True\n",
      "Sample 1911 - Lengths Match: True\n",
      "Sample 1912 - Lengths Match: True\n",
      "Sample 1913 - Lengths Match: True\n",
      "Sample 1914 - Lengths Match: True\n",
      "Sample 1915 - Lengths Match: True\n",
      "Sample 1916 - Lengths Match: True\n",
      "Sample 1917 - Lengths Match: True\n",
      "Sample 1918 - Lengths Match: True\n",
      "Sample 1919 - Lengths Match: True\n",
      "Sample 1920 - Lengths Match: True\n",
      "Sample 1921 - Lengths Match: True\n",
      "Sample 1922 - Lengths Match: True\n",
      "Sample 1923 - Lengths Match: True\n",
      "Sample 1924 - Lengths Match: True\n",
      "Sample 1925 - Lengths Match: True\n",
      "Sample 1926 - Lengths Match: True\n",
      "Sample 1927 - Lengths Match: True\n",
      "Sample 1928 - Lengths Match: True\n",
      "Sample 1929 - Lengths Match: True\n",
      "Sample 1930 - Lengths Match: True\n",
      "Sample 1931 - Lengths Match: True\n",
      "Sample 1932 - Lengths Match: True\n",
      "Sample 1933 - Lengths Match: True\n",
      "Sample 1934 - Lengths Match: True\n",
      "Sample 1935 - Lengths Match: True\n",
      "Sample 1936 - Lengths Match: True\n",
      "Sample 1937 - Lengths Match: True\n",
      "Sample 1938 - Lengths Match: True\n",
      "Sample 1939 - Lengths Match: True\n",
      "Sample 1940 - Lengths Match: True\n",
      "Sample 1941 - Lengths Match: True\n",
      "Sample 1942 - Lengths Match: True\n",
      "Sample 1943 - Lengths Match: True\n",
      "Sample 1944 - Lengths Match: True\n",
      "Sample 1945 - Lengths Match: True\n",
      "Sample 1946 - Lengths Match: True\n",
      "Sample 1947 - Lengths Match: True\n",
      "Sample 1948 - Lengths Match: True\n",
      "Sample 1949 - Lengths Match: True\n",
      "Sample 1950 - Lengths Match: True\n",
      "Sample 1951 - Lengths Match: True\n",
      "Sample 1952 - Lengths Match: True\n",
      "Sample 1953 - Lengths Match: True\n",
      "Sample 1954 - Lengths Match: True\n",
      "Sample 1955 - Lengths Match: True\n",
      "Sample 1956 - Lengths Match: True\n",
      "Sample 1957 - Lengths Match: True\n",
      "Sample 1958 - Lengths Match: True\n",
      "Sample 1959 - Lengths Match: True\n",
      "Sample 1960 - Lengths Match: True\n",
      "Sample 1961 - Lengths Match: True\n",
      "Sample 1962 - Lengths Match: True\n",
      "Sample 1963 - Lengths Match: True\n",
      "Sample 1964 - Lengths Match: True\n",
      "Sample 1965 - Lengths Match: True\n",
      "Sample 1966 - Lengths Match: True\n",
      "Sample 1967 - Lengths Match: True\n",
      "Sample 1968 - Lengths Match: True\n",
      "Sample 1969 - Lengths Match: True\n",
      "Sample 1970 - Lengths Match: True\n",
      "Sample 1971 - Lengths Match: True\n",
      "Sample 1972 - Lengths Match: True\n",
      "Sample 1973 - Lengths Match: True\n",
      "Sample 1974 - Lengths Match: True\n",
      "Sample 1975 - Lengths Match: True\n",
      "Sample 1976 - Lengths Match: True\n",
      "Sample 1977 - Lengths Match: True\n",
      "Sample 1978 - Lengths Match: True\n",
      "Sample 1979 - Lengths Match: True\n",
      "Sample 1980 - Lengths Match: True\n",
      "Sample 1981 - Lengths Match: True\n",
      "Sample 1982 - Lengths Match: True\n",
      "Sample 1983 - Lengths Match: True\n",
      "Sample 1984 - Lengths Match: True\n",
      "Sample 1985 - Lengths Match: True\n",
      "Sample 1986 - Lengths Match: True\n",
      "Sample 1987 - Lengths Match: True\n",
      "Sample 1988 - Lengths Match: True\n",
      "Sample 1989 - Lengths Match: True\n",
      "Sample 1990 - Lengths Match: True\n",
      "Sample 1991 - Lengths Match: True\n",
      "Sample 1992 - Lengths Match: True\n",
      "Sample 1993 - Lengths Match: True\n",
      "Sample 1994 - Lengths Match: True\n",
      "Sample 1995 - Lengths Match: True\n",
      "Sample 1996 - Lengths Match: True\n",
      "Sample 1997 - Lengths Match: True\n",
      "Sample 1998 - Lengths Match: True\n",
      "Sample 1999 - Lengths Match: True\n",
      "Sample 2000 - Lengths Match: True\n",
      "Sample 2001 - Lengths Match: True\n",
      "Sample 2002 - Lengths Match: True\n",
      "Sample 2003 - Lengths Match: True\n",
      "Sample 2004 - Lengths Match: True\n",
      "Sample 2005 - Lengths Match: True\n",
      "Sample 2006 - Lengths Match: True\n",
      "Sample 2007 - Lengths Match: True\n",
      "Sample 2008 - Lengths Match: True\n",
      "Sample 2009 - Lengths Match: True\n",
      "Sample 2010 - Lengths Match: True\n",
      "Sample 2011 - Lengths Match: True\n",
      "Sample 2012 - Lengths Match: True\n",
      "Sample 2013 - Lengths Match: True\n",
      "Sample 2014 - Lengths Match: True\n",
      "Sample 2015 - Lengths Match: True\n",
      "Sample 2016 - Lengths Match: True\n",
      "Sample 2017 - Lengths Match: True\n",
      "Sample 2018 - Lengths Match: True\n",
      "Sample 2019 - Lengths Match: True\n",
      "Sample 2020 - Lengths Match: True\n",
      "Sample 2021 - Lengths Match: True\n",
      "Sample 2022 - Lengths Match: True\n",
      "Sample 2023 - Lengths Match: True\n",
      "Sample 2024 - Lengths Match: True\n",
      "Sample 2025 - Lengths Match: True\n",
      "Sample 2026 - Lengths Match: True\n",
      "Sample 2027 - Lengths Match: True\n",
      "Sample 2028 - Lengths Match: True\n",
      "Sample 2029 - Lengths Match: True\n",
      "Sample 2030 - Lengths Match: True\n",
      "Sample 2031 - Lengths Match: True\n",
      "Sample 2032 - Lengths Match: True\n",
      "Sample 2033 - Lengths Match: True\n",
      "Sample 2034 - Lengths Match: True\n",
      "Sample 2035 - Lengths Match: True\n",
      "Sample 2036 - Lengths Match: True\n",
      "Sample 2037 - Lengths Match: True\n",
      "Sample 2038 - Lengths Match: True\n",
      "Sample 2039 - Lengths Match: True\n",
      "Sample 2040 - Lengths Match: True\n",
      "Sample 2041 - Lengths Match: True\n",
      "Sample 2042 - Lengths Match: True\n",
      "Sample 2043 - Lengths Match: True\n",
      "Sample 2044 - Lengths Match: True\n",
      "Sample 2045 - Lengths Match: True\n",
      "Sample 2046 - Lengths Match: True\n",
      "Sample 2047 - Lengths Match: True\n",
      "Sample 2048 - Lengths Match: True\n",
      "Sample 2049 - Lengths Match: True\n",
      "Sample 2050 - Lengths Match: True\n",
      "Sample 2051 - Lengths Match: True\n",
      "Sample 2052 - Lengths Match: True\n",
      "Sample 2053 - Lengths Match: True\n",
      "Sample 2054 - Lengths Match: True\n",
      "Sample 2055 - Lengths Match: True\n",
      "Sample 2056 - Lengths Match: True\n",
      "Sample 2057 - Lengths Match: True\n",
      "Sample 2058 - Lengths Match: True\n",
      "Sample 2059 - Lengths Match: True\n",
      "Sample 2060 - Lengths Match: True\n",
      "Sample 2061 - Lengths Match: True\n",
      "Sample 2062 - Lengths Match: True\n",
      "Sample 2063 - Lengths Match: True\n",
      "Sample 2064 - Lengths Match: True\n",
      "Sample 2065 - Lengths Match: True\n",
      "Sample 2066 - Lengths Match: True\n",
      "Sample 2067 - Lengths Match: True\n",
      "Sample 2068 - Lengths Match: True\n",
      "Sample 2069 - Lengths Match: True\n",
      "Sample 2070 - Lengths Match: True\n",
      "Sample 2071 - Lengths Match: True\n",
      "Sample 2072 - Lengths Match: True\n",
      "Sample 2073 - Lengths Match: True\n",
      "Sample 2074 - Lengths Match: True\n",
      "Sample 2075 - Lengths Match: True\n",
      "Sample 2076 - Lengths Match: True\n",
      "Sample 2077 - Lengths Match: True\n",
      "Sample 2078 - Lengths Match: True\n",
      "Sample 2079 - Lengths Match: True\n",
      "Sample 2080 - Lengths Match: True\n",
      "Sample 2081 - Lengths Match: True\n",
      "Sample 2082 - Lengths Match: True\n",
      "Sample 2083 - Lengths Match: True\n",
      "Sample 2084 - Lengths Match: True\n",
      "Sample 2085 - Lengths Match: True\n",
      "Sample 2086 - Lengths Match: True\n",
      "Sample 2087 - Lengths Match: True\n",
      "Sample 2088 - Lengths Match: True\n",
      "Sample 2089 - Lengths Match: True\n",
      "Sample 2090 - Lengths Match: True\n",
      "Sample 2091 - Lengths Match: True\n",
      "Sample 2092 - Lengths Match: True\n",
      "Sample 2093 - Lengths Match: True\n",
      "Sample 2094 - Lengths Match: True\n",
      "Sample 2095 - Lengths Match: True\n",
      "Sample 2096 - Lengths Match: True\n",
      "Sample 2097 - Lengths Match: True\n",
      "Sample 2098 - Lengths Match: True\n",
      "Sample 2099 - Lengths Match: True\n",
      "Sample 2100 - Lengths Match: True\n",
      "Sample 2101 - Lengths Match: True\n",
      "Sample 2102 - Lengths Match: True\n",
      "Sample 2103 - Lengths Match: True\n",
      "Sample 2104 - Lengths Match: True\n",
      "Sample 2105 - Lengths Match: True\n",
      "Sample 2106 - Lengths Match: True\n",
      "Sample 2107 - Lengths Match: True\n",
      "Sample 2108 - Lengths Match: True\n",
      "Sample 2109 - Lengths Match: True\n",
      "Sample 2110 - Lengths Match: True\n",
      "Sample 2111 - Lengths Match: True\n",
      "Sample 2112 - Lengths Match: True\n",
      "Sample 2113 - Lengths Match: True\n",
      "Sample 2114 - Lengths Match: True\n",
      "Sample 2115 - Lengths Match: True\n",
      "Sample 2116 - Lengths Match: True\n",
      "Sample 2117 - Lengths Match: True\n",
      "Sample 2118 - Lengths Match: True\n",
      "Sample 2119 - Lengths Match: True\n",
      "Sample 2120 - Lengths Match: True\n",
      "Sample 2121 - Lengths Match: True\n",
      "Sample 2122 - Lengths Match: True\n",
      "Sample 2123 - Lengths Match: True\n",
      "Sample 2124 - Lengths Match: True\n",
      "Sample 2125 - Lengths Match: True\n",
      "Sample 2126 - Lengths Match: True\n",
      "Sample 2127 - Lengths Match: True\n",
      "Sample 2128 - Lengths Match: True\n",
      "Sample 2129 - Lengths Match: True\n",
      "Sample 2130 - Lengths Match: True\n",
      "Sample 2131 - Lengths Match: True\n",
      "Sample 2132 - Lengths Match: True\n",
      "Sample 2133 - Lengths Match: True\n",
      "Sample 2134 - Lengths Match: True\n",
      "Sample 2135 - Lengths Match: True\n",
      "Sample 2136 - Lengths Match: True\n",
      "Sample 2137 - Lengths Match: True\n",
      "Sample 2138 - Lengths Match: True\n",
      "Sample 2139 - Lengths Match: True\n",
      "Sample 2140 - Lengths Match: True\n",
      "Sample 2141 - Lengths Match: True\n",
      "Sample 2142 - Lengths Match: True\n",
      "Sample 2143 - Lengths Match: True\n",
      "Sample 2144 - Lengths Match: True\n",
      "Sample 2145 - Lengths Match: True\n",
      "Sample 2146 - Lengths Match: True\n",
      "Sample 2147 - Lengths Match: True\n",
      "Sample 2148 - Lengths Match: True\n",
      "Sample 2149 - Lengths Match: True\n",
      "Sample 2150 - Lengths Match: True\n",
      "Sample 2151 - Lengths Match: True\n",
      "Sample 2152 - Lengths Match: True\n",
      "Sample 2153 - Lengths Match: True\n",
      "Sample 2154 - Lengths Match: True\n",
      "Sample 2155 - Lengths Match: True\n",
      "Sample 2156 - Lengths Match: True\n",
      "Sample 2157 - Lengths Match: True\n",
      "Sample 2158 - Lengths Match: True\n",
      "Sample 2159 - Lengths Match: True\n",
      "Sample 2160 - Lengths Match: True\n",
      "Sample 2161 - Lengths Match: True\n",
      "Sample 2162 - Lengths Match: True\n",
      "Sample 2163 - Lengths Match: True\n",
      "Sample 2164 - Lengths Match: True\n",
      "Sample 2165 - Lengths Match: True\n",
      "Sample 2166 - Lengths Match: True\n",
      "Sample 2167 - Lengths Match: True\n",
      "Sample 2168 - Lengths Match: True\n",
      "Sample 2169 - Lengths Match: True\n",
      "Sample 2170 - Lengths Match: True\n",
      "Sample 2171 - Lengths Match: True\n",
      "Sample 2172 - Lengths Match: True\n",
      "Sample 2173 - Lengths Match: True\n",
      "Sample 2174 - Lengths Match: True\n",
      "Sample 2175 - Lengths Match: True\n",
      "Sample 2176 - Lengths Match: True\n",
      "Sample 2177 - Lengths Match: True\n",
      "Sample 2178 - Lengths Match: True\n",
      "Sample 2179 - Lengths Match: True\n",
      "Sample 2180 - Lengths Match: True\n",
      "Sample 2181 - Lengths Match: True\n",
      "Sample 2182 - Lengths Match: True\n",
      "Sample 2183 - Lengths Match: True\n",
      "Sample 2184 - Lengths Match: True\n",
      "Sample 2185 - Lengths Match: True\n",
      "Sample 2186 - Lengths Match: True\n",
      "Sample 2187 - Lengths Match: True\n",
      "Sample 2188 - Lengths Match: True\n",
      "Sample 2189 - Lengths Match: True\n",
      "Sample 2190 - Lengths Match: True\n",
      "Sample 2191 - Lengths Match: True\n",
      "Sample 2192 - Lengths Match: True\n",
      "Sample 2193 - Lengths Match: True\n",
      "Sample 2194 - Lengths Match: True\n",
      "Sample 2195 - Lengths Match: True\n",
      "Sample 2196 - Lengths Match: True\n",
      "Sample 2197 - Lengths Match: True\n",
      "Sample 2198 - Lengths Match: True\n",
      "Sample 2199 - Lengths Match: True\n",
      "Sample 2200 - Lengths Match: True\n",
      "Sample 2201 - Lengths Match: True\n",
      "Sample 2202 - Lengths Match: True\n",
      "Sample 2203 - Lengths Match: True\n",
      "Sample 2204 - Lengths Match: True\n",
      "Sample 2205 - Lengths Match: True\n",
      "Sample 2206 - Lengths Match: True\n",
      "Sample 2207 - Lengths Match: True\n",
      "Sample 2208 - Lengths Match: True\n",
      "Sample 2209 - Lengths Match: True\n",
      "Sample 2210 - Lengths Match: True\n",
      "Sample 2211 - Lengths Match: True\n",
      "Sample 2212 - Lengths Match: True\n",
      "Sample 2213 - Lengths Match: True\n",
      "Sample 2214 - Lengths Match: True\n",
      "Sample 2215 - Lengths Match: True\n",
      "Sample 2216 - Lengths Match: True\n",
      "Sample 2217 - Lengths Match: True\n",
      "Sample 2218 - Lengths Match: True\n",
      "Sample 2219 - Lengths Match: True\n",
      "Sample 2220 - Lengths Match: True\n",
      "Sample 2221 - Lengths Match: True\n",
      "Sample 2222 - Lengths Match: True\n",
      "Sample 2223 - Lengths Match: True\n",
      "Sample 2224 - Lengths Match: True\n",
      "Sample 2225 - Lengths Match: True\n",
      "Sample 2226 - Lengths Match: True\n",
      "Sample 2227 - Lengths Match: True\n",
      "Sample 2228 - Lengths Match: True\n",
      "Sample 2229 - Lengths Match: True\n",
      "Sample 2230 - Lengths Match: True\n",
      "Sample 2231 - Lengths Match: True\n",
      "Sample 2232 - Lengths Match: True\n",
      "Sample 2233 - Lengths Match: True\n",
      "Sample 2234 - Lengths Match: True\n",
      "Sample 2235 - Lengths Match: True\n",
      "Sample 2236 - Lengths Match: True\n",
      "Sample 2237 - Lengths Match: True\n",
      "Sample 2238 - Lengths Match: True\n",
      "Sample 2239 - Lengths Match: True\n",
      "Sample 2240 - Lengths Match: True\n",
      "Sample 2241 - Lengths Match: True\n",
      "Sample 2242 - Lengths Match: True\n",
      "Sample 2243 - Lengths Match: True\n",
      "Sample 2244 - Lengths Match: True\n",
      "Sample 2245 - Lengths Match: True\n",
      "Sample 2246 - Lengths Match: True\n",
      "Sample 2247 - Lengths Match: True\n",
      "Sample 2248 - Lengths Match: True\n",
      "Sample 2249 - Lengths Match: True\n",
      "Sample 2250 - Lengths Match: True\n",
      "Sample 2251 - Lengths Match: True\n",
      "Sample 2252 - Lengths Match: True\n",
      "Sample 2253 - Lengths Match: True\n",
      "Sample 2254 - Lengths Match: True\n",
      "Sample 2255 - Lengths Match: True\n",
      "Sample 2256 - Lengths Match: True\n",
      "Sample 2257 - Lengths Match: True\n",
      "Sample 2258 - Lengths Match: True\n",
      "Sample 2259 - Lengths Match: True\n",
      "Sample 2260 - Lengths Match: True\n",
      "Sample 2261 - Lengths Match: True\n",
      "Sample 2262 - Lengths Match: True\n",
      "Sample 2263 - Lengths Match: True\n",
      "Sample 2264 - Lengths Match: True\n",
      "Sample 2265 - Lengths Match: True\n",
      "Sample 2266 - Lengths Match: True\n",
      "Sample 2267 - Lengths Match: True\n",
      "Sample 2268 - Lengths Match: True\n",
      "Sample 2269 - Lengths Match: True\n",
      "Sample 2270 - Lengths Match: True\n",
      "Sample 2271 - Lengths Match: True\n",
      "Sample 2272 - Lengths Match: True\n",
      "Sample 2273 - Lengths Match: True\n",
      "Sample 2274 - Lengths Match: True\n",
      "Sample 2275 - Lengths Match: True\n",
      "Sample 2276 - Lengths Match: True\n",
      "Sample 2277 - Lengths Match: True\n",
      "Sample 2278 - Lengths Match: True\n",
      "Sample 2279 - Lengths Match: True\n",
      "Sample 2280 - Lengths Match: True\n",
      "Sample 2281 - Lengths Match: True\n",
      "Sample 2282 - Lengths Match: True\n",
      "Sample 2283 - Lengths Match: True\n",
      "Sample 2284 - Lengths Match: True\n",
      "Sample 2285 - Lengths Match: True\n",
      "Sample 2286 - Lengths Match: True\n",
      "Sample 2287 - Lengths Match: True\n",
      "Sample 2288 - Lengths Match: True\n",
      "Sample 2289 - Lengths Match: True\n",
      "Sample 2290 - Lengths Match: True\n",
      "Sample 2291 - Lengths Match: True\n",
      "Sample 2292 - Lengths Match: True\n",
      "Sample 2293 - Lengths Match: True\n",
      "Sample 2294 - Lengths Match: True\n",
      "Sample 2295 - Lengths Match: True\n",
      "Sample 2296 - Lengths Match: True\n",
      "Sample 2297 - Lengths Match: True\n",
      "Sample 2298 - Lengths Match: True\n",
      "Sample 2299 - Lengths Match: True\n",
      "Sample 2300 - Lengths Match: True\n",
      "Sample 2301 - Lengths Match: True\n",
      "Sample 2302 - Lengths Match: True\n",
      "Sample 2303 - Lengths Match: True\n",
      "Sample 2304 - Lengths Match: True\n",
      "Sample 2305 - Lengths Match: True\n",
      "Sample 2306 - Lengths Match: True\n",
      "Sample 2307 - Lengths Match: True\n",
      "Sample 2308 - Lengths Match: True\n",
      "Sample 2309 - Lengths Match: True\n",
      "Sample 2310 - Lengths Match: True\n",
      "Sample 2311 - Lengths Match: True\n",
      "Sample 2312 - Lengths Match: True\n",
      "Sample 2313 - Lengths Match: True\n",
      "Sample 2314 - Lengths Match: True\n",
      "Sample 2315 - Lengths Match: True\n",
      "Sample 2316 - Lengths Match: True\n",
      "Sample 2317 - Lengths Match: True\n",
      "Sample 2318 - Lengths Match: True\n",
      "Sample 2319 - Lengths Match: True\n",
      "Sample 2320 - Lengths Match: True\n",
      "Sample 2321 - Lengths Match: True\n",
      "Sample 2322 - Lengths Match: True\n",
      "Sample 2323 - Lengths Match: True\n",
      "Sample 2324 - Lengths Match: True\n",
      "Sample 2325 - Lengths Match: True\n",
      "Sample 2326 - Lengths Match: True\n",
      "Sample 2327 - Lengths Match: True\n",
      "Sample 2328 - Lengths Match: True\n",
      "Sample 2329 - Lengths Match: True\n",
      "Sample 2330 - Lengths Match: True\n",
      "Sample 2331 - Lengths Match: True\n",
      "Sample 2332 - Lengths Match: True\n",
      "Sample 2333 - Lengths Match: True\n",
      "Sample 2334 - Lengths Match: True\n",
      "Sample 2335 - Lengths Match: True\n",
      "Sample 2336 - Lengths Match: True\n",
      "Sample 2337 - Lengths Match: True\n",
      "Sample 2338 - Lengths Match: True\n",
      "Sample 2339 - Lengths Match: True\n",
      "Sample 2340 - Lengths Match: True\n",
      "Sample 2341 - Lengths Match: True\n",
      "Sample 2342 - Lengths Match: True\n",
      "Sample 2343 - Lengths Match: True\n",
      "Sample 2344 - Lengths Match: True\n",
      "Sample 2345 - Lengths Match: True\n",
      "Sample 2346 - Lengths Match: True\n",
      "Sample 2347 - Lengths Match: True\n",
      "Sample 2348 - Lengths Match: True\n",
      "Sample 2349 - Lengths Match: True\n",
      "Sample 2350 - Lengths Match: True\n",
      "Sample 2351 - Lengths Match: True\n",
      "Sample 2352 - Lengths Match: True\n",
      "Sample 2353 - Lengths Match: True\n",
      "Sample 2354 - Lengths Match: True\n",
      "Sample 2355 - Lengths Match: True\n",
      "Sample 2356 - Lengths Match: True\n",
      "Sample 2357 - Lengths Match: True\n",
      "Sample 2358 - Lengths Match: True\n",
      "Sample 2359 - Lengths Match: True\n",
      "Sample 2360 - Lengths Match: True\n",
      "Sample 2361 - Lengths Match: True\n",
      "Sample 2362 - Lengths Match: True\n",
      "Sample 2363 - Lengths Match: True\n",
      "Sample 2364 - Lengths Match: True\n",
      "Sample 2365 - Lengths Match: True\n",
      "Sample 2366 - Lengths Match: True\n",
      "Sample 2367 - Lengths Match: True\n",
      "Sample 2368 - Lengths Match: True\n",
      "Sample 2369 - Lengths Match: True\n",
      "Sample 2370 - Lengths Match: True\n",
      "Sample 2371 - Lengths Match: True\n",
      "Sample 2372 - Lengths Match: True\n",
      "Sample 2373 - Lengths Match: True\n",
      "Sample 2374 - Lengths Match: True\n",
      "Sample 2375 - Lengths Match: True\n",
      "Sample 2376 - Lengths Match: True\n",
      "Sample 2377 - Lengths Match: True\n",
      "Sample 2378 - Lengths Match: True\n",
      "Sample 2379 - Lengths Match: True\n",
      "Sample 2380 - Lengths Match: True\n",
      "Sample 2381 - Lengths Match: True\n",
      "Sample 2382 - Lengths Match: True\n",
      "Sample 2383 - Lengths Match: True\n",
      "Sample 2384 - Lengths Match: True\n",
      "Sample 2385 - Lengths Match: True\n",
      "Sample 2386 - Lengths Match: True\n",
      "Sample 2387 - Lengths Match: True\n",
      "Sample 2388 - Lengths Match: True\n",
      "Sample 2389 - Lengths Match: True\n",
      "Sample 2390 - Lengths Match: True\n",
      "Sample 2391 - Lengths Match: True\n",
      "Sample 2392 - Lengths Match: True\n",
      "Sample 2393 - Lengths Match: True\n",
      "Sample 2394 - Lengths Match: True\n",
      "Sample 2395 - Lengths Match: True\n",
      "Sample 2396 - Lengths Match: True\n",
      "Sample 2397 - Lengths Match: True\n",
      "Sample 2398 - Lengths Match: True\n",
      "Sample 2399 - Lengths Match: True\n",
      "Sample 2400 - Lengths Match: True\n",
      "Sample 2401 - Lengths Match: True\n",
      "Sample 2402 - Lengths Match: True\n",
      "Sample 2403 - Lengths Match: True\n",
      "Sample 2404 - Lengths Match: True\n",
      "Sample 2405 - Lengths Match: True\n",
      "Sample 2406 - Lengths Match: True\n",
      "Sample 2407 - Lengths Match: True\n",
      "Sample 2408 - Lengths Match: True\n",
      "Sample 2409 - Lengths Match: True\n",
      "Sample 2410 - Lengths Match: True\n",
      "Sample 2411 - Lengths Match: True\n",
      "Sample 2412 - Lengths Match: True\n",
      "Sample 2413 - Lengths Match: True\n",
      "Sample 2414 - Lengths Match: True\n",
      "Sample 2415 - Lengths Match: True\n",
      "Sample 2416 - Lengths Match: True\n",
      "Sample 2417 - Lengths Match: True\n",
      "Sample 2418 - Lengths Match: True\n",
      "Sample 2419 - Lengths Match: True\n",
      "Sample 2420 - Lengths Match: True\n",
      "Sample 2421 - Lengths Match: True\n",
      "Sample 2422 - Lengths Match: True\n",
      "Sample 2423 - Lengths Match: True\n",
      "Sample 2424 - Lengths Match: True\n",
      "Sample 2425 - Lengths Match: True\n",
      "Sample 2426 - Lengths Match: True\n",
      "Sample 2427 - Lengths Match: True\n",
      "Sample 2428 - Lengths Match: True\n",
      "Sample 2429 - Lengths Match: True\n",
      "Sample 2430 - Lengths Match: True\n",
      "Sample 2431 - Lengths Match: True\n",
      "Sample 2432 - Lengths Match: True\n",
      "Sample 2433 - Lengths Match: True\n",
      "Sample 2434 - Lengths Match: True\n",
      "Sample 2435 - Lengths Match: True\n",
      "Sample 2436 - Lengths Match: True\n",
      "Sample 2437 - Lengths Match: True\n",
      "Sample 2438 - Lengths Match: True\n",
      "Sample 2439 - Lengths Match: True\n",
      "Sample 2440 - Lengths Match: True\n",
      "Sample 2441 - Lengths Match: True\n",
      "Sample 2442 - Lengths Match: True\n",
      "Sample 2443 - Lengths Match: True\n",
      "Sample 2444 - Lengths Match: True\n",
      "Sample 2445 - Lengths Match: True\n",
      "Sample 2446 - Lengths Match: True\n",
      "Sample 2447 - Lengths Match: True\n",
      "Sample 2448 - Lengths Match: True\n",
      "Sample 2449 - Lengths Match: True\n",
      "Sample 2450 - Lengths Match: True\n",
      "Sample 2451 - Lengths Match: True\n",
      "Sample 2452 - Lengths Match: True\n",
      "Sample 2453 - Lengths Match: True\n",
      "Sample 2454 - Lengths Match: True\n",
      "Sample 2455 - Lengths Match: True\n",
      "Sample 2456 - Lengths Match: True\n",
      "Sample 2457 - Lengths Match: True\n",
      "Sample 2458 - Lengths Match: True\n",
      "Sample 2459 - Lengths Match: True\n",
      "Sample 2460 - Lengths Match: True\n",
      "Sample 2461 - Lengths Match: True\n",
      "Sample 2462 - Lengths Match: True\n",
      "Sample 2463 - Lengths Match: True\n",
      "Sample 2464 - Lengths Match: True\n",
      "Sample 2465 - Lengths Match: True\n",
      "Sample 2466 - Lengths Match: True\n",
      "Sample 2467 - Lengths Match: True\n",
      "Sample 2468 - Lengths Match: True\n",
      "Sample 2469 - Lengths Match: True\n",
      "Sample 2470 - Lengths Match: True\n",
      "Sample 2471 - Lengths Match: True\n",
      "Sample 2472 - Lengths Match: True\n",
      "Sample 2473 - Lengths Match: True\n",
      "Sample 2474 - Lengths Match: True\n",
      "Sample 2475 - Lengths Match: True\n",
      "Sample 2476 - Lengths Match: True\n",
      "Sample 2477 - Lengths Match: True\n",
      "Sample 2478 - Lengths Match: True\n",
      "Sample 2479 - Lengths Match: True\n",
      "Sample 2480 - Lengths Match: True\n",
      "Sample 2481 - Lengths Match: True\n",
      "Sample 2482 - Lengths Match: True\n",
      "Sample 2483 - Lengths Match: True\n",
      "Sample 2484 - Lengths Match: True\n",
      "Sample 2485 - Lengths Match: True\n",
      "Sample 2486 - Lengths Match: True\n",
      "Sample 2487 - Lengths Match: True\n",
      "Sample 2488 - Lengths Match: True\n",
      "Sample 2489 - Lengths Match: True\n",
      "Sample 2490 - Lengths Match: True\n",
      "Sample 2491 - Lengths Match: True\n",
      "Sample 2492 - Lengths Match: True\n",
      "Sample 2493 - Lengths Match: True\n",
      "Sample 2494 - Lengths Match: True\n",
      "Sample 2495 - Lengths Match: True\n",
      "Sample 2496 - Lengths Match: True\n",
      "Sample 2497 - Lengths Match: True\n",
      "Sample 2498 - Lengths Match: True\n",
      "Sample 2499 - Lengths Match: True\n",
      "Sample 2500 - Lengths Match: True\n",
      "Sample 2501 - Lengths Match: True\n",
      "Sample 2502 - Lengths Match: True\n",
      "Sample 2503 - Lengths Match: True\n",
      "Sample 2504 - Lengths Match: True\n",
      "Sample 2505 - Lengths Match: True\n",
      "Sample 2506 - Lengths Match: True\n",
      "Sample 2507 - Lengths Match: True\n",
      "Sample 2508 - Lengths Match: True\n",
      "Sample 2509 - Lengths Match: True\n",
      "Sample 2510 - Lengths Match: True\n",
      "Sample 2511 - Lengths Match: True\n",
      "Sample 2512 - Lengths Match: True\n",
      "Sample 2513 - Lengths Match: True\n",
      "Sample 2514 - Lengths Match: True\n",
      "Sample 2515 - Lengths Match: True\n",
      "Sample 2516 - Lengths Match: True\n",
      "Sample 2517 - Lengths Match: True\n",
      "Sample 2518 - Lengths Match: True\n",
      "Sample 2519 - Lengths Match: True\n",
      "Sample 2520 - Lengths Match: True\n",
      "Sample 2521 - Lengths Match: True\n",
      "Sample 2522 - Lengths Match: True\n",
      "Sample 2523 - Lengths Match: True\n",
      "Sample 2524 - Lengths Match: True\n",
      "Sample 2525 - Lengths Match: True\n",
      "Sample 2526 - Lengths Match: True\n",
      "Sample 2527 - Lengths Match: True\n",
      "Sample 2528 - Lengths Match: True\n",
      "Sample 2529 - Lengths Match: True\n",
      "Sample 2530 - Lengths Match: True\n",
      "Sample 2531 - Lengths Match: True\n",
      "Sample 2532 - Lengths Match: True\n",
      "Sample 2533 - Lengths Match: True\n",
      "Sample 2534 - Lengths Match: True\n",
      "Sample 2535 - Lengths Match: True\n",
      "Sample 2536 - Lengths Match: True\n",
      "Sample 2537 - Lengths Match: True\n",
      "Sample 2538 - Lengths Match: True\n",
      "Sample 2539 - Lengths Match: True\n",
      "Sample 2540 - Lengths Match: True\n",
      "Sample 2541 - Lengths Match: True\n",
      "Sample 2542 - Lengths Match: True\n",
      "Sample 2543 - Lengths Match: True\n",
      "Sample 2544 - Lengths Match: True\n",
      "Sample 2545 - Lengths Match: True\n",
      "Sample 2546 - Lengths Match: True\n",
      "Sample 2547 - Lengths Match: True\n",
      "Sample 2548 - Lengths Match: True\n",
      "Sample 2549 - Lengths Match: True\n",
      "Sample 2550 - Lengths Match: True\n",
      "Sample 2551 - Lengths Match: True\n",
      "Sample 2552 - Lengths Match: True\n",
      "Sample 2553 - Lengths Match: True\n",
      "Sample 2554 - Lengths Match: True\n",
      "Sample 2555 - Lengths Match: True\n",
      "Sample 2556 - Lengths Match: True\n",
      "Sample 2557 - Lengths Match: True\n",
      "Sample 2558 - Lengths Match: True\n",
      "Sample 2559 - Lengths Match: True\n",
      "Sample 2560 - Lengths Match: True\n",
      "Sample 2561 - Lengths Match: True\n",
      "Sample 2562 - Lengths Match: True\n",
      "Sample 2563 - Lengths Match: True\n",
      "Sample 2564 - Lengths Match: True\n",
      "Sample 2565 - Lengths Match: True\n",
      "Sample 2566 - Lengths Match: True\n",
      "Sample 2567 - Lengths Match: True\n",
      "Sample 2568 - Lengths Match: True\n",
      "Sample 2569 - Lengths Match: True\n",
      "Sample 2570 - Lengths Match: True\n",
      "Sample 2571 - Lengths Match: True\n",
      "Sample 2572 - Lengths Match: True\n",
      "Sample 2573 - Lengths Match: True\n",
      "Sample 2574 - Lengths Match: True\n",
      "Sample 2575 - Lengths Match: True\n",
      "Sample 2576 - Lengths Match: True\n",
      "Sample 2577 - Lengths Match: True\n",
      "Sample 2578 - Lengths Match: True\n",
      "Sample 2579 - Lengths Match: True\n",
      "Sample 2580 - Lengths Match: True\n",
      "Sample 2581 - Lengths Match: True\n",
      "Sample 2582 - Lengths Match: True\n",
      "Sample 2583 - Lengths Match: True\n",
      "Sample 2584 - Lengths Match: True\n",
      "Sample 2585 - Lengths Match: True\n",
      "Sample 2586 - Lengths Match: True\n",
      "Sample 2587 - Lengths Match: True\n",
      "Sample 2588 - Lengths Match: True\n",
      "Sample 2589 - Lengths Match: True\n",
      "Sample 2590 - Lengths Match: True\n",
      "Sample 2591 - Lengths Match: True\n",
      "Sample 2592 - Lengths Match: True\n",
      "Sample 2593 - Lengths Match: True\n",
      "Sample 2594 - Lengths Match: True\n",
      "Sample 2595 - Lengths Match: True\n",
      "Sample 2596 - Lengths Match: True\n",
      "Sample 2597 - Lengths Match: True\n",
      "Sample 2598 - Lengths Match: True\n",
      "Sample 2599 - Lengths Match: True\n",
      "Sample 2600 - Lengths Match: True\n",
      "Sample 2601 - Lengths Match: True\n",
      "Sample 2602 - Lengths Match: True\n",
      "Sample 2603 - Lengths Match: True\n",
      "Sample 2604 - Lengths Match: True\n",
      "Sample 2605 - Lengths Match: True\n",
      "Sample 2606 - Lengths Match: True\n",
      "Sample 2607 - Lengths Match: True\n",
      "Sample 2608 - Lengths Match: True\n",
      "Sample 2609 - Lengths Match: True\n",
      "Sample 2610 - Lengths Match: True\n",
      "Sample 2611 - Lengths Match: True\n",
      "Sample 2612 - Lengths Match: True\n",
      "Sample 2613 - Lengths Match: True\n",
      "Sample 2614 - Lengths Match: True\n",
      "Sample 2615 - Lengths Match: True\n",
      "Sample 2616 - Lengths Match: True\n",
      "Sample 2617 - Lengths Match: True\n",
      "Sample 2618 - Lengths Match: True\n",
      "Sample 2619 - Lengths Match: True\n",
      "Sample 2620 - Lengths Match: True\n",
      "Sample 2621 - Lengths Match: True\n",
      "Sample 2622 - Lengths Match: True\n",
      "Sample 2623 - Lengths Match: True\n",
      "Sample 2624 - Lengths Match: True\n",
      "Sample 2625 - Lengths Match: True\n",
      "Sample 2626 - Lengths Match: True\n",
      "Sample 2627 - Lengths Match: True\n",
      "Sample 2628 - Lengths Match: True\n",
      "Sample 2629 - Lengths Match: True\n",
      "Sample 2630 - Lengths Match: True\n",
      "Sample 2631 - Lengths Match: True\n",
      "Sample 2632 - Lengths Match: True\n",
      "Sample 2633 - Lengths Match: True\n",
      "Sample 2634 - Lengths Match: True\n",
      "Sample 2635 - Lengths Match: True\n",
      "Sample 2636 - Lengths Match: True\n",
      "Sample 2637 - Lengths Match: True\n",
      "Sample 2638 - Lengths Match: True\n",
      "Sample 2639 - Lengths Match: True\n",
      "Sample 2640 - Lengths Match: True\n",
      "Sample 2641 - Lengths Match: True\n",
      "Sample 2642 - Lengths Match: True\n",
      "Sample 2643 - Lengths Match: True\n",
      "Sample 2644 - Lengths Match: True\n",
      "Sample 2645 - Lengths Match: True\n",
      "Sample 2646 - Lengths Match: True\n",
      "Sample 2647 - Lengths Match: True\n",
      "Sample 2648 - Lengths Match: True\n",
      "Sample 2649 - Lengths Match: True\n",
      "Sample 2650 - Lengths Match: True\n",
      "Sample 2651 - Lengths Match: True\n",
      "Sample 2652 - Lengths Match: True\n",
      "Sample 2653 - Lengths Match: True\n",
      "Sample 2654 - Lengths Match: True\n",
      "Sample 2655 - Lengths Match: True\n",
      "Sample 2656 - Lengths Match: True\n",
      "Sample 2657 - Lengths Match: True\n",
      "Sample 2658 - Lengths Match: True\n",
      "Sample 2659 - Lengths Match: True\n",
      "Sample 2660 - Lengths Match: True\n",
      "Sample 2661 - Lengths Match: True\n",
      "Sample 2662 - Lengths Match: True\n",
      "Sample 2663 - Lengths Match: True\n",
      "Sample 2664 - Lengths Match: True\n",
      "Sample 2665 - Lengths Match: True\n",
      "Sample 2666 - Lengths Match: True\n",
      "Sample 2667 - Lengths Match: True\n",
      "Sample 2668 - Lengths Match: True\n",
      "Sample 2669 - Lengths Match: True\n",
      "Sample 2670 - Lengths Match: True\n",
      "Sample 2671 - Lengths Match: True\n",
      "Sample 2672 - Lengths Match: True\n",
      "Sample 2673 - Lengths Match: True\n",
      "Sample 2674 - Lengths Match: True\n",
      "Sample 2675 - Lengths Match: True\n",
      "Sample 2676 - Lengths Match: True\n",
      "Sample 2677 - Lengths Match: True\n",
      "Sample 2678 - Lengths Match: True\n",
      "Sample 2679 - Lengths Match: True\n",
      "Sample 2680 - Lengths Match: True\n",
      "Sample 2681 - Lengths Match: True\n",
      "Sample 2682 - Lengths Match: True\n",
      "Sample 2683 - Lengths Match: True\n",
      "Sample 2684 - Lengths Match: True\n",
      "Sample 2685 - Lengths Match: True\n",
      "Sample 2686 - Lengths Match: True\n",
      "Sample 2687 - Lengths Match: True\n",
      "Sample 2688 - Lengths Match: True\n",
      "Sample 2689 - Lengths Match: True\n",
      "Sample 2690 - Lengths Match: True\n",
      "Sample 2691 - Lengths Match: True\n",
      "Sample 2692 - Lengths Match: True\n",
      "Sample 2693 - Lengths Match: True\n",
      "Sample 2694 - Lengths Match: True\n",
      "Sample 2695 - Lengths Match: True\n",
      "Sample 2696 - Lengths Match: True\n",
      "Sample 2697 - Lengths Match: True\n",
      "Sample 2698 - Lengths Match: True\n",
      "Sample 2699 - Lengths Match: True\n",
      "Sample 2700 - Lengths Match: True\n",
      "Sample 2701 - Lengths Match: True\n",
      "Sample 2702 - Lengths Match: True\n",
      "Sample 2703 - Lengths Match: True\n",
      "Sample 2704 - Lengths Match: True\n",
      "Sample 2705 - Lengths Match: True\n",
      "Sample 2706 - Lengths Match: True\n",
      "Sample 2707 - Lengths Match: True\n",
      "Sample 2708 - Lengths Match: True\n",
      "Sample 2709 - Lengths Match: True\n",
      "Sample 2710 - Lengths Match: True\n",
      "Sample 2711 - Lengths Match: True\n",
      "Sample 2712 - Lengths Match: True\n",
      "Sample 2713 - Lengths Match: True\n",
      "Sample 2714 - Lengths Match: True\n",
      "Sample 2715 - Lengths Match: True\n",
      "Sample 2716 - Lengths Match: True\n",
      "Sample 2717 - Lengths Match: True\n",
      "Sample 2718 - Lengths Match: True\n",
      "Sample 2719 - Lengths Match: True\n",
      "Sample 2720 - Lengths Match: True\n",
      "Sample 2721 - Lengths Match: True\n",
      "Sample 2722 - Lengths Match: True\n",
      "Sample 2723 - Lengths Match: True\n",
      "Sample 2724 - Lengths Match: True\n",
      "Sample 2725 - Lengths Match: True\n",
      "Sample 2726 - Lengths Match: True\n",
      "Sample 2727 - Lengths Match: True\n",
      "Sample 2728 - Lengths Match: True\n",
      "Sample 2729 - Lengths Match: True\n",
      "Sample 2730 - Lengths Match: True\n",
      "Sample 2731 - Lengths Match: True\n",
      "Sample 2732 - Lengths Match: True\n",
      "Sample 2733 - Lengths Match: True\n",
      "Sample 2734 - Lengths Match: True\n",
      "Sample 2735 - Lengths Match: True\n",
      "Sample 2736 - Lengths Match: True\n",
      "Sample 2737 - Lengths Match: True\n",
      "Sample 2738 - Lengths Match: True\n",
      "Sample 2739 - Lengths Match: True\n",
      "Sample 2740 - Lengths Match: True\n",
      "Sample 2741 - Lengths Match: True\n",
      "Sample 2742 - Lengths Match: True\n",
      "Sample 2743 - Lengths Match: True\n",
      "Sample 2744 - Lengths Match: True\n",
      "Sample 2745 - Lengths Match: True\n",
      "Sample 2746 - Lengths Match: True\n",
      "Sample 2747 - Lengths Match: True\n",
      "Sample 2748 - Lengths Match: True\n",
      "Sample 2749 - Lengths Match: True\n",
      "Sample 2750 - Lengths Match: True\n",
      "Sample 2751 - Lengths Match: True\n",
      "Sample 2752 - Lengths Match: True\n",
      "Sample 2753 - Lengths Match: True\n",
      "Sample 2754 - Lengths Match: True\n",
      "Sample 2755 - Lengths Match: True\n",
      "Sample 2756 - Lengths Match: True\n",
      "Sample 2757 - Lengths Match: True\n",
      "Sample 2758 - Lengths Match: True\n",
      "Sample 2759 - Lengths Match: True\n",
      "Sample 2760 - Lengths Match: True\n",
      "Sample 2761 - Lengths Match: True\n",
      "Sample 2762 - Lengths Match: True\n",
      "Sample 2763 - Lengths Match: True\n",
      "Sample 2764 - Lengths Match: True\n",
      "Sample 2765 - Lengths Match: True\n",
      "Sample 2766 - Lengths Match: True\n",
      "Sample 2767 - Lengths Match: True\n",
      "Sample 2768 - Lengths Match: True\n",
      "Sample 2769 - Lengths Match: True\n",
      "Sample 2770 - Lengths Match: True\n",
      "Sample 2771 - Lengths Match: True\n",
      "Sample 2772 - Lengths Match: True\n",
      "Sample 2773 - Lengths Match: True\n",
      "Sample 2774 - Lengths Match: True\n",
      "Sample 2775 - Lengths Match: True\n",
      "Sample 2776 - Lengths Match: True\n",
      "Sample 2777 - Lengths Match: True\n",
      "Sample 2778 - Lengths Match: True\n",
      "Sample 2779 - Lengths Match: True\n",
      "Sample 2780 - Lengths Match: True\n",
      "Sample 2781 - Lengths Match: True\n",
      "Sample 2782 - Lengths Match: True\n",
      "Sample 2783 - Lengths Match: True\n",
      "Sample 2784 - Lengths Match: True\n",
      "Sample 2785 - Lengths Match: True\n",
      "Sample 2786 - Lengths Match: True\n",
      "Sample 2787 - Lengths Match: True\n",
      "Sample 2788 - Lengths Match: True\n",
      "Sample 2789 - Lengths Match: True\n",
      "Sample 2790 - Lengths Match: True\n",
      "Sample 2791 - Lengths Match: True\n",
      "Sample 2792 - Lengths Match: True\n",
      "Sample 2793 - Lengths Match: True\n",
      "Sample 2794 - Lengths Match: True\n",
      "Sample 2795 - Lengths Match: True\n",
      "Sample 2796 - Lengths Match: True\n",
      "Sample 2797 - Lengths Match: True\n",
      "Sample 2798 - Lengths Match: True\n",
      "Sample 2799 - Lengths Match: True\n",
      "Sample 2800 - Lengths Match: True\n",
      "Sample 2801 - Lengths Match: True\n",
      "Sample 2802 - Lengths Match: True\n",
      "Sample 2803 - Lengths Match: True\n",
      "Sample 2804 - Lengths Match: True\n",
      "Sample 2805 - Lengths Match: True\n",
      "Sample 2806 - Lengths Match: True\n",
      "Sample 2807 - Lengths Match: True\n",
      "Sample 2808 - Lengths Match: True\n",
      "Sample 2809 - Lengths Match: True\n",
      "Sample 2810 - Lengths Match: True\n",
      "Sample 2811 - Lengths Match: True\n",
      "Sample 2812 - Lengths Match: True\n",
      "Sample 2813 - Lengths Match: True\n",
      "Sample 2814 - Lengths Match: True\n",
      "Sample 2815 - Lengths Match: True\n",
      "Sample 2816 - Lengths Match: True\n",
      "Sample 2817 - Lengths Match: True\n",
      "Sample 2818 - Lengths Match: True\n",
      "Sample 2819 - Lengths Match: True\n",
      "Sample 2820 - Lengths Match: True\n",
      "Sample 2821 - Lengths Match: True\n",
      "Sample 2822 - Lengths Match: True\n",
      "Sample 2823 - Lengths Match: True\n",
      "Sample 2824 - Lengths Match: True\n",
      "Sample 2825 - Lengths Match: True\n",
      "Sample 2826 - Lengths Match: True\n",
      "Sample 2827 - Lengths Match: True\n",
      "Sample 2828 - Lengths Match: True\n",
      "Sample 2829 - Lengths Match: True\n",
      "Sample 2830 - Lengths Match: True\n",
      "Sample 2831 - Lengths Match: True\n",
      "Sample 2832 - Lengths Match: True\n",
      "Sample 2833 - Lengths Match: True\n",
      "Sample 2834 - Lengths Match: True\n",
      "Sample 2835 - Lengths Match: True\n",
      "Sample 2836 - Lengths Match: True\n",
      "Sample 2837 - Lengths Match: True\n",
      "Sample 2838 - Lengths Match: True\n",
      "Sample 2839 - Lengths Match: True\n",
      "Sample 2840 - Lengths Match: True\n",
      "Sample 2841 - Lengths Match: True\n",
      "Sample 2842 - Lengths Match: True\n",
      "Sample 2843 - Lengths Match: True\n",
      "Sample 2844 - Lengths Match: True\n",
      "Sample 2845 - Lengths Match: True\n",
      "Sample 2846 - Lengths Match: True\n",
      "Sample 2847 - Lengths Match: True\n",
      "Sample 2848 - Lengths Match: True\n",
      "Sample 2849 - Lengths Match: True\n",
      "Sample 2850 - Lengths Match: True\n",
      "Sample 2851 - Lengths Match: True\n",
      "Sample 2852 - Lengths Match: True\n",
      "Sample 2853 - Lengths Match: True\n",
      "Sample 2854 - Lengths Match: True\n",
      "Sample 2855 - Lengths Match: True\n",
      "Sample 2856 - Lengths Match: True\n",
      "Sample 2857 - Lengths Match: True\n",
      "Sample 2858 - Lengths Match: True\n",
      "Sample 2859 - Lengths Match: True\n",
      "Sample 2860 - Lengths Match: True\n",
      "Sample 2861 - Lengths Match: True\n",
      "Sample 2862 - Lengths Match: True\n",
      "Sample 2863 - Lengths Match: True\n",
      "Sample 2864 - Lengths Match: True\n",
      "Sample 2865 - Lengths Match: True\n",
      "Sample 2866 - Lengths Match: True\n",
      "Sample 2867 - Lengths Match: True\n",
      "Sample 2868 - Lengths Match: True\n",
      "Sample 2869 - Lengths Match: True\n",
      "Sample 2870 - Lengths Match: True\n",
      "Sample 2871 - Lengths Match: True\n",
      "Sample 2872 - Lengths Match: True\n",
      "Sample 2873 - Lengths Match: True\n",
      "Sample 2874 - Lengths Match: True\n",
      "Sample 2875 - Lengths Match: True\n",
      "Sample 2876 - Lengths Match: True\n",
      "Sample 2877 - Lengths Match: True\n",
      "Sample 2878 - Lengths Match: True\n",
      "Sample 2879 - Lengths Match: True\n",
      "Sample 2880 - Lengths Match: True\n",
      "Sample 2881 - Lengths Match: True\n",
      "Sample 2882 - Lengths Match: True\n",
      "Sample 2883 - Lengths Match: True\n",
      "Sample 2884 - Lengths Match: True\n",
      "Sample 2885 - Lengths Match: True\n",
      "Sample 2886 - Lengths Match: True\n",
      "Sample 2887 - Lengths Match: True\n",
      "Sample 2888 - Lengths Match: True\n",
      "Sample 2889 - Lengths Match: True\n",
      "Sample 2890 - Lengths Match: True\n",
      "Sample 2891 - Lengths Match: True\n",
      "Sample 2892 - Lengths Match: True\n",
      "Sample 2893 - Lengths Match: True\n",
      "Sample 2894 - Lengths Match: True\n",
      "Sample 2895 - Lengths Match: True\n",
      "Sample 2896 - Lengths Match: True\n",
      "Sample 2897 - Lengths Match: True\n",
      "Sample 2898 - Lengths Match: True\n",
      "Sample 2899 - Lengths Match: True\n",
      "Sample 2900 - Lengths Match: True\n",
      "Sample 2901 - Lengths Match: True\n",
      "Sample 2902 - Lengths Match: True\n",
      "Sample 2903 - Lengths Match: True\n",
      "Sample 2904 - Lengths Match: True\n",
      "Sample 2905 - Lengths Match: True\n",
      "Sample 2906 - Lengths Match: True\n",
      "Sample 2907 - Lengths Match: True\n",
      "Sample 2908 - Lengths Match: True\n",
      "Sample 2909 - Lengths Match: True\n",
      "Sample 2910 - Lengths Match: True\n",
      "Sample 2911 - Lengths Match: True\n",
      "Sample 2912 - Lengths Match: True\n",
      "Sample 2913 - Lengths Match: True\n",
      "Sample 2914 - Lengths Match: True\n",
      "Sample 2915 - Lengths Match: True\n",
      "Sample 2916 - Lengths Match: True\n",
      "Sample 2917 - Lengths Match: True\n",
      "Sample 2918 - Lengths Match: True\n",
      "Sample 2919 - Lengths Match: True\n",
      "Sample 2920 - Lengths Match: True\n",
      "Sample 2921 - Lengths Match: True\n",
      "Sample 2922 - Lengths Match: True\n",
      "Sample 2923 - Lengths Match: True\n",
      "Sample 2924 - Lengths Match: True\n",
      "Sample 2925 - Lengths Match: True\n",
      "Sample 2926 - Lengths Match: True\n",
      "Sample 2927 - Lengths Match: True\n",
      "Sample 2928 - Lengths Match: True\n",
      "Sample 2929 - Lengths Match: True\n",
      "Sample 2930 - Lengths Match: True\n",
      "Sample 2931 - Lengths Match: True\n",
      "Sample 2932 - Lengths Match: True\n",
      "Sample 2933 - Lengths Match: True\n",
      "Sample 2934 - Lengths Match: True\n",
      "Sample 2935 - Lengths Match: True\n",
      "Sample 2936 - Lengths Match: True\n",
      "Sample 2937 - Lengths Match: True\n",
      "Sample 2938 - Lengths Match: True\n",
      "Sample 2939 - Lengths Match: True\n",
      "Sample 2940 - Lengths Match: True\n",
      "Sample 2941 - Lengths Match: True\n",
      "Sample 2942 - Lengths Match: True\n",
      "Sample 2943 - Lengths Match: True\n",
      "Sample 2944 - Lengths Match: True\n",
      "Sample 2945 - Lengths Match: True\n",
      "Sample 2946 - Lengths Match: True\n",
      "Sample 2947 - Lengths Match: True\n",
      "Sample 2948 - Lengths Match: True\n",
      "Sample 2949 - Lengths Match: True\n",
      "Sample 2950 - Lengths Match: True\n",
      "Sample 2951 - Lengths Match: True\n",
      "Sample 2952 - Lengths Match: True\n",
      "Sample 2953 - Lengths Match: True\n",
      "Sample 2954 - Lengths Match: True\n",
      "Sample 2955 - Lengths Match: True\n",
      "Sample 2956 - Lengths Match: True\n",
      "Sample 2957 - Lengths Match: True\n",
      "Sample 2958 - Lengths Match: True\n",
      "Sample 2959 - Lengths Match: True\n",
      "Sample 2960 - Lengths Match: True\n",
      "Sample 2961 - Lengths Match: True\n",
      "Sample 2962 - Lengths Match: True\n",
      "Sample 2963 - Lengths Match: True\n",
      "Sample 2964 - Lengths Match: True\n",
      "Sample 2965 - Lengths Match: True\n",
      "Sample 2966 - Lengths Match: True\n",
      "Sample 2967 - Lengths Match: True\n",
      "Sample 2968 - Lengths Match: True\n",
      "Sample 2969 - Lengths Match: True\n",
      "Sample 2970 - Lengths Match: True\n",
      "Sample 2971 - Lengths Match: True\n",
      "Sample 2972 - Lengths Match: True\n",
      "Sample 2973 - Lengths Match: True\n",
      "Sample 2974 - Lengths Match: True\n",
      "Sample 2975 - Lengths Match: True\n",
      "Sample 2976 - Lengths Match: True\n",
      "Sample 2977 - Lengths Match: True\n",
      "Sample 2978 - Lengths Match: True\n",
      "Sample 2979 - Lengths Match: True\n",
      "Sample 2980 - Lengths Match: True\n",
      "Sample 2981 - Lengths Match: True\n",
      "Sample 2982 - Lengths Match: True\n",
      "Sample 2983 - Lengths Match: True\n",
      "Sample 2984 - Lengths Match: True\n",
      "Sample 2985 - Lengths Match: True\n",
      "Sample 2986 - Lengths Match: True\n",
      "Sample 2987 - Lengths Match: True\n",
      "Sample 2988 - Lengths Match: True\n",
      "Sample 2989 - Lengths Match: True\n",
      "Sample 2990 - Lengths Match: True\n",
      "Sample 2991 - Lengths Match: True\n",
      "Sample 2992 - Lengths Match: True\n",
      "Sample 2993 - Lengths Match: True\n",
      "Sample 2994 - Lengths Match: True\n",
      "Sample 2995 - Lengths Match: True\n",
      "Sample 2996 - Lengths Match: True\n",
      "Sample 2997 - Lengths Match: True\n",
      "Sample 2998 - Lengths Match: True\n",
      "Sample 2999 - Lengths Match: True\n",
      "Sample 3000 - Lengths Match: True\n",
      "Sample 3001 - Lengths Match: True\n",
      "Sample 3002 - Lengths Match: True\n",
      "Sample 3003 - Lengths Match: True\n",
      "Sample 3004 - Lengths Match: True\n",
      "Sample 3005 - Lengths Match: True\n",
      "Sample 3006 - Lengths Match: True\n",
      "Sample 3007 - Lengths Match: True\n",
      "Sample 3008 - Lengths Match: True\n",
      "Sample 3009 - Lengths Match: True\n",
      "Sample 3010 - Lengths Match: True\n",
      "Sample 3011 - Lengths Match: True\n",
      "Sample 3012 - Lengths Match: True\n",
      "Sample 3013 - Lengths Match: True\n",
      "Sample 3014 - Lengths Match: True\n",
      "Sample 3015 - Lengths Match: True\n",
      "Sample 3016 - Lengths Match: True\n",
      "Sample 3017 - Lengths Match: True\n",
      "Sample 3018 - Lengths Match: True\n",
      "Sample 3019 - Lengths Match: True\n",
      "Sample 3020 - Lengths Match: True\n",
      "Sample 3021 - Lengths Match: True\n",
      "Sample 3022 - Lengths Match: True\n",
      "Sample 3023 - Lengths Match: True\n",
      "Sample 3024 - Lengths Match: True\n",
      "Sample 3025 - Lengths Match: True\n",
      "Sample 3026 - Lengths Match: True\n",
      "Sample 3027 - Lengths Match: True\n",
      "Sample 3028 - Lengths Match: True\n",
      "Sample 3029 - Lengths Match: True\n",
      "Sample 3030 - Lengths Match: True\n",
      "Sample 3031 - Lengths Match: True\n",
      "Sample 3032 - Lengths Match: True\n",
      "Sample 3033 - Lengths Match: True\n",
      "Sample 3034 - Lengths Match: True\n",
      "Sample 3035 - Lengths Match: True\n",
      "Sample 3036 - Lengths Match: True\n",
      "Sample 3037 - Lengths Match: True\n",
      "Sample 3038 - Lengths Match: True\n",
      "Sample 3039 - Lengths Match: True\n",
      "Sample 3040 - Lengths Match: True\n",
      "Sample 3041 - Lengths Match: True\n",
      "Sample 3042 - Lengths Match: True\n",
      "Sample 3043 - Lengths Match: True\n",
      "Sample 3044 - Lengths Match: True\n",
      "Sample 3045 - Lengths Match: True\n",
      "Sample 3046 - Lengths Match: True\n",
      "Sample 3047 - Lengths Match: True\n",
      "Sample 3048 - Lengths Match: True\n",
      "Sample 3049 - Lengths Match: True\n",
      "Sample 3050 - Lengths Match: True\n",
      "Sample 3051 - Lengths Match: True\n",
      "Sample 3052 - Lengths Match: True\n",
      "Sample 3053 - Lengths Match: True\n",
      "Sample 3054 - Lengths Match: True\n",
      "Sample 3055 - Lengths Match: True\n",
      "Sample 3056 - Lengths Match: True\n",
      "Sample 3057 - Lengths Match: True\n",
      "Sample 3058 - Lengths Match: True\n",
      "Sample 3059 - Lengths Match: True\n",
      "Sample 3060 - Lengths Match: True\n",
      "Sample 3061 - Lengths Match: True\n",
      "Sample 3062 - Lengths Match: True\n",
      "Sample 3063 - Lengths Match: True\n",
      "Sample 3064 - Lengths Match: True\n",
      "Sample 3065 - Lengths Match: True\n",
      "Sample 3066 - Lengths Match: True\n",
      "Sample 3067 - Lengths Match: True\n",
      "Sample 3068 - Lengths Match: True\n",
      "Sample 3069 - Lengths Match: True\n",
      "Sample 3070 - Lengths Match: True\n",
      "Sample 3071 - Lengths Match: True\n",
      "Sample 3072 - Lengths Match: True\n",
      "Sample 3073 - Lengths Match: True\n",
      "Sample 3074 - Lengths Match: True\n",
      "Sample 3075 - Lengths Match: True\n",
      "Sample 3076 - Lengths Match: True\n",
      "Sample 3077 - Lengths Match: True\n",
      "Sample 3078 - Lengths Match: True\n",
      "Sample 3079 - Lengths Match: True\n",
      "Sample 3080 - Lengths Match: True\n",
      "Sample 3081 - Lengths Match: True\n",
      "Sample 3082 - Lengths Match: True\n",
      "Sample 3083 - Lengths Match: True\n",
      "Sample 3084 - Lengths Match: True\n",
      "Sample 3085 - Lengths Match: True\n",
      "Sample 3086 - Lengths Match: True\n",
      "Sample 3087 - Lengths Match: True\n",
      "Sample 3088 - Lengths Match: True\n",
      "Sample 3089 - Lengths Match: True\n",
      "Sample 3090 - Lengths Match: True\n",
      "Sample 3091 - Lengths Match: True\n",
      "Sample 3092 - Lengths Match: True\n",
      "Sample 3093 - Lengths Match: True\n",
      "Sample 3094 - Lengths Match: True\n",
      "Sample 3095 - Lengths Match: True\n",
      "Sample 3096 - Lengths Match: True\n",
      "Sample 3097 - Lengths Match: True\n",
      "Sample 3098 - Lengths Match: True\n",
      "Sample 3099 - Lengths Match: True\n",
      "Sample 3100 - Lengths Match: True\n",
      "Sample 3101 - Lengths Match: True\n",
      "Sample 3102 - Lengths Match: True\n",
      "Sample 3103 - Lengths Match: True\n",
      "Sample 3104 - Lengths Match: True\n",
      "Sample 3105 - Lengths Match: True\n",
      "Sample 3106 - Lengths Match: True\n",
      "Sample 3107 - Lengths Match: True\n",
      "Sample 3108 - Lengths Match: True\n",
      "Sample 3109 - Lengths Match: True\n",
      "Sample 3110 - Lengths Match: True\n",
      "Sample 3111 - Lengths Match: True\n",
      "Sample 3112 - Lengths Match: True\n",
      "Sample 3113 - Lengths Match: True\n",
      "Sample 3114 - Lengths Match: True\n",
      "Sample 3115 - Lengths Match: True\n",
      "Sample 3116 - Lengths Match: True\n",
      "Sample 3117 - Lengths Match: True\n",
      "Sample 3118 - Lengths Match: True\n",
      "Sample 3119 - Lengths Match: True\n",
      "Sample 3120 - Lengths Match: True\n",
      "Sample 3121 - Lengths Match: True\n",
      "Sample 3122 - Lengths Match: True\n",
      "Sample 3123 - Lengths Match: True\n",
      "Sample 3124 - Lengths Match: True\n",
      "Sample 3125 - Lengths Match: True\n",
      "Sample 3126 - Lengths Match: True\n",
      "Sample 3127 - Lengths Match: True\n",
      "Sample 3128 - Lengths Match: True\n",
      "Sample 3129 - Lengths Match: True\n",
      "Sample 3130 - Lengths Match: True\n",
      "Sample 3131 - Lengths Match: True\n",
      "Sample 3132 - Lengths Match: True\n",
      "Sample 3133 - Lengths Match: True\n",
      "Sample 3134 - Lengths Match: True\n",
      "Sample 3135 - Lengths Match: True\n",
      "Sample 3136 - Lengths Match: True\n",
      "Sample 3137 - Lengths Match: True\n",
      "Sample 3138 - Lengths Match: True\n",
      "Sample 3139 - Lengths Match: True\n",
      "Sample 3140 - Lengths Match: True\n",
      "Sample 3141 - Lengths Match: True\n",
      "Sample 3142 - Lengths Match: True\n",
      "Sample 3143 - Lengths Match: True\n",
      "Sample 3144 - Lengths Match: True\n",
      "Sample 3145 - Lengths Match: True\n",
      "Sample 3146 - Lengths Match: True\n",
      "Sample 3147 - Lengths Match: True\n",
      "Sample 3148 - Lengths Match: True\n",
      "Sample 3149 - Lengths Match: True\n",
      "Sample 3150 - Lengths Match: True\n",
      "Sample 3151 - Lengths Match: True\n",
      "Sample 3152 - Lengths Match: True\n",
      "Sample 3153 - Lengths Match: True\n",
      "Sample 3154 - Lengths Match: True\n",
      "Sample 3155 - Lengths Match: True\n",
      "Sample 3156 - Lengths Match: True\n",
      "Sample 3157 - Lengths Match: True\n",
      "Sample 3158 - Lengths Match: True\n",
      "Sample 3159 - Lengths Match: True\n",
      "Sample 3160 - Lengths Match: True\n",
      "Sample 3161 - Lengths Match: True\n",
      "Sample 3162 - Lengths Match: True\n",
      "Sample 3163 - Lengths Match: True\n",
      "Sample 3164 - Lengths Match: True\n",
      "Sample 3165 - Lengths Match: True\n",
      "Sample 3166 - Lengths Match: True\n",
      "Sample 3167 - Lengths Match: True\n",
      "Sample 3168 - Lengths Match: True\n",
      "Sample 3169 - Lengths Match: True\n",
      "Sample 3170 - Lengths Match: True\n",
      "Sample 3171 - Lengths Match: True\n",
      "Sample 3172 - Lengths Match: True\n",
      "Sample 3173 - Lengths Match: True\n",
      "Sample 3174 - Lengths Match: True\n",
      "Sample 3175 - Lengths Match: True\n",
      "Sample 3176 - Lengths Match: True\n",
      "Sample 3177 - Lengths Match: True\n",
      "Sample 3178 - Lengths Match: True\n",
      "Sample 3179 - Lengths Match: True\n",
      "Sample 3180 - Lengths Match: True\n",
      "Sample 3181 - Lengths Match: True\n",
      "Sample 3182 - Lengths Match: True\n",
      "Sample 3183 - Lengths Match: True\n",
      "Sample 3184 - Lengths Match: True\n",
      "Sample 3185 - Lengths Match: True\n",
      "Sample 3186 - Lengths Match: True\n",
      "Sample 3187 - Lengths Match: True\n",
      "Sample 3188 - Lengths Match: True\n",
      "Sample 3189 - Lengths Match: True\n",
      "Sample 3190 - Lengths Match: True\n",
      "Sample 3191 - Lengths Match: True\n",
      "Sample 3192 - Lengths Match: True\n",
      "Sample 3193 - Lengths Match: True\n",
      "Sample 3194 - Lengths Match: True\n",
      "Sample 3195 - Lengths Match: True\n",
      "Sample 3196 - Lengths Match: True\n",
      "Sample 3197 - Lengths Match: True\n",
      "Sample 3198 - Lengths Match: True\n",
      "Sample 3199 - Lengths Match: True\n",
      "Sample 3200 - Lengths Match: True\n",
      "Sample 3201 - Lengths Match: True\n",
      "Sample 3202 - Lengths Match: True\n",
      "Sample 3203 - Lengths Match: True\n",
      "Sample 3204 - Lengths Match: True\n",
      "Sample 3205 - Lengths Match: True\n",
      "Sample 3206 - Lengths Match: True\n",
      "Sample 3207 - Lengths Match: True\n",
      "Sample 3208 - Lengths Match: True\n",
      "Sample 3209 - Lengths Match: True\n",
      "Sample 3210 - Lengths Match: True\n",
      "Sample 3211 - Lengths Match: True\n",
      "Sample 3212 - Lengths Match: True\n",
      "Sample 3213 - Lengths Match: True\n",
      "Sample 3214 - Lengths Match: True\n",
      "Sample 3215 - Lengths Match: True\n",
      "Sample 3216 - Lengths Match: True\n",
      "Sample 3217 - Lengths Match: True\n",
      "Sample 3218 - Lengths Match: True\n",
      "Sample 3219 - Lengths Match: True\n",
      "Sample 3220 - Lengths Match: True\n",
      "Sample 3221 - Lengths Match: True\n",
      "Sample 3222 - Lengths Match: True\n",
      "Sample 3223 - Lengths Match: True\n",
      "Sample 3224 - Lengths Match: True\n",
      "Sample 3225 - Lengths Match: True\n",
      "Sample 3226 - Lengths Match: True\n",
      "Sample 3227 - Lengths Match: True\n",
      "Sample 3228 - Lengths Match: True\n",
      "Sample 3229 - Lengths Match: True\n",
      "Sample 3230 - Lengths Match: True\n",
      "Sample 3231 - Lengths Match: True\n",
      "Sample 3232 - Lengths Match: True\n",
      "Sample 3233 - Lengths Match: True\n",
      "Sample 3234 - Lengths Match: True\n",
      "Sample 3235 - Lengths Match: True\n",
      "Sample 3236 - Lengths Match: True\n",
      "Sample 3237 - Lengths Match: True\n",
      "Sample 3238 - Lengths Match: True\n",
      "Sample 3239 - Lengths Match: True\n",
      "Sample 3240 - Lengths Match: True\n",
      "Sample 3241 - Lengths Match: True\n",
      "Sample 3242 - Lengths Match: True\n",
      "Sample 3243 - Lengths Match: True\n",
      "Sample 3244 - Lengths Match: True\n",
      "Sample 3245 - Lengths Match: True\n",
      "Sample 3246 - Lengths Match: True\n",
      "Sample 3247 - Lengths Match: True\n",
      "Sample 3248 - Lengths Match: True\n",
      "Sample 3249 - Lengths Match: True\n",
      "Sample 3250 - Lengths Match: True\n",
      "Sample 3251 - Lengths Match: True\n",
      "Sample 3252 - Lengths Match: True\n",
      "Sample 3253 - Lengths Match: True\n",
      "Sample 3254 - Lengths Match: True\n",
      "Sample 3255 - Lengths Match: True\n",
      "Sample 3256 - Lengths Match: True\n",
      "Sample 3257 - Lengths Match: True\n",
      "Sample 3258 - Lengths Match: True\n",
      "Sample 3259 - Lengths Match: True\n",
      "Sample 3260 - Lengths Match: True\n",
      "Sample 3261 - Lengths Match: True\n",
      "Sample 3262 - Lengths Match: True\n",
      "Sample 3263 - Lengths Match: True\n",
      "Sample 3264 - Lengths Match: True\n",
      "Sample 3265 - Lengths Match: True\n",
      "Sample 3266 - Lengths Match: True\n",
      "Sample 3267 - Lengths Match: True\n",
      "Sample 3268 - Lengths Match: True\n",
      "Sample 3269 - Lengths Match: True\n",
      "Sample 3270 - Lengths Match: True\n",
      "Sample 3271 - Lengths Match: True\n",
      "Sample 3272 - Lengths Match: True\n",
      "Sample 3273 - Lengths Match: True\n",
      "Sample 3274 - Lengths Match: True\n",
      "Sample 3275 - Lengths Match: True\n",
      "Sample 3276 - Lengths Match: True\n",
      "Sample 3277 - Lengths Match: True\n",
      "Sample 3278 - Lengths Match: True\n",
      "Sample 3279 - Lengths Match: True\n",
      "Sample 3280 - Lengths Match: True\n",
      "Sample 3281 - Lengths Match: True\n",
      "Sample 3282 - Lengths Match: True\n",
      "Sample 3283 - Lengths Match: True\n",
      "Sample 3284 - Lengths Match: True\n",
      "Sample 3285 - Lengths Match: True\n",
      "Sample 3286 - Lengths Match: True\n",
      "Sample 3287 - Lengths Match: True\n",
      "Sample 3288 - Lengths Match: True\n",
      "Sample 3289 - Lengths Match: True\n",
      "Sample 3290 - Lengths Match: True\n",
      "Sample 3291 - Lengths Match: True\n",
      "Sample 3292 - Lengths Match: True\n",
      "Sample 3293 - Lengths Match: True\n",
      "Sample 3294 - Lengths Match: True\n",
      "Sample 3295 - Lengths Match: True\n",
      "Sample 3296 - Lengths Match: True\n",
      "Sample 3297 - Lengths Match: True\n",
      "Sample 3298 - Lengths Match: True\n",
      "Sample 3299 - Lengths Match: True\n",
      "Sample 3300 - Lengths Match: True\n",
      "Sample 3301 - Lengths Match: True\n",
      "Sample 3302 - Lengths Match: True\n",
      "Sample 3303 - Lengths Match: True\n",
      "Sample 3304 - Lengths Match: True\n",
      "Sample 3305 - Lengths Match: True\n",
      "Sample 3306 - Lengths Match: True\n",
      "Sample 3307 - Lengths Match: True\n",
      "Sample 3308 - Lengths Match: True\n",
      "Sample 3309 - Lengths Match: True\n",
      "Sample 3310 - Lengths Match: True\n",
      "Sample 3311 - Lengths Match: True\n",
      "Sample 3312 - Lengths Match: True\n",
      "Sample 3313 - Lengths Match: True\n",
      "Sample 3314 - Lengths Match: True\n",
      "Sample 3315 - Lengths Match: True\n",
      "Sample 3316 - Lengths Match: True\n",
      "Sample 3317 - Lengths Match: True\n",
      "Sample 3318 - Lengths Match: True\n",
      "Sample 3319 - Lengths Match: True\n",
      "Sample 3320 - Lengths Match: True\n",
      "Sample 3321 - Lengths Match: True\n",
      "Sample 3322 - Lengths Match: True\n",
      "Sample 3323 - Lengths Match: True\n",
      "Sample 3324 - Lengths Match: True\n",
      "Sample 3325 - Lengths Match: True\n",
      "Sample 3326 - Lengths Match: True\n",
      "Sample 3327 - Lengths Match: True\n",
      "Sample 3328 - Lengths Match: True\n",
      "Sample 3329 - Lengths Match: True\n",
      "Sample 3330 - Lengths Match: True\n",
      "Sample 3331 - Lengths Match: True\n",
      "Sample 3332 - Lengths Match: True\n",
      "Sample 3333 - Lengths Match: True\n",
      "Sample 3334 - Lengths Match: True\n",
      "Sample 3335 - Lengths Match: True\n",
      "Sample 3336 - Lengths Match: True\n",
      "Sample 3337 - Lengths Match: True\n",
      "Sample 3338 - Lengths Match: True\n",
      "Sample 3339 - Lengths Match: True\n",
      "Sample 3340 - Lengths Match: True\n",
      "Sample 3341 - Lengths Match: True\n",
      "Sample 3342 - Lengths Match: True\n",
      "Sample 3343 - Lengths Match: True\n",
      "Sample 3344 - Lengths Match: True\n",
      "Sample 3345 - Lengths Match: True\n",
      "Sample 3346 - Lengths Match: True\n",
      "Sample 3347 - Lengths Match: True\n",
      "Sample 3348 - Lengths Match: True\n",
      "Sample 3349 - Lengths Match: True\n",
      "Sample 3350 - Lengths Match: True\n",
      "Sample 3351 - Lengths Match: True\n",
      "Sample 3352 - Lengths Match: True\n",
      "Sample 3353 - Lengths Match: True\n",
      "Sample 3354 - Lengths Match: True\n",
      "Sample 3355 - Lengths Match: True\n",
      "Sample 3356 - Lengths Match: True\n",
      "Sample 3357 - Lengths Match: True\n",
      "Sample 3358 - Lengths Match: True\n",
      "Sample 3359 - Lengths Match: True\n",
      "Sample 3360 - Lengths Match: True\n",
      "Sample 3361 - Lengths Match: True\n",
      "Sample 3362 - Lengths Match: True\n",
      "Sample 3363 - Lengths Match: True\n",
      "Sample 3364 - Lengths Match: True\n",
      "Sample 3365 - Lengths Match: True\n",
      "Sample 3366 - Lengths Match: True\n",
      "Sample 3367 - Lengths Match: True\n",
      "Sample 3368 - Lengths Match: True\n",
      "Sample 3369 - Lengths Match: True\n",
      "Sample 3370 - Lengths Match: True\n",
      "Sample 3371 - Lengths Match: True\n",
      "Sample 3372 - Lengths Match: True\n",
      "Sample 3373 - Lengths Match: True\n",
      "Sample 3374 - Lengths Match: True\n",
      "Sample 3375 - Lengths Match: True\n",
      "Sample 3376 - Lengths Match: True\n",
      "Sample 3377 - Lengths Match: True\n",
      "Sample 3378 - Lengths Match: True\n",
      "Sample 3379 - Lengths Match: True\n",
      "Sample 3380 - Lengths Match: True\n",
      "Sample 3381 - Lengths Match: True\n",
      "Sample 3382 - Lengths Match: True\n",
      "Sample 3383 - Lengths Match: True\n",
      "Sample 3384 - Lengths Match: True\n",
      "Sample 3385 - Lengths Match: True\n",
      "Sample 3386 - Lengths Match: True\n",
      "Sample 3387 - Lengths Match: True\n",
      "Sample 3388 - Lengths Match: True\n",
      "Sample 3389 - Lengths Match: True\n",
      "Sample 3390 - Lengths Match: True\n",
      "Sample 3391 - Lengths Match: True\n",
      "Sample 3392 - Lengths Match: True\n",
      "Sample 3393 - Lengths Match: True\n",
      "Sample 3394 - Lengths Match: True\n",
      "Sample 3395 - Lengths Match: True\n",
      "Sample 3396 - Lengths Match: True\n",
      "Sample 3397 - Lengths Match: True\n",
      "Sample 3398 - Lengths Match: True\n",
      "Sample 3399 - Lengths Match: True\n",
      "Sample 3400 - Lengths Match: True\n",
      "Sample 3401 - Lengths Match: True\n",
      "Sample 3402 - Lengths Match: True\n",
      "Sample 3403 - Lengths Match: True\n",
      "Sample 3404 - Lengths Match: True\n",
      "Sample 3405 - Lengths Match: True\n",
      "Sample 3406 - Lengths Match: True\n",
      "Sample 3407 - Lengths Match: True\n",
      "Sample 3408 - Lengths Match: True\n",
      "Sample 3409 - Lengths Match: True\n",
      "Sample 3410 - Lengths Match: True\n",
      "Sample 3411 - Lengths Match: True\n",
      "Sample 3412 - Lengths Match: True\n",
      "Sample 3413 - Lengths Match: True\n",
      "Sample 3414 - Lengths Match: True\n",
      "Sample 3415 - Lengths Match: True\n",
      "Sample 3416 - Lengths Match: True\n",
      "Sample 3417 - Lengths Match: True\n",
      "Sample 3418 - Lengths Match: True\n",
      "Sample 3419 - Lengths Match: True\n",
      "Sample 3420 - Lengths Match: True\n",
      "Sample 3421 - Lengths Match: True\n",
      "Sample 3422 - Lengths Match: True\n",
      "Sample 3423 - Lengths Match: True\n",
      "Sample 3424 - Lengths Match: True\n",
      "Sample 3425 - Lengths Match: True\n",
      "Sample 3426 - Lengths Match: True\n",
      "Sample 3427 - Lengths Match: True\n",
      "Sample 3428 - Lengths Match: True\n",
      "Sample 3429 - Lengths Match: True\n",
      "Sample 3430 - Lengths Match: True\n",
      "Sample 3431 - Lengths Match: True\n",
      "Sample 3432 - Lengths Match: True\n",
      "Sample 3433 - Lengths Match: True\n",
      "Sample 3434 - Lengths Match: True\n",
      "Sample 3435 - Lengths Match: True\n",
      "Sample 3436 - Lengths Match: True\n",
      "Sample 3437 - Lengths Match: True\n",
      "Sample 3438 - Lengths Match: True\n",
      "Sample 3439 - Lengths Match: True\n",
      "Sample 3440 - Lengths Match: True\n",
      "Sample 3441 - Lengths Match: True\n",
      "Sample 3442 - Lengths Match: True\n",
      "Sample 3443 - Lengths Match: True\n",
      "Sample 3444 - Lengths Match: True\n",
      "Sample 3445 - Lengths Match: True\n",
      "Sample 3446 - Lengths Match: True\n",
      "Sample 3447 - Lengths Match: True\n",
      "Sample 3448 - Lengths Match: True\n",
      "Sample 3449 - Lengths Match: True\n",
      "Sample 3450 - Lengths Match: True\n",
      "Sample 3451 - Lengths Match: True\n",
      "Sample 3452 - Lengths Match: True\n",
      "Sample 3453 - Lengths Match: True\n",
      "Sample 3454 - Lengths Match: True\n",
      "Sample 3455 - Lengths Match: True\n",
      "Sample 3456 - Lengths Match: True\n",
      "Sample 3457 - Lengths Match: True\n",
      "Sample 3458 - Lengths Match: True\n",
      "Sample 3459 - Lengths Match: True\n",
      "Sample 3460 - Lengths Match: True\n",
      "Sample 3461 - Lengths Match: True\n",
      "Sample 3462 - Lengths Match: True\n",
      "Sample 3463 - Lengths Match: True\n",
      "Sample 3464 - Lengths Match: True\n",
      "Sample 3465 - Lengths Match: True\n",
      "Sample 3466 - Lengths Match: True\n",
      "Sample 3467 - Lengths Match: True\n",
      "Sample 3468 - Lengths Match: True\n",
      "Sample 3469 - Lengths Match: True\n",
      "Sample 3470 - Lengths Match: True\n",
      "Sample 3471 - Lengths Match: True\n",
      "Sample 3472 - Lengths Match: True\n",
      "Sample 3473 - Lengths Match: True\n",
      "Sample 3474 - Lengths Match: True\n",
      "Sample 3475 - Lengths Match: True\n",
      "Sample 3476 - Lengths Match: True\n",
      "Sample 3477 - Lengths Match: True\n",
      "Sample 3478 - Lengths Match: True\n",
      "Sample 3479 - Lengths Match: True\n",
      "Sample 3480 - Lengths Match: True\n",
      "Sample 3481 - Lengths Match: True\n",
      "Sample 3482 - Lengths Match: True\n",
      "Sample 3483 - Lengths Match: True\n",
      "Sample 3484 - Lengths Match: True\n",
      "Sample 3485 - Lengths Match: True\n",
      "Sample 3486 - Lengths Match: True\n",
      "Sample 3487 - Lengths Match: True\n",
      "Sample 3488 - Lengths Match: True\n",
      "Sample 3489 - Lengths Match: True\n",
      "Sample 3490 - Lengths Match: True\n",
      "Sample 3491 - Lengths Match: True\n",
      "Sample 3492 - Lengths Match: True\n",
      "Sample 3493 - Lengths Match: True\n",
      "Sample 3494 - Lengths Match: True\n",
      "Sample 3495 - Lengths Match: True\n",
      "Sample 3496 - Lengths Match: True\n",
      "Sample 3497 - Lengths Match: True\n",
      "Sample 3498 - Lengths Match: True\n",
      "Sample 3499 - Lengths Match: True\n",
      "Sample 3500 - Lengths Match: True\n",
      "Sample 3501 - Lengths Match: True\n",
      "Sample 3502 - Lengths Match: True\n",
      "Sample 3503 - Lengths Match: True\n",
      "Sample 3504 - Lengths Match: True\n",
      "Sample 3505 - Lengths Match: True\n",
      "Sample 3506 - Lengths Match: True\n",
      "Sample 3507 - Lengths Match: True\n",
      "Sample 3508 - Lengths Match: True\n",
      "Sample 3509 - Lengths Match: True\n",
      "Sample 3510 - Lengths Match: True\n",
      "Sample 3511 - Lengths Match: True\n",
      "Sample 3512 - Lengths Match: True\n",
      "Sample 3513 - Lengths Match: True\n",
      "Sample 3514 - Lengths Match: True\n",
      "Sample 3515 - Lengths Match: True\n",
      "Sample 3516 - Lengths Match: True\n",
      "Sample 3517 - Lengths Match: True\n",
      "Sample 3518 - Lengths Match: True\n",
      "Sample 3519 - Lengths Match: True\n",
      "Sample 3520 - Lengths Match: True\n",
      "Sample 3521 - Lengths Match: True\n",
      "Sample 3522 - Lengths Match: True\n",
      "Sample 3523 - Lengths Match: True\n",
      "Sample 3524 - Lengths Match: True\n",
      "Sample 3525 - Lengths Match: True\n",
      "Sample 3526 - Lengths Match: True\n",
      "Sample 3527 - Lengths Match: True\n",
      "Sample 3528 - Lengths Match: True\n",
      "Sample 3529 - Lengths Match: True\n",
      "Sample 3530 - Lengths Match: True\n",
      "Sample 3531 - Lengths Match: True\n",
      "Sample 3532 - Lengths Match: True\n",
      "Sample 3533 - Lengths Match: True\n",
      "Sample 3534 - Lengths Match: True\n",
      "Sample 3535 - Lengths Match: True\n",
      "Sample 3536 - Lengths Match: True\n",
      "Sample 3537 - Lengths Match: True\n",
      "Sample 3538 - Lengths Match: True\n",
      "Sample 3539 - Lengths Match: True\n",
      "Sample 3540 - Lengths Match: True\n",
      "Sample 3541 - Lengths Match: True\n",
      "Sample 3542 - Lengths Match: True\n",
      "Sample 3543 - Lengths Match: True\n",
      "Sample 3544 - Lengths Match: True\n",
      "Sample 3545 - Lengths Match: True\n",
      "Sample 3546 - Lengths Match: True\n",
      "Sample 3547 - Lengths Match: True\n",
      "Sample 3548 - Lengths Match: True\n",
      "Sample 3549 - Lengths Match: True\n",
      "Sample 3550 - Lengths Match: True\n",
      "Sample 3551 - Lengths Match: True\n",
      "Sample 3552 - Lengths Match: True\n",
      "Sample 3553 - Lengths Match: True\n",
      "Sample 3554 - Lengths Match: True\n",
      "Sample 3555 - Lengths Match: True\n",
      "Sample 3556 - Lengths Match: True\n",
      "Sample 3557 - Lengths Match: True\n",
      "Sample 3558 - Lengths Match: True\n",
      "Sample 3559 - Lengths Match: True\n",
      "Sample 3560 - Lengths Match: True\n",
      "Sample 3561 - Lengths Match: True\n",
      "Sample 3562 - Lengths Match: True\n",
      "Sample 3563 - Lengths Match: True\n",
      "Sample 3564 - Lengths Match: True\n",
      "Sample 3565 - Lengths Match: True\n",
      "Sample 3566 - Lengths Match: True\n",
      "Sample 3567 - Lengths Match: True\n",
      "Sample 3568 - Lengths Match: True\n",
      "Sample 3569 - Lengths Match: True\n",
      "Sample 3570 - Lengths Match: True\n",
      "Sample 3571 - Lengths Match: True\n",
      "Sample 3572 - Lengths Match: True\n",
      "Sample 3573 - Lengths Match: True\n",
      "Sample 3574 - Lengths Match: True\n",
      "Sample 3575 - Lengths Match: True\n",
      "Sample 3576 - Lengths Match: True\n",
      "Sample 3577 - Lengths Match: True\n",
      "Sample 3578 - Lengths Match: True\n",
      "Sample 3579 - Lengths Match: True\n",
      "Sample 3580 - Lengths Match: True\n",
      "Sample 3581 - Lengths Match: True\n",
      "Sample 3582 - Lengths Match: True\n",
      "Sample 3583 - Lengths Match: True\n",
      "Sample 3584 - Lengths Match: True\n",
      "Sample 3585 - Lengths Match: True\n",
      "Sample 3586 - Lengths Match: True\n",
      "Sample 3587 - Lengths Match: True\n",
      "Sample 3588 - Lengths Match: True\n",
      "Sample 3589 - Lengths Match: True\n",
      "Sample 3590 - Lengths Match: True\n",
      "Sample 3591 - Lengths Match: True\n",
      "Sample 3592 - Lengths Match: True\n",
      "Sample 3593 - Lengths Match: True\n",
      "Sample 3594 - Lengths Match: True\n",
      "Sample 3595 - Lengths Match: True\n",
      "Sample 3596 - Lengths Match: True\n",
      "Sample 3597 - Lengths Match: True\n",
      "Sample 3598 - Lengths Match: True\n",
      "Sample 3599 - Lengths Match: True\n",
      "Sample 3600 - Lengths Match: True\n",
      "Sample 3601 - Lengths Match: True\n",
      "Sample 3602 - Lengths Match: True\n",
      "Sample 3603 - Lengths Match: True\n",
      "Sample 3604 - Lengths Match: True\n",
      "Sample 3605 - Lengths Match: True\n",
      "Sample 3606 - Lengths Match: True\n",
      "Sample 3607 - Lengths Match: True\n",
      "Sample 3608 - Lengths Match: True\n",
      "Sample 3609 - Lengths Match: True\n",
      "Sample 3610 - Lengths Match: True\n",
      "Sample 3611 - Lengths Match: True\n",
      "Sample 3612 - Lengths Match: True\n",
      "Sample 3613 - Lengths Match: True\n",
      "Sample 3614 - Lengths Match: True\n",
      "Sample 3615 - Lengths Match: True\n",
      "Sample 3616 - Lengths Match: True\n",
      "Sample 3617 - Lengths Match: True\n",
      "Sample 3618 - Lengths Match: True\n",
      "Sample 3619 - Lengths Match: True\n",
      "Sample 3620 - Lengths Match: True\n",
      "Sample 3621 - Lengths Match: True\n",
      "Sample 3622 - Lengths Match: True\n",
      "Sample 3623 - Lengths Match: True\n",
      "Sample 3624 - Lengths Match: True\n",
      "Sample 3625 - Lengths Match: True\n",
      "Sample 3626 - Lengths Match: True\n",
      "Sample 3627 - Lengths Match: True\n",
      "Sample 3628 - Lengths Match: True\n",
      "Sample 3629 - Lengths Match: True\n",
      "Sample 3630 - Lengths Match: True\n",
      "Sample 3631 - Lengths Match: True\n",
      "Sample 3632 - Lengths Match: True\n",
      "Sample 3633 - Lengths Match: True\n",
      "Sample 3634 - Lengths Match: True\n",
      "Sample 3635 - Lengths Match: True\n",
      "Sample 3636 - Lengths Match: True\n",
      "Sample 3637 - Lengths Match: True\n",
      "Sample 3638 - Lengths Match: True\n",
      "Sample 3639 - Lengths Match: True\n",
      "Sample 3640 - Lengths Match: True\n",
      "Sample 3641 - Lengths Match: True\n",
      "Sample 3642 - Lengths Match: True\n",
      "Sample 3643 - Lengths Match: True\n",
      "Sample 3644 - Lengths Match: True\n",
      "Sample 3645 - Lengths Match: True\n",
      "Sample 3646 - Lengths Match: True\n",
      "Sample 3647 - Lengths Match: True\n",
      "Sample 3648 - Lengths Match: True\n",
      "Sample 3649 - Lengths Match: True\n",
      "Sample 3650 - Lengths Match: True\n",
      "Sample 3651 - Lengths Match: True\n",
      "Sample 3652 - Lengths Match: True\n",
      "Sample 3653 - Lengths Match: True\n",
      "Sample 3654 - Lengths Match: True\n",
      "Sample 3655 - Lengths Match: True\n",
      "Sample 3656 - Lengths Match: True\n",
      "Sample 3657 - Lengths Match: True\n",
      "Sample 3658 - Lengths Match: True\n",
      "Sample 3659 - Lengths Match: True\n",
      "Sample 3660 - Lengths Match: True\n",
      "Sample 3661 - Lengths Match: True\n",
      "Sample 3662 - Lengths Match: True\n",
      "Sample 3663 - Lengths Match: True\n",
      "Sample 3664 - Lengths Match: True\n",
      "Sample 3665 - Lengths Match: True\n",
      "Sample 3666 - Lengths Match: True\n",
      "Sample 3667 - Lengths Match: True\n",
      "Sample 3668 - Lengths Match: True\n",
      "Sample 3669 - Lengths Match: True\n",
      "Sample 3670 - Lengths Match: True\n",
      "Sample 3671 - Lengths Match: True\n",
      "Sample 3672 - Lengths Match: True\n",
      "Sample 3673 - Lengths Match: True\n",
      "Sample 3674 - Lengths Match: True\n",
      "Sample 3675 - Lengths Match: True\n",
      "Sample 3676 - Lengths Match: True\n",
      "Sample 3677 - Lengths Match: True\n",
      "Sample 3678 - Lengths Match: True\n",
      "Sample 3679 - Lengths Match: True\n",
      "Sample 3680 - Lengths Match: True\n",
      "Sample 3681 - Lengths Match: True\n",
      "Sample 3682 - Lengths Match: True\n",
      "Sample 3683 - Lengths Match: True\n",
      "Sample 3684 - Lengths Match: True\n",
      "Sample 3685 - Lengths Match: True\n",
      "Sample 3686 - Lengths Match: True\n",
      "Sample 3687 - Lengths Match: True\n",
      "Sample 3688 - Lengths Match: True\n",
      "Sample 3689 - Lengths Match: True\n",
      "Sample 3690 - Lengths Match: True\n",
      "Sample 3691 - Lengths Match: True\n",
      "Sample 3692 - Lengths Match: True\n",
      "Sample 3693 - Lengths Match: True\n",
      "Sample 3694 - Lengths Match: True\n",
      "Sample 3695 - Lengths Match: True\n",
      "Sample 3696 - Lengths Match: True\n",
      "Sample 3697 - Lengths Match: True\n",
      "Sample 3698 - Lengths Match: True\n",
      "Sample 3699 - Lengths Match: True\n",
      "Sample 3700 - Lengths Match: True\n",
      "Sample 3701 - Lengths Match: True\n",
      "Sample 3702 - Lengths Match: True\n",
      "Sample 3703 - Lengths Match: True\n",
      "Sample 3704 - Lengths Match: True\n",
      "Sample 3705 - Lengths Match: True\n",
      "Sample 3706 - Lengths Match: True\n",
      "Sample 3707 - Lengths Match: True\n",
      "Sample 3708 - Lengths Match: True\n",
      "Sample 3709 - Lengths Match: True\n",
      "Sample 3710 - Lengths Match: True\n",
      "Sample 3711 - Lengths Match: True\n",
      "Sample 3712 - Lengths Match: True\n",
      "Sample 3713 - Lengths Match: True\n",
      "Sample 3714 - Lengths Match: True\n",
      "Sample 3715 - Lengths Match: True\n",
      "Sample 3716 - Lengths Match: True\n",
      "Sample 3717 - Lengths Match: True\n",
      "Sample 3718 - Lengths Match: True\n",
      "Sample 3719 - Lengths Match: True\n",
      "Sample 3720 - Lengths Match: True\n",
      "Sample 3721 - Lengths Match: True\n",
      "Sample 3722 - Lengths Match: True\n",
      "Sample 3723 - Lengths Match: True\n",
      "Sample 3724 - Lengths Match: True\n",
      "Sample 3725 - Lengths Match: True\n",
      "Sample 3726 - Lengths Match: True\n",
      "Sample 3727 - Lengths Match: True\n",
      "Sample 3728 - Lengths Match: True\n",
      "Sample 3729 - Lengths Match: True\n",
      "Sample 3730 - Lengths Match: True\n",
      "Sample 3731 - Lengths Match: True\n",
      "Sample 3732 - Lengths Match: True\n",
      "Sample 3733 - Lengths Match: True\n",
      "Sample 3734 - Lengths Match: True\n",
      "Sample 3735 - Lengths Match: True\n",
      "Sample 3736 - Lengths Match: True\n",
      "Sample 3737 - Lengths Match: True\n",
      "Sample 3738 - Lengths Match: True\n",
      "Sample 3739 - Lengths Match: True\n",
      "Sample 3740 - Lengths Match: True\n",
      "Sample 3741 - Lengths Match: True\n",
      "Sample 3742 - Lengths Match: True\n",
      "Sample 3743 - Lengths Match: True\n",
      "Sample 3744 - Lengths Match: True\n",
      "Sample 3745 - Lengths Match: True\n",
      "Sample 3746 - Lengths Match: True\n",
      "Sample 3747 - Lengths Match: True\n",
      "Sample 3748 - Lengths Match: True\n",
      "Sample 3749 - Lengths Match: True\n",
      "Sample 3750 - Lengths Match: True\n",
      "Sample 3751 - Lengths Match: True\n",
      "Sample 3752 - Lengths Match: True\n",
      "Sample 3753 - Lengths Match: True\n",
      "Sample 3754 - Lengths Match: True\n",
      "Sample 3755 - Lengths Match: True\n",
      "Sample 3756 - Lengths Match: True\n",
      "Sample 3757 - Lengths Match: True\n",
      "Sample 3758 - Lengths Match: True\n",
      "Sample 3759 - Lengths Match: True\n",
      "Sample 3760 - Lengths Match: True\n",
      "Sample 3761 - Lengths Match: True\n",
      "Sample 3762 - Lengths Match: True\n",
      "Sample 3763 - Lengths Match: True\n",
      "Sample 3764 - Lengths Match: True\n",
      "Sample 3765 - Lengths Match: True\n",
      "Sample 3766 - Lengths Match: True\n",
      "Sample 3767 - Lengths Match: True\n",
      "Sample 3768 - Lengths Match: True\n",
      "Sample 3769 - Lengths Match: True\n",
      "Sample 3770 - Lengths Match: True\n",
      "Sample 3771 - Lengths Match: True\n",
      "Sample 3772 - Lengths Match: True\n",
      "Sample 3773 - Lengths Match: True\n",
      "Sample 3774 - Lengths Match: True\n",
      "Sample 3775 - Lengths Match: True\n",
      "Sample 3776 - Lengths Match: True\n",
      "Sample 3777 - Lengths Match: True\n",
      "Sample 3778 - Lengths Match: True\n",
      "Sample 3779 - Lengths Match: True\n",
      "Sample 3780 - Lengths Match: True\n",
      "Sample 3781 - Lengths Match: True\n",
      "Sample 3782 - Lengths Match: True\n",
      "Sample 3783 - Lengths Match: True\n",
      "Sample 3784 - Lengths Match: True\n",
      "Sample 3785 - Lengths Match: True\n",
      "Sample 3786 - Lengths Match: True\n",
      "Sample 3787 - Lengths Match: True\n",
      "Sample 3788 - Lengths Match: True\n",
      "Sample 3789 - Lengths Match: True\n",
      "Sample 3790 - Lengths Match: True\n",
      "Sample 3791 - Lengths Match: True\n",
      "Sample 3792 - Lengths Match: True\n",
      "Sample 3793 - Lengths Match: True\n",
      "Sample 3794 - Lengths Match: True\n",
      "Sample 3795 - Lengths Match: True\n",
      "Sample 3796 - Lengths Match: True\n",
      "Sample 3797 - Lengths Match: True\n",
      "Sample 3798 - Lengths Match: True\n",
      "Sample 3799 - Lengths Match: True\n",
      "Sample 3800 - Lengths Match: True\n",
      "Sample 3801 - Lengths Match: True\n",
      "Sample 3802 - Lengths Match: True\n",
      "Sample 3803 - Lengths Match: True\n",
      "Sample 3804 - Lengths Match: True\n",
      "Sample 3805 - Lengths Match: True\n",
      "Sample 3806 - Lengths Match: True\n",
      "Sample 3807 - Lengths Match: True\n",
      "Sample 3808 - Lengths Match: True\n",
      "Sample 3809 - Lengths Match: True\n",
      "Sample 3810 - Lengths Match: True\n",
      "Sample 3811 - Lengths Match: True\n",
      "Sample 3812 - Lengths Match: True\n",
      "Sample 3813 - Lengths Match: True\n",
      "Sample 3814 - Lengths Match: True\n",
      "Sample 3815 - Lengths Match: True\n",
      "Sample 3816 - Lengths Match: True\n",
      "Sample 3817 - Lengths Match: True\n",
      "Sample 3818 - Lengths Match: True\n",
      "Sample 3819 - Lengths Match: True\n",
      "Sample 3820 - Lengths Match: True\n",
      "Sample 3821 - Lengths Match: True\n",
      "Sample 3822 - Lengths Match: True\n",
      "Sample 3823 - Lengths Match: True\n",
      "Sample 3824 - Lengths Match: True\n",
      "Sample 3825 - Lengths Match: True\n",
      "Sample 3826 - Lengths Match: True\n",
      "Sample 3827 - Lengths Match: True\n",
      "Sample 3828 - Lengths Match: True\n",
      "Sample 3829 - Lengths Match: True\n",
      "Sample 3830 - Lengths Match: True\n",
      "Sample 3831 - Lengths Match: True\n",
      "Sample 3832 - Lengths Match: True\n",
      "Sample 3833 - Lengths Match: True\n",
      "Sample 3834 - Lengths Match: True\n",
      "Sample 3835 - Lengths Match: True\n",
      "Sample 3836 - Lengths Match: True\n",
      "Sample 3837 - Lengths Match: True\n",
      "Sample 3838 - Lengths Match: True\n",
      "Sample 3839 - Lengths Match: True\n",
      "Sample 3840 - Lengths Match: True\n",
      "Sample 3841 - Lengths Match: True\n",
      "Sample 3842 - Lengths Match: True\n",
      "Sample 3843 - Lengths Match: True\n",
      "Sample 3844 - Lengths Match: True\n",
      "Sample 3845 - Lengths Match: True\n",
      "Sample 3846 - Lengths Match: True\n",
      "Sample 3847 - Lengths Match: True\n",
      "Sample 3848 - Lengths Match: True\n",
      "Sample 3849 - Lengths Match: True\n",
      "Sample 3850 - Lengths Match: True\n",
      "Sample 3851 - Lengths Match: True\n",
      "Sample 3852 - Lengths Match: True\n",
      "Sample 3853 - Lengths Match: True\n",
      "Sample 3854 - Lengths Match: True\n",
      "Sample 3855 - Lengths Match: True\n",
      "Sample 3856 - Lengths Match: True\n",
      "Sample 3857 - Lengths Match: True\n",
      "Sample 3858 - Lengths Match: True\n",
      "Sample 3859 - Lengths Match: True\n",
      "Sample 3860 - Lengths Match: True\n",
      "Sample 3861 - Lengths Match: True\n",
      "Sample 3862 - Lengths Match: True\n",
      "Sample 3863 - Lengths Match: True\n",
      "Sample 3864 - Lengths Match: True\n",
      "Sample 3865 - Lengths Match: True\n",
      "Sample 3866 - Lengths Match: True\n",
      "Sample 3867 - Lengths Match: True\n",
      "Sample 3868 - Lengths Match: True\n",
      "Sample 3869 - Lengths Match: True\n",
      "Sample 3870 - Lengths Match: True\n",
      "Sample 3871 - Lengths Match: True\n",
      "Sample 3872 - Lengths Match: True\n",
      "Sample 3873 - Lengths Match: True\n",
      "Sample 3874 - Lengths Match: True\n",
      "Sample 3875 - Lengths Match: True\n",
      "Sample 3876 - Lengths Match: True\n",
      "Sample 3877 - Lengths Match: True\n",
      "Sample 3878 - Lengths Match: True\n",
      "Sample 3879 - Lengths Match: True\n",
      "Sample 3880 - Lengths Match: True\n",
      "Sample 3881 - Lengths Match: True\n",
      "Sample 3882 - Lengths Match: True\n",
      "Sample 3883 - Lengths Match: True\n",
      "Sample 3884 - Lengths Match: True\n",
      "Sample 3885 - Lengths Match: True\n",
      "Sample 3886 - Lengths Match: True\n",
      "Sample 3887 - Lengths Match: True\n",
      "Sample 3888 - Lengths Match: True\n",
      "Sample 3889 - Lengths Match: True\n",
      "Sample 3890 - Lengths Match: True\n",
      "Sample 3891 - Lengths Match: True\n",
      "Sample 3892 - Lengths Match: True\n",
      "Sample 3893 - Lengths Match: True\n",
      "Sample 3894 - Lengths Match: True\n",
      "Sample 3895 - Lengths Match: True\n",
      "Sample 3896 - Lengths Match: True\n",
      "Sample 3897 - Lengths Match: True\n",
      "Sample 3898 - Lengths Match: True\n",
      "Sample 3899 - Lengths Match: True\n",
      "Sample 3900 - Lengths Match: True\n",
      "Sample 3901 - Lengths Match: True\n",
      "Sample 3902 - Lengths Match: True\n",
      "Sample 3903 - Lengths Match: True\n",
      "Sample 3904 - Lengths Match: True\n",
      "Sample 3905 - Lengths Match: True\n",
      "Sample 3906 - Lengths Match: True\n",
      "Sample 3907 - Lengths Match: True\n",
      "Sample 3908 - Lengths Match: True\n",
      "Sample 3909 - Lengths Match: True\n",
      "Sample 3910 - Lengths Match: True\n",
      "Sample 3911 - Lengths Match: True\n",
      "Sample 3912 - Lengths Match: True\n",
      "Sample 3913 - Lengths Match: True\n",
      "Sample 3914 - Lengths Match: True\n",
      "Sample 3915 - Lengths Match: True\n",
      "Sample 3916 - Lengths Match: True\n",
      "Sample 3917 - Lengths Match: True\n",
      "Sample 3918 - Lengths Match: True\n",
      "Sample 3919 - Lengths Match: True\n",
      "Sample 3920 - Lengths Match: True\n",
      "Sample 3921 - Lengths Match: True\n",
      "Sample 3922 - Lengths Match: True\n",
      "Sample 3923 - Lengths Match: True\n",
      "Sample 3924 - Lengths Match: True\n",
      "Sample 3925 - Lengths Match: True\n",
      "Sample 3926 - Lengths Match: True\n",
      "Sample 3927 - Lengths Match: True\n",
      "Sample 3928 - Lengths Match: True\n",
      "Sample 3929 - Lengths Match: True\n",
      "Sample 3930 - Lengths Match: True\n",
      "Sample 3931 - Lengths Match: True\n",
      "Sample 3932 - Lengths Match: True\n",
      "Sample 3933 - Lengths Match: True\n",
      "Sample 3934 - Lengths Match: True\n",
      "Sample 3935 - Lengths Match: True\n",
      "Indices causing IndexError: []\n"
     ]
    }
   ],
   "source": [
    "def check_token_lengths(dataset):\n",
    "    error_indices = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        try:\n",
    "            bert_tokens, ids_tensor, tags_tensor = dataset[idx]\n",
    "\n",
    "            is_length_match = len(bert_tokens) == len(ids_tensor) == len(tags_tensor)\n",
    "\n",
    "            print(f\"Sample {idx + 1} - Lengths Match: {is_length_match}\")\n",
    "        except IndexError:\n",
    "            print(f\"IndexError at index {idx}\")\n",
    "            error_indices.append(idx)\n",
    "\n",
    "    return error_indices\n",
    "\n",
    "# Check for the training dataset\n",
    "error_indices = check_token_lengths(review_train_ds)\n",
    "\n",
    "print(\"Indices causing IndexError:\", error_indices)\n",
    "\n",
    "\n",
    "\n",
    "# Check for the test dataset\n",
    "# check_token_lengths(review_test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the existence of indices before removal\n",
    "for idx in error_indices:\n",
    "    if idx < len(review_train_ds):\n",
    "        print(f\"Index {idx} exists in the dataset.\")\n",
    "    else:\n",
    "        print(f\"Index {idx} is out of range for the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic indices\n",
    "indices_to_remove = error_indices.copy()  # Ensure a copy to avoid modifying the original list\n",
    "\n",
    "# Create a new instance of dataset_ATM without the problematic indices\n",
    "cleaned_review_train_ds = dataset_ATM(\n",
    "    [review_train_ds[i] for i in range(len(review_train_ds)) if i not in indices_to_remove],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Check for the cleaned training dataset\n",
    "check_token_lengths(cleaned_review_train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic indices\n",
    "indices_to_remove = error_indices\n",
    "\n",
    "# Create a new instance of dataset_ATM without the problematic indices\n",
    "cleaned_review_train_ds = dataset_ATM(\n",
    "    [review_train_ds[i] for i in range(len(review_train_ds)) if i not in indices_to_remove],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Check for the cleaned training dataset\n",
    "check_token_lengths(cleaned_review_train_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Tokens: ['when', 'it', 'arrives', 'around', 'before', 'dusk', ',', 'the', 'line', 'is', 'not', 'long', ',', 'but', 'here', 'is', 'a', 'sur', '##au', ',', 'do', 'not', 'worry', '.']\n",
      "Token IDs: tensor([ 2043,  2009,  8480,  2105,  2077, 18406,  1010,  1996,  2240,  2003,\n",
      "         2025,  2146,  1010,  2021,  2182,  2003,  1037,  7505,  4887,  1010,\n",
      "         2079,  2025,  4737,  1012])\n",
      "Tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "Tokens: ['when', 'it', 'arrives', 'around', 'before', 'dusk', ',', 'the', 'line', 'is', 'not', 'long', ',', 'but', 'here', 'is', 'a', 'sur', '##au', ',', 'do', 'not', 'worry', '.']\n",
      "Token IDs: tensor([ 2043,  2009,  8480,  2105,  2077, 18406,  1010,  1996,  2240,  2003,\n",
      "         2025,  2146,  1010,  2021,  2182,  2003,  1037,  7505,  4887,  1010,\n",
      "         2079,  2025,  4737,  1012])\n",
      "Tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "Tokens: ['stream', '##line', 'food', 'ordering', ',', 'fast', 'moving', 'lane', '.', 'self', 'service', '.', 'spacious', 'dining', 'area', ',', 'can', 'pay', 'extra', 'for', 'ac', 'space', '.', 'food', 'is', 'above', 'average', '.', 'took', 'chicken', ',', 'cock', '##les', ',', 'squid', 'with', 'a', 'syrup', 'drink', 'is', 'rm', '22', '.', 'you', 'can', 'get', 'free', 'filter', 'water', 'at', 'the', 'hand', 'wash']\n",
      "Token IDs: tensor([ 5460,  4179,  2833, 13063,  1010,  3435,  3048,  4644,  1012,  2969,\n",
      "         2326,  1012, 22445,  7759,  2181,  1010,  2064,  3477,  4469,  2005,\n",
      "         9353,  2686,  1012,  2833,  2003,  2682,  2779,  1012,  2165,  7975,\n",
      "         1010, 10338,  4244,  1010, 26852,  2007,  1037, 23353,  4392,  2003,\n",
      "        28549,  2570,  1012,  2017,  2064,  2131,  2489, 11307,  2300,  2012,\n",
      "         1996,  2192,  9378])\n",
      "Tags: tensor([0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "Sample 4:\n",
      "Tokens: ['stream', '##line', 'food', 'ordering', ',', 'fast', 'moving', 'lane', '.', 'self', 'service', '.', 'spacious', 'dining', 'area', ',', 'can', 'pay', 'extra', 'for', 'ac', 'space', '.', 'food', 'is', 'above', 'average', '.', 'took', 'chicken', ',', 'cock', '##les', ',', 'squid', 'with', 'a', 'syrup', 'drink', 'is', 'rm', '22', '.', 'you', 'can', 'get', 'free', 'filter', 'water', 'at', 'the', 'hand', 'wash']\n",
      "Token IDs: tensor([ 5460,  4179,  2833, 13063,  1010,  3435,  3048,  4644,  1012,  2969,\n",
      "         2326,  1012, 22445,  7759,  2181,  1010,  2064,  3477,  4469,  2005,\n",
      "         9353,  2686,  1012,  2833,  2003,  2682,  2779,  1012,  2165,  7975,\n",
      "         1010, 10338,  4244,  1010, 26852,  2007,  1037, 23353,  4392,  2003,\n",
      "        28549,  2570,  1012,  2017,  2064,  2131,  2489, 11307,  2300,  2012,\n",
      "         1996,  2192,  9378])\n",
      "Tags: tensor([0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "Sample 5:\n",
      "Tokens: ['stream', '##line', 'food', 'ordering', ',', 'fast', 'moving', 'lane', '.', 'self', 'service', '.', 'spacious', 'dining', 'area', ',', 'can', 'pay', 'extra', 'for', 'ac', 'space', '.', 'food', 'is', 'above', 'average', '.', 'took', 'chicken', ',', 'cock', '##les', ',', 'squid', 'with', 'a', 'syrup', 'drink', 'is', 'rm', '22', '.', 'you', 'can', 'get', 'free', 'filter', 'water', 'at', 'the', 'hand', 'wash']\n",
      "Token IDs: tensor([ 5460,  4179,  2833, 13063,  1010,  3435,  3048,  4644,  1012,  2969,\n",
      "         2326,  1012, 22445,  7759,  2181,  1010,  2064,  3477,  4469,  2005,\n",
      "         9353,  2686,  1012,  2833,  2003,  2682,  2779,  1012,  2165,  7975,\n",
      "         1010, 10338,  4244,  1010, 26852,  2007,  1037, 23353,  4392,  2003,\n",
      "        28549,  2570,  1012,  2017,  2064,  2131,  2489, 11307,  2300,  2012,\n",
      "         1996,  2192,  9378])\n",
      "Tags: tensor([0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2, 0, 0, 0, 0])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the first 5 samples\n",
    "for i in range(5):\n",
    "    sample = review_train_ds[i]\n",
    "    tokens, ids_tensor, tags_tensor = sample\n",
    "\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", ids_tensor)\n",
    "    print(\"Tags:\", tags_tensor)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'place', 'was', 'okay', 'but', 'the', 'food', 'were', 'under', '##w', '##helm', '##ing', '.', 'as', 'a', 'tourist', 'it', 'did', 'not', 'appeal', 'to', 'me', 'and', 'did', 'not', 'showcase', 'the', 'cuisine', 'of', 'malaysia', '.', 'we', 'ate', 'here', 'because', 'our', 'guide', 'told', 'us', 'we', 'could', 'find', 'nas', '##i', 'le', '##ma', '##k', 'and', 'me', '##e', 'gore', '##ng', 'here', 'but', 'as', 'stated', 'above', ',', 'the', 'nas', '##i', 'le', '##ma', '##k', 'was', 'under', '##w', '##helm', '##ing', 'and', 'there', 'was', 'no', 'me', '##e', 'gore', '##ng', 'in', 'sight'], tensor([ 1996,  2173,  2001,  3100,  2021,  1996,  2833,  2020,  2104,  2860,\n",
      "        24546,  2075,  1012,  2004,  1037,  7538,  2009,  2106,  2025,  5574,\n",
      "         2000,  2033,  1998,  2106,  2025, 13398,  1996, 12846,  1997,  6027,\n",
      "         1012,  2057,  8823,  2182,  2138,  2256,  5009,  2409,  2149,  2057,\n",
      "         2071,  2424, 17235,  2072,  3393,  2863,  2243,  1998,  2033,  2063,\n",
      "        13638,  3070,  2182,  2021,  2004,  3090,  2682,  1010,  1996, 17235,\n",
      "         2072,  3393,  2863,  2243,  2001,  2104,  2860, 24546,  2075,  1998,\n",
      "         2045,  2001,  2053,  2033,  2063, 13638,  3070,  1999,  4356]), tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(review_train_ds[235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(review_train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(review_test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ATE(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "\n",
    "        for data in loader:\n",
    "            try:\n",
    "                t0 = time.time()\n",
    "                ids_tensors, tags_tensors, masks_tensors = data\n",
    "                ids_tensors = ids_tensors.to(DEVICE)\n",
    "                tags_tensors = tags_tensors.to(DEVICE)\n",
    "                masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "                loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer_ATE.step()\n",
    "                optimizer_ATE.zero_grad()\n",
    "\n",
    "                finish_data += 1\n",
    "                current_times.append(round(time.time() - t0, 3))\n",
    "                current = np.mean(current_times)\n",
    "                hr, min, sec = evl_time(current * (all_data - finish_data) + current * all_data * (epochs - epoch - 1))\n",
    "                print('epoch:', epoch, \" batch:\", finish_data, \"/\", all_data, \" loss:\", np.mean(losses),\n",
    "                      \" hr:\", hr, \" min:\", min, \" sec:\", sec)\n",
    "            except IndexError:\n",
    "                print(\"IndexError during training. Skipping this sample.\")\n",
    "\n",
    "        save_model(model_ATE, 'bert_ATE.pkl')\n",
    "\n",
    "\n",
    "def test_model_ATE(loader):\n",
    "    pred = []\n",
    "    truth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            try:\n",
    "                ids_tensors, tags_tensors, masks_tensors = data\n",
    "                ids_tensors = ids_tensors.to(DEVICE)\n",
    "                tags_tensors = tags_tensors.to(DEVICE)\n",
    "                masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "                outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
    "\n",
    "                _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "                pred += list([int(j) for i in predictions for j in i])\n",
    "                truth += list([int(j) for i in tags_tensors for j in i])\n",
    "            except IndexError:\n",
    "                print(\"IndexError during testing. Skipping this sample.\")\n",
    "\n",
    "    return truth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the training set: 3934\n",
      "Number of rows in the testing set: 984\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows in the training and testing sets\n",
    "print(\"Number of rows in the training set:\", train_df.shape[0])\n",
    "print(\"Number of rows in the testing set:\", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  batch: 1 / 787  loss: 1.2178300619125366  hr: 1  min: 54  sec: 48\n",
      "epoch: 0  batch: 2 / 787  loss: 1.0419926643371582  hr: 2  min: 8  sec: 58\n",
      "epoch: 0  batch: 3 / 787  loss: 0.9302248358726501  hr: 2  min: 2  sec: 1\n",
      "epoch: 0  batch: 4 / 787  loss: 0.8415917158126831  hr: 1  min: 54  sec: 41\n",
      "epoch: 0  batch: 5 / 787  loss: 0.7626883625984192  hr: 1  min: 53  sec: 49\n",
      "epoch: 0  batch: 6 / 787  loss: 0.7305687467257181  hr: 1  min: 50  sec: 10\n",
      "epoch: 0  batch: 7 / 787  loss: 0.6660657269614083  hr: 1  min: 51  sec: 54\n",
      "epoch: 0  batch: 8 / 787  loss: 0.639711294323206  hr: 1  min: 51  sec: 0\n",
      "epoch: 0  batch: 9 / 787  loss: 0.5985711713631948  hr: 1  min: 48  sec: 40\n",
      "epoch: 0  batch: 10 / 787  loss: 0.5865313827991485  hr: 1  min: 47  sec: 45\n",
      "epoch: 0  batch: 11 / 787  loss: 0.5717776173895056  hr: 1  min: 47  sec: 41\n",
      "epoch: 0  batch: 12 / 787  loss: 0.5825061872601509  hr: 1  min: 45  sec: 33\n",
      "epoch: 0  batch: 13 / 787  loss: 0.570311365219263  hr: 1  min: 43  sec: 54\n",
      "epoch: 0  batch: 14 / 787  loss: 0.5571652714695249  hr: 1  min: 44  sec: 12\n",
      "epoch: 0  batch: 15 / 787  loss: 0.5609068969885508  hr: 1  min: 42  sec: 32\n",
      "epoch: 0  batch: 16 / 787  loss: 0.542664922773838  hr: 1  min: 44  sec: 23\n",
      "epoch: 0  batch: 17 / 787  loss: 0.5327010592993568  hr: 1  min: 44  sec: 8\n",
      "epoch: 0  batch: 18 / 787  loss: 0.5205645329422421  hr: 1  min: 44  sec: 54\n",
      "epoch: 0  batch: 19 / 787  loss: 0.5069785792576639  hr: 1  min: 44  sec: 46\n",
      "epoch: 0  batch: 20 / 787  loss: 0.5074484214186669  hr: 1  min: 44  sec: 26\n",
      "epoch: 0  batch: 21 / 787  loss: 0.5015791342371986  hr: 1  min: 44  sec: 49\n",
      "epoch: 0  batch: 22 / 787  loss: 0.4960120198401538  hr: 1  min: 45  sec: 5\n",
      "epoch: 0  batch: 23 / 787  loss: 0.49167726221291913  hr: 1  min: 45  sec: 40\n",
      "epoch: 0  batch: 24 / 787  loss: 0.48176339517037076  hr: 1  min: 45  sec: 26\n",
      "epoch: 0  batch: 25 / 787  loss: 0.47880647659301756  hr: 1  min: 45  sec: 38\n",
      "epoch: 0  batch: 26 / 787  loss: 0.4720806743089969  hr: 1  min: 45  sec: 58\n",
      "epoch: 0  batch: 27 / 787  loss: 0.4719438177567941  hr: 1  min: 44  sec: 51\n",
      "epoch: 0  batch: 28 / 787  loss: 0.471474897648607  hr: 1  min: 43  sec: 57\n",
      "epoch: 0  batch: 29 / 787  loss: 0.46679682053368665  hr: 1  min: 43  sec: 10\n",
      "epoch: 0  batch: 30 / 787  loss: 0.4602016101280848  hr: 1  min: 43  sec: 35\n",
      "epoch: 0  batch: 31 / 787  loss: 0.45594952760204194  hr: 1  min: 44  sec: 5\n",
      "epoch: 0  batch: 32 / 787  loss: 0.4506125086918473  hr: 1  min: 43  sec: 59\n",
      "epoch: 0  batch: 33 / 787  loss: 0.4431058466434479  hr: 1  min: 46  sec: 25\n",
      "epoch: 0  batch: 34 / 787  loss: 0.43815982780035806  hr: 1  min: 45  sec: 52\n",
      "epoch: 0  batch: 35 / 787  loss: 0.4339566571371896  hr: 1  min: 45  sec: 44\n",
      "epoch: 0  batch: 36 / 787  loss: 0.4320412145720588  hr: 1  min: 45  sec: 42\n",
      "epoch: 0  batch: 37 / 787  loss: 0.4291904745875178  hr: 1  min: 45  sec: 12\n",
      "epoch: 0  batch: 38 / 787  loss: 0.4249072772891898  hr: 1  min: 46  sec: 2\n",
      "epoch: 0  batch: 39 / 787  loss: 0.4202912021905948  hr: 1  min: 46  sec: 7\n",
      "epoch: 0  batch: 40 / 787  loss: 0.4159712567925453  hr: 1  min: 46  sec: 13\n",
      "epoch: 0  batch: 41 / 787  loss: 0.41204569688657433  hr: 1  min: 46  sec: 12\n",
      "epoch: 0  batch: 42 / 787  loss: 0.40762467292093096  hr: 1  min: 46  sec: 2\n",
      "epoch: 0  batch: 43 / 787  loss: 0.40575561169968094  hr: 1  min: 45  sec: 38\n",
      "epoch: 0  batch: 44 / 787  loss: 0.40190515971996565  hr: 1  min: 45  sec: 49\n",
      "epoch: 0  batch: 45 / 787  loss: 0.40256216890282104  hr: 1  min: 44  sec: 55\n",
      "epoch: 0  batch: 46 / 787  loss: 0.4001239288760268  hr: 1  min: 44  sec: 54\n",
      "epoch: 0  batch: 47 / 787  loss: 0.39697192545900956  hr: 1  min: 45  sec: 4\n",
      "epoch: 0  batch: 48 / 787  loss: 0.3944801560913523  hr: 1  min: 45  sec: 17\n",
      "epoch: 0  batch: 49 / 787  loss: 0.38984041128839764  hr: 1  min: 45  sec: 33\n",
      "epoch: 0  batch: 50 / 787  loss: 0.38670046120882035  hr: 1  min: 45  sec: 6\n",
      "epoch: 0  batch: 51 / 787  loss: 0.3840921050777622  hr: 1  min: 44  sec: 55\n",
      "epoch: 0  batch: 52 / 787  loss: 0.38143913132640034  hr: 1  min: 44  sec: 38\n",
      "epoch: 0  batch: 53 / 787  loss: 0.38029690030610785  hr: 1  min: 44  sec: 7\n",
      "epoch: 0  batch: 54 / 787  loss: 0.3788875371769623  hr: 1  min: 43  sec: 51\n",
      "epoch: 0  batch: 55 / 787  loss: 0.37607231302694843  hr: 1  min: 43  sec: 57\n",
      "epoch: 0  batch: 56 / 787  loss: 0.3725115087415491  hr: 1  min: 43  sec: 35\n",
      "epoch: 0  batch: 57 / 787  loss: 0.3696705766937189  hr: 1  min: 43  sec: 25\n",
      "epoch: 0  batch: 58 / 787  loss: 0.3661866419274232  hr: 1  min: 45  sec: 23\n",
      "epoch: 0  batch: 59 / 787  loss: 0.364344121035883  hr: 1  min: 45  sec: 27\n",
      "epoch: 0  batch: 60 / 787  loss: 0.3610305719077587  hr: 1  min: 47  sec: 19\n",
      "epoch: 0  batch: 61 / 787  loss: 0.35829047125871066  hr: 1  min: 47  sec: 16\n",
      "epoch: 0  batch: 62 / 787  loss: 0.35597452449221767  hr: 1  min: 47  sec: 18\n",
      "epoch: 0  batch: 63 / 787  loss: 0.35422303279240924  hr: 1  min: 47  sec: 4\n",
      "epoch: 0  batch: 64 / 787  loss: 0.3517961958423257  hr: 1  min: 47  sec: 5\n",
      "epoch: 0  batch: 65 / 787  loss: 0.3493888300198775  hr: 1  min: 46  sec: 46\n",
      "epoch: 0  batch: 66 / 787  loss: 0.3465984316937851  hr: 1  min: 46  sec: 39\n",
      "epoch: 0  batch: 67 / 787  loss: 0.3436359772041662  hr: 1  min: 46  sec: 43\n",
      "epoch: 0  batch: 68 / 787  loss: 0.34136439443511124  hr: 1  min: 46  sec: 32\n",
      "epoch: 0  batch: 69 / 787  loss: 0.339983973166217  hr: 1  min: 46  sec: 27\n",
      "epoch: 0  batch: 70 / 787  loss: 0.3381337700145585  hr: 1  min: 46  sec: 7\n",
      "epoch: 0  batch: 71 / 787  loss: 0.33544857611118906  hr: 1  min: 46  sec: 22\n",
      "epoch: 0  batch: 72 / 787  loss: 0.33471573351158035  hr: 1  min: 46  sec: 19\n",
      "epoch: 0  batch: 73 / 787  loss: 0.33315743383479446  hr: 1  min: 46  sec: 17\n",
      "epoch: 0  batch: 74 / 787  loss: 0.330635191016906  hr: 1  min: 46  sec: 0\n",
      "epoch: 0  batch: 75 / 787  loss: 0.3292716372013092  hr: 1  min: 45  sec: 55\n",
      "epoch: 0  batch: 76 / 787  loss: 0.32800211953489405  hr: 1  min: 45  sec: 47\n",
      "epoch: 0  batch: 77 / 787  loss: 0.3258794888660505  hr: 1  min: 45  sec: 42\n",
      "epoch: 0  batch: 78 / 787  loss: 0.32385746924540937  hr: 1  min: 45  sec: 35\n",
      "epoch: 0  batch: 79 / 787  loss: 0.32175727792178527  hr: 1  min: 45  sec: 15\n",
      "epoch: 0  batch: 80 / 787  loss: 0.31950256284326317  hr: 1  min: 44  sec: 54\n",
      "epoch: 0  batch: 81 / 787  loss: 0.3194607623197414  hr: 1  min: 44  sec: 30\n",
      "epoch: 0  batch: 82 / 787  loss: 0.3173817583337063  hr: 1  min: 44  sec: 23\n",
      "epoch: 0  batch: 83 / 787  loss: 0.31498711421547165  hr: 1  min: 44  sec: 20\n",
      "epoch: 0  batch: 84 / 787  loss: 0.3132790994076502  hr: 1  min: 44  sec: 8\n",
      "epoch: 0  batch: 85 / 787  loss: 0.3119367359315648  hr: 1  min: 43  sec: 56\n",
      "epoch: 0  batch: 86 / 787  loss: 0.3113248159372529  hr: 1  min: 43  sec: 29\n",
      "epoch: 0  batch: 87 / 787  loss: 0.3099489749848158  hr: 1  min: 43  sec: 6\n",
      "epoch: 0  batch: 88 / 787  loss: 0.3094784112816507  hr: 1  min: 43  sec: 10\n",
      "epoch: 0  batch: 89 / 787  loss: 0.30776024182860773  hr: 1  min: 42  sec: 59\n",
      "epoch: 0  batch: 90 / 787  loss: 0.3060916112528907  hr: 1  min: 43  sec: 11\n",
      "epoch: 0  batch: 91 / 787  loss: 0.30482550047256135  hr: 1  min: 43  sec: 4\n",
      "epoch: 0  batch: 92 / 787  loss: 0.3020486627583918  hr: 1  min: 43  sec: 10\n",
      "epoch: 0  batch: 93 / 787  loss: 0.3004028614490263  hr: 1  min: 43  sec: 7\n",
      "epoch: 0  batch: 94 / 787  loss: 0.2994383527560437  hr: 1  min: 42  sec: 59\n",
      "epoch: 0  batch: 95 / 787  loss: 0.2985275028567565  hr: 1  min: 42  sec: 49\n",
      "epoch: 0  batch: 96 / 787  loss: 0.2962632195558399  hr: 1  min: 43  sec: 6\n",
      "epoch: 0  batch: 97 / 787  loss: 0.295336377605335  hr: 1  min: 43  sec: 10\n",
      "epoch: 0  batch: 98 / 787  loss: 0.2935570785585715  hr: 1  min: 43  sec: 14\n",
      "epoch: 0  batch: 99 / 787  loss: 0.29222390447000063  hr: 1  min: 43  sec: 14\n",
      "epoch: 0  batch: 100 / 787  loss: 0.2903581845015287  hr: 1  min: 43  sec: 22\n",
      "epoch: 0  batch: 101 / 787  loss: 0.2895504858086605  hr: 1  min: 43  sec: 6\n",
      "epoch: 0  batch: 102 / 787  loss: 0.2882606256387982  hr: 1  min: 43  sec: 0\n",
      "epoch: 0  batch: 103 / 787  loss: 0.2861863986670392  hr: 1  min: 43  sec: 32\n",
      "epoch: 0  batch: 104 / 787  loss: 0.28489065929674184  hr: 1  min: 43  sec: 27\n",
      "epoch: 0  batch: 105 / 787  loss: 0.2838387063571385  hr: 1  min: 43  sec: 20\n",
      "epoch: 0  batch: 106 / 787  loss: 0.2829025535369819  hr: 1  min: 43  sec: 14\n",
      "epoch: 0  batch: 107 / 787  loss: 0.28260766331837556  hr: 1  min: 43  sec: 13\n",
      "epoch: 0  batch: 108 / 787  loss: 0.2816618934825615  hr: 1  min: 43  sec: 12\n",
      "epoch: 0  batch: 109 / 787  loss: 0.2805406633046789  hr: 1  min: 43  sec: 6\n",
      "epoch: 0  batch: 110 / 787  loss: 0.2791312969543717  hr: 1  min: 42  sec: 56\n",
      "epoch: 0  batch: 111 / 787  loss: 0.27745327165534905  hr: 1  min: 43  sec: 3\n",
      "epoch: 0  batch: 112 / 787  loss: 0.2757325050687151  hr: 1  min: 43  sec: 11\n",
      "epoch: 0  batch: 113 / 787  loss: 0.2748522860682116  hr: 1  min: 43  sec: 9\n",
      "epoch: 0  batch: 114 / 787  loss: 0.27387596817131626  hr: 1  min: 43  sec: 5\n",
      "epoch: 0  batch: 115 / 787  loss: 0.2721980620985446  hr: 1  min: 44  sec: 0\n",
      "epoch: 0  batch: 116 / 787  loss: 0.2708163285178357  hr: 1  min: 44  sec: 5\n",
      "epoch: 0  batch: 117 / 787  loss: 0.26921834433690095  hr: 1  min: 43  sec: 59\n",
      "epoch: 0  batch: 118 / 787  loss: 0.2680552072696767  hr: 1  min: 44  sec: 8\n",
      "epoch: 0  batch: 119 / 787  loss: 0.2670712209298831  hr: 1  min: 44  sec: 3\n",
      "epoch: 0  batch: 120 / 787  loss: 0.2661335566391548  hr: 1  min: 43  sec: 57\n",
      "epoch: 0  batch: 121 / 787  loss: 0.2654967220607868  hr: 1  min: 43  sec: 43\n",
      "epoch: 0  batch: 122 / 787  loss: 0.2646226271009836  hr: 1  min: 43  sec: 29\n",
      "epoch: 0  batch: 123 / 787  loss: 0.26419287023505544  hr: 1  min: 43  sec: 11\n",
      "epoch: 0  batch: 124 / 787  loss: 0.2627379379325336  hr: 1  min: 43  sec: 14\n",
      "epoch: 0  batch: 125 / 787  loss: 0.2624767456650734  hr: 1  min: 42  sec: 57\n",
      "epoch: 0  batch: 126 / 787  loss: 0.26160912671022946  hr: 1  min: 42  sec: 51\n",
      "epoch: 0  batch: 127 / 787  loss: 0.2605380725086205  hr: 1  min: 42  sec: 45\n",
      "epoch: 0  batch: 128 / 787  loss: 0.25931532779941335  hr: 1  min: 42  sec: 39\n",
      "epoch: 0  batch: 129 / 787  loss: 0.25867078753635864  hr: 1  min: 42  sec: 36\n",
      "epoch: 0  batch: 130 / 787  loss: 0.25756239793621577  hr: 1  min: 42  sec: 33\n",
      "epoch: 0  batch: 131 / 787  loss: 0.2566349103136827  hr: 1  min: 42  sec: 34\n",
      "epoch: 0  batch: 132 / 787  loss: 0.2554161309292822  hr: 1  min: 42  sec: 23\n",
      "epoch: 0  batch: 133 / 787  loss: 0.25397286821800963  hr: 1  min: 42  sec: 12\n",
      "epoch: 0  batch: 134 / 787  loss: 0.25276274945753724  hr: 1  min: 42  sec: 9\n",
      "epoch: 0  batch: 135 / 787  loss: 0.2517173230648041  hr: 1  min: 41  sec: 57\n",
      "epoch: 0  batch: 136 / 787  loss: 0.25090930479414325  hr: 1  min: 41  sec: 50\n",
      "epoch: 0  batch: 137 / 787  loss: 0.24943150487476892  hr: 1  min: 41  sec: 53\n",
      "epoch: 0  batch: 138 / 787  loss: 0.24856439011468404  hr: 1  min: 41  sec: 45\n",
      "epoch: 0  batch: 139 / 787  loss: 0.24752228883959407  hr: 1  min: 41  sec: 34\n",
      "epoch: 0  batch: 140 / 787  loss: 0.24691417845232147  hr: 1  min: 41  sec: 32\n",
      "epoch: 0  batch: 141 / 787  loss: 0.24552450278866375  hr: 1  min: 41  sec: 25\n",
      "epoch: 0  batch: 142 / 787  loss: 0.24490659838725984  hr: 1  min: 41  sec: 22\n",
      "epoch: 0  batch: 143 / 787  loss: 0.2441517130009361  hr: 1  min: 41  sec: 15\n",
      "epoch: 0  batch: 144 / 787  loss: 0.24300753213982615  hr: 1  min: 41  sec: 14\n",
      "epoch: 0  batch: 145 / 787  loss: 0.24171715083307233  hr: 1  min: 41  sec: 32\n",
      "epoch: 0  batch: 146 / 787  loss: 0.24099772068837735  hr: 1  min: 41  sec: 26\n",
      "epoch: 0  batch: 147 / 787  loss: 0.2398935789305742  hr: 1  min: 41  sec: 27\n",
      "epoch: 0  batch: 148 / 787  loss: 0.23920898177233096  hr: 1  min: 41  sec: 16\n",
      "epoch: 0  batch: 149 / 787  loss: 0.23837364542504286  hr: 1  min: 41  sec: 12\n",
      "epoch: 0  batch: 150 / 787  loss: 0.23735334925353527  hr: 1  min: 41  sec: 4\n",
      "epoch: 0  batch: 151 / 787  loss: 0.23601538431368127  hr: 1  min: 41  sec: 31\n",
      "epoch: 0  batch: 152 / 787  loss: 0.2354864368803407  hr: 1  min: 41  sec: 35\n",
      "epoch: 0  batch: 153 / 787  loss: 0.23483245157533222  hr: 1  min: 41  sec: 42\n",
      "epoch: 0  batch: 154 / 787  loss: 0.23415713752438497  hr: 1  min: 41  sec: 30\n",
      "epoch: 0  batch: 155 / 787  loss: 0.23330774211114452  hr: 1  min: 41  sec: 34\n",
      "epoch: 0  batch: 156 / 787  loss: 0.23251200027954885  hr: 1  min: 41  sec: 29\n",
      "epoch: 0  batch: 157 / 787  loss: 0.23171476991313278  hr: 1  min: 41  sec: 26\n",
      "epoch: 0  batch: 158 / 787  loss: 0.23086132071440732  hr: 1  min: 41  sec: 23\n",
      "epoch: 0  batch: 159 / 787  loss: 0.23005926580923908  hr: 1  min: 41  sec: 25\n",
      "epoch: 0  batch: 160 / 787  loss: 0.22915437412448228  hr: 1  min: 41  sec: 15\n",
      "epoch: 0  batch: 161 / 787  loss: 0.22857607304124358  hr: 1  min: 41  sec: 5\n",
      "epoch: 0  batch: 162 / 787  loss: 0.22792821211947334  hr: 1  min: 41  sec: 2\n",
      "epoch: 0  batch: 163 / 787  loss: 0.22794161115321646  hr: 1  min: 40  sec: 56\n",
      "epoch: 0  batch: 164 / 787  loss: 0.22690311121958784  hr: 1  min: 41  sec: 0\n",
      "epoch: 0  batch: 165 / 787  loss: 0.22618386390985865  hr: 1  min: 40  sec: 58\n",
      "epoch: 0  batch: 166 / 787  loss: 0.2256215542150908  hr: 1  min: 40  sec: 49\n",
      "epoch: 0  batch: 167 / 787  loss: 0.22487549820613717  hr: 1  min: 40  sec: 43\n",
      "epoch: 0  batch: 168 / 787  loss: 0.2240120393135363  hr: 1  min: 40  sec: 37\n",
      "epoch: 0  batch: 169 / 787  loss: 0.22347702268929875  hr: 1  min: 40  sec: 23\n",
      "epoch: 0  batch: 170 / 787  loss: 0.222828483954072  hr: 1  min: 40  sec: 23\n",
      "epoch: 0  batch: 171 / 787  loss: 0.2218835220089433  hr: 1  min: 40  sec: 22\n",
      "epoch: 0  batch: 172 / 787  loss: 0.22113781916194183  hr: 1  min: 40  sec: 25\n",
      "epoch: 0  batch: 173 / 787  loss: 0.22049193256507718  hr: 1  min: 40  sec: 17\n",
      "epoch: 0  batch: 174 / 787  loss: 0.21971349156964784  hr: 1  min: 40  sec: 15\n",
      "epoch: 0  batch: 175 / 787  loss: 0.21931462726422718  hr: 1  min: 40  sec: 13\n",
      "epoch: 0  batch: 176 / 787  loss: 0.21859944141893226  hr: 1  min: 40  sec: 13\n",
      "epoch: 0  batch: 177 / 787  loss: 0.21795650630515848  hr: 1  min: 40  sec: 16\n",
      "epoch: 0  batch: 178 / 787  loss: 0.21784900451141798  hr: 1  min: 40  sec: 15\n",
      "epoch: 0  batch: 179 / 787  loss: 0.21715867594300703  hr: 1  min: 40  sec: 7\n",
      "epoch: 0  batch: 180 / 787  loss: 0.21674743211931652  hr: 1  min: 40  sec: 9\n",
      "epoch: 0  batch: 181 / 787  loss: 0.2161458375457242  hr: 1  min: 40  sec: 2\n",
      "epoch: 0  batch: 182 / 787  loss: 0.21529231173405936  hr: 1  min: 40  sec: 19\n",
      "epoch: 0  batch: 183 / 787  loss: 0.21464664344497716  hr: 1  min: 40  sec: 15\n",
      "epoch: 0  batch: 184 / 787  loss: 0.2143808708364225  hr: 1  min: 40  sec: 11\n",
      "epoch: 0  batch: 185 / 787  loss: 0.21408501724536355  hr: 1  min: 40  sec: 12\n",
      "epoch: 0  batch: 186 / 787  loss: 0.21335925536370406  hr: 1  min: 40  sec: 8\n",
      "epoch: 0  batch: 187 / 787  loss: 0.21271003936143482  hr: 1  min: 40  sec: 16\n",
      "epoch: 0  batch: 188 / 787  loss: 0.21221397113689083  hr: 1  min: 40  sec: 16\n",
      "epoch: 0  batch: 189 / 787  loss: 0.2116740713950503  hr: 1  min: 40  sec: 16\n",
      "epoch: 0  batch: 190 / 787  loss: 0.21119364760816098  hr: 1  min: 40  sec: 5\n",
      "epoch: 0  batch: 191 / 787  loss: 0.2104275575131958  hr: 1  min: 40  sec: 3\n",
      "epoch: 0  batch: 192 / 787  loss: 0.21027695975499228  hr: 1  min: 39  sec: 55\n",
      "epoch: 0  batch: 193 / 787  loss: 0.21012363044323082  hr: 1  min: 39  sec: 46\n",
      "epoch: 0  batch: 194 / 787  loss: 0.20944176101577036  hr: 1  min: 39  sec: 43\n",
      "epoch: 0  batch: 195 / 787  loss: 0.20884398644169172  hr: 1  min: 39  sec: 50\n",
      "epoch: 0  batch: 196 / 787  loss: 0.2080858535684493  hr: 1  min: 39  sec: 39\n",
      "epoch: 0  batch: 197 / 787  loss: 0.20737967072858424  hr: 1  min: 39  sec: 40\n",
      "epoch: 0  batch: 198 / 787  loss: 0.20712957437140772  hr: 1  min: 39  sec: 39\n",
      "epoch: 0  batch: 199 / 787  loss: 0.20668052616131366  hr: 1  min: 39  sec: 45\n",
      "epoch: 0  batch: 200 / 787  loss: 0.20609340518712999  hr: 1  min: 40  sec: 16\n",
      "epoch: 0  batch: 201 / 787  loss: 0.20564714904448286  hr: 1  min: 40  sec: 9\n",
      "epoch: 0  batch: 202 / 787  loss: 0.20495987849512903  hr: 1  min: 40  sec: 11\n",
      "epoch: 0  batch: 203 / 787  loss: 0.20447219640310174  hr: 1  min: 40  sec: 5\n",
      "epoch: 0  batch: 204 / 787  loss: 0.20421791423623467  hr: 1  min: 39  sec: 54\n",
      "epoch: 0  batch: 205 / 787  loss: 0.203896403421716  hr: 1  min: 39  sec: 50\n",
      "epoch: 0  batch: 206 / 787  loss: 0.20350772970654432  hr: 1  min: 39  sec: 44\n",
      "epoch: 0  batch: 207 / 787  loss: 0.20311374336049176  hr: 1  min: 39  sec: 43\n",
      "epoch: 0  batch: 208 / 787  loss: 0.20251041704502243  hr: 1  min: 39  sec: 35\n",
      "epoch: 0  batch: 209 / 787  loss: 0.20199913731174607  hr: 1  min: 39  sec: 35\n",
      "epoch: 0  batch: 210 / 787  loss: 0.20159885173752196  hr: 1  min: 39  sec: 27\n",
      "epoch: 0  batch: 211 / 787  loss: 0.20118491316293652  hr: 1  min: 39  sec: 24\n",
      "epoch: 0  batch: 212 / 787  loss: 0.20110007565257684  hr: 1  min: 39  sec: 17\n",
      "epoch: 0  batch: 213 / 787  loss: 0.20092988979648535  hr: 1  min: 39  sec: 12\n",
      "epoch: 0  batch: 214 / 787  loss: 0.20034083560387664  hr: 1  min: 39  sec: 9\n",
      "epoch: 0  batch: 215 / 787  loss: 0.1997502210181813  hr: 1  min: 39  sec: 2\n",
      "epoch: 0  batch: 216 / 787  loss: 0.19925109283239753  hr: 1  min: 39  sec: 2\n",
      "epoch: 0  batch: 217 / 787  loss: 0.19914944696536263  hr: 1  min: 38  sec: 54\n",
      "epoch: 0  batch: 218 / 787  loss: 0.19912892668892485  hr: 1  min: 38  sec: 50\n",
      "epoch: 0  batch: 219 / 787  loss: 0.19840583630484532  hr: 1  min: 39  sec: 0\n",
      "epoch: 0  batch: 220 / 787  loss: 0.19813014529645442  hr: 1  min: 38  sec: 58\n",
      "epoch: 0  batch: 221 / 787  loss: 0.19770770027761544  hr: 1  min: 39  sec: 1\n",
      "epoch: 0  batch: 222 / 787  loss: 0.19717035362043897  hr: 1  min: 38  sec: 56\n",
      "epoch: 0  batch: 223 / 787  loss: 0.1968071870672863  hr: 1  min: 38  sec: 47\n",
      "epoch: 0  batch: 224 / 787  loss: 0.1964295776893518  hr: 1  min: 38  sec: 40\n",
      "epoch: 0  batch: 225 / 787  loss: 0.1960380357503891  hr: 1  min: 38  sec: 36\n",
      "epoch: 0  batch: 226 / 787  loss: 0.19557184178744796  hr: 1  min: 38  sec: 32\n",
      "epoch: 0  batch: 227 / 787  loss: 0.1950951281623168  hr: 1  min: 38  sec: 35\n",
      "epoch: 0  batch: 228 / 787  loss: 0.19469223112652176  hr: 1  min: 38  sec: 31\n",
      "epoch: 0  batch: 229 / 787  loss: 0.19413696717487153  hr: 1  min: 38  sec: 22\n",
      "epoch: 0  batch: 230 / 787  loss: 0.19387687192015027  hr: 1  min: 38  sec: 12\n",
      "epoch: 0  batch: 231 / 787  loss: 0.19320586112477048  hr: 1  min: 38  sec: 8\n",
      "epoch: 0  batch: 232 / 787  loss: 0.1926953635050048  hr: 1  min: 38  sec: 6\n",
      "epoch: 0  batch: 233 / 787  loss: 0.19242647570026278  hr: 1  min: 37  sec: 58\n",
      "epoch: 0  batch: 234 / 787  loss: 0.19207494121649835  hr: 1  min: 37  sec: 55\n",
      "epoch: 0  batch: 235 / 787  loss: 0.19161292721933507  hr: 1  min: 37  sec: 50\n",
      "epoch: 0  batch: 236 / 787  loss: 0.19120524102285252  hr: 1  min: 37  sec: 45\n",
      "epoch: 0  batch: 237 / 787  loss: 0.19076169771044063  hr: 1  min: 37  sec: 38\n",
      "epoch: 0  batch: 238 / 787  loss: 0.19026486298554585  hr: 1  min: 37  sec: 40\n",
      "epoch: 0  batch: 239 / 787  loss: 0.18978515309457  hr: 1  min: 37  sec: 32\n",
      "epoch: 0  batch: 240 / 787  loss: 0.18938359178913136  hr: 1  min: 37  sec: 26\n",
      "epoch: 0  batch: 241 / 787  loss: 0.18901697130433256  hr: 1  min: 37  sec: 27\n",
      "epoch: 0  batch: 242 / 787  loss: 0.1883951548548523  hr: 1  min: 37  sec: 33\n",
      "epoch: 0  batch: 243 / 787  loss: 0.18794764944739303  hr: 1  min: 37  sec: 34\n",
      "epoch: 0  batch: 244 / 787  loss: 0.1874823045199279  hr: 1  min: 37  sec: 30\n",
      "epoch: 0  batch: 245 / 787  loss: 0.1870284468087615  hr: 1  min: 37  sec: 27\n",
      "epoch: 0  batch: 246 / 787  loss: 0.18670020529591455  hr: 1  min: 37  sec: 23\n",
      "epoch: 0  batch: 247 / 787  loss: 0.18624812335922167  hr: 1  min: 37  sec: 21\n",
      "epoch: 0  batch: 248 / 787  loss: 0.1858207257013888  hr: 1  min: 37  sec: 25\n",
      "epoch: 0  batch: 249 / 787  loss: 0.18525898646578254  hr: 1  min: 37  sec: 19\n",
      "epoch: 0  batch: 250 / 787  loss: 0.18479613731801509  hr: 1  min: 37  sec: 12\n",
      "epoch: 0  batch: 251 / 787  loss: 0.18428974674339313  hr: 1  min: 37  sec: 7\n",
      "epoch: 0  batch: 252 / 787  loss: 0.18441516014614276  hr: 1  min: 37  sec: 0\n",
      "epoch: 0  batch: 253 / 787  loss: 0.1842153911210096  hr: 1  min: 36  sec: 55\n",
      "epoch: 0  batch: 254 / 787  loss: 0.18403681069905833  hr: 1  min: 36  sec: 52\n",
      "epoch: 0  batch: 255 / 787  loss: 0.18352945226372458  hr: 1  min: 36  sec: 53\n",
      "epoch: 0  batch: 256 / 787  loss: 0.1830520703078946  hr: 1  min: 36  sec: 50\n",
      "epoch: 0  batch: 257 / 787  loss: 0.18263483533077668  hr: 1  min: 36  sec: 59\n",
      "epoch: 0  batch: 258 / 787  loss: 0.18220479083211386  hr: 1  min: 36  sec: 52\n",
      "epoch: 0  batch: 259 / 787  loss: 0.1819070003006219  hr: 1  min: 36  sec: 50\n",
      "epoch: 0  batch: 260 / 787  loss: 0.18151784643817406  hr: 1  min: 36  sec: 46\n",
      "epoch: 0  batch: 261 / 787  loss: 0.1810211822162186  hr: 1  min: 36  sec: 54\n",
      "epoch: 0  batch: 262 / 787  loss: 0.1808001480939734  hr: 1  min: 36  sec: 53\n",
      "epoch: 0  batch: 263 / 787  loss: 0.18044841793780092  hr: 1  min: 36  sec: 50\n",
      "epoch: 0  batch: 264 / 787  loss: 0.18016985642977737  hr: 1  min: 36  sec: 41\n",
      "epoch: 0  batch: 265 / 787  loss: 0.17969889244380988  hr: 1  min: 36  sec: 38\n",
      "epoch: 0  batch: 266 / 787  loss: 0.17916332798680865  hr: 1  min: 36  sec: 38\n",
      "epoch: 0  batch: 267 / 787  loss: 0.1788466271668784  hr: 1  min: 36  sec: 33\n",
      "epoch: 0  batch: 268 / 787  loss: 0.17845943236528938  hr: 1  min: 36  sec: 32\n",
      "epoch: 0  batch: 269 / 787  loss: 0.1782465297936507  hr: 1  min: 36  sec: 28\n",
      "epoch: 0  batch: 270 / 787  loss: 0.1780063830592014  hr: 1  min: 36  sec: 24\n",
      "epoch: 0  batch: 271 / 787  loss: 0.17777066125201124  hr: 1  min: 36  sec: 22\n",
      "epoch: 0  batch: 272 / 787  loss: 0.17735421887653716  hr: 1  min: 36  sec: 42\n",
      "epoch: 0  batch: 273 / 787  loss: 0.17705582756371724  hr: 1  min: 36  sec: 44\n",
      "epoch: 0  batch: 274 / 787  loss: 0.1767842146656374  hr: 1  min: 36  sec: 38\n",
      "epoch: 0  batch: 275 / 787  loss: 0.17657532813874158  hr: 1  min: 36  sec: 37\n",
      "epoch: 0  batch: 276 / 787  loss: 0.17638784228567628  hr: 1  min: 36  sec: 32\n",
      "epoch: 0  batch: 277 / 787  loss: 0.17613535893522875  hr: 1  min: 36  sec: 25\n",
      "epoch: 0  batch: 278 / 787  loss: 0.17570125425515845  hr: 1  min: 36  sec: 38\n",
      "epoch: 0  batch: 279 / 787  loss: 0.17548440083006805  hr: 1  min: 36  sec: 34\n",
      "epoch: 0  batch: 280 / 787  loss: 0.17502953181309358  hr: 1  min: 36  sec: 34\n",
      "epoch: 0  batch: 281 / 787  loss: 0.1749062683976842  hr: 1  min: 36  sec: 30\n",
      "epoch: 0  batch: 282 / 787  loss: 0.17481070492707246  hr: 1  min: 36  sec: 26\n",
      "epoch: 0  batch: 283 / 787  loss: 0.1744170916585956  hr: 1  min: 36  sec: 34\n",
      "epoch: 0  batch: 284 / 787  loss: 0.1738957964985723  hr: 1  min: 36  sec: 37\n",
      "epoch: 0  batch: 285 / 787  loss: 0.17351067055735672  hr: 1  min: 36  sec: 34\n",
      "epoch: 0  batch: 286 / 787  loss: 0.17320982249570893  hr: 1  min: 36  sec: 31\n",
      "epoch: 0  batch: 287 / 787  loss: 0.17281847266733438  hr: 1  min: 36  sec: 24\n",
      "epoch: 0  batch: 288 / 787  loss: 0.1725685841584992  hr: 1  min: 36  sec: 22\n",
      "epoch: 0  batch: 289 / 787  loss: 0.17223879348190185  hr: 1  min: 36  sec: 19\n",
      "epoch: 0  batch: 290 / 787  loss: 0.17200083685075415  hr: 1  min: 36  sec: 16\n",
      "epoch: 0  batch: 291 / 787  loss: 0.1715938518035043  hr: 1  min: 36  sec: 35\n",
      "epoch: 0  batch: 292 / 787  loss: 0.17137015843126055  hr: 1  min: 36  sec: 35\n",
      "epoch: 0  batch: 293 / 787  loss: 0.17097613304446582  hr: 1  min: 36  sec: 31\n",
      "epoch: 0  batch: 294 / 787  loss: 0.17064646732847707  hr: 1  min: 36  sec: 27\n",
      "epoch: 0  batch: 295 / 787  loss: 0.17031496793536816  hr: 1  min: 36  sec: 20\n",
      "epoch: 0  batch: 296 / 787  loss: 0.16992808147207708  hr: 1  min: 36  sec: 20\n",
      "epoch: 0  batch: 297 / 787  loss: 0.16967044945066223  hr: 1  min: 36  sec: 14\n",
      "epoch: 0  batch: 298 / 787  loss: 0.16952021298742534  hr: 1  min: 36  sec: 15\n",
      "epoch: 0  batch: 299 / 787  loss: 0.1692178142584088  hr: 1  min: 36  sec: 11\n",
      "epoch: 0  batch: 300 / 787  loss: 0.168909199424088  hr: 1  min: 36  sec: 8\n",
      "epoch: 0  batch: 301 / 787  loss: 0.1685754690331676  hr: 1  min: 36  sec: 5\n",
      "epoch: 0  batch: 302 / 787  loss: 0.16822436615972725  hr: 1  min: 36  sec: 3\n",
      "epoch: 0  batch: 303 / 787  loss: 0.16781857946623277  hr: 1  min: 35  sec: 57\n",
      "epoch: 0  batch: 304 / 787  loss: 0.16752661347977424  hr: 1  min: 35  sec: 56\n",
      "epoch: 0  batch: 305 / 787  loss: 0.16737743090899265  hr: 1  min: 35  sec: 51\n",
      "epoch: 0  batch: 306 / 787  loss: 0.1671191268156167  hr: 1  min: 35  sec: 44\n",
      "epoch: 0  batch: 307 / 787  loss: 0.16684415955885226  hr: 1  min: 35  sec: 40\n",
      "epoch: 0  batch: 308 / 787  loss: 0.16643695832024535  hr: 1  min: 35  sec: 37\n",
      "epoch: 0  batch: 309 / 787  loss: 0.16609873580460024  hr: 1  min: 35  sec: 31\n",
      "epoch: 0  batch: 310 / 787  loss: 0.1659954646782529  hr: 1  min: 35  sec: 22\n",
      "epoch: 0  batch: 311 / 787  loss: 0.16565330233125442  hr: 1  min: 35  sec: 18\n",
      "epoch: 0  batch: 312 / 787  loss: 0.16530650769336483  hr: 1  min: 35  sec: 13\n",
      "epoch: 0  batch: 313 / 787  loss: 0.16504489465024524  hr: 1  min: 35  sec: 18\n",
      "epoch: 0  batch: 314 / 787  loss: 0.1647986763388298  hr: 1  min: 35  sec: 12\n",
      "epoch: 0  batch: 315 / 787  loss: 0.16441905854476824  hr: 1  min: 35  sec: 10\n",
      "epoch: 0  batch: 316 / 787  loss: 0.1642702552077325  hr: 1  min: 35  sec: 5\n",
      "epoch: 0  batch: 317 / 787  loss: 0.16402357037558166  hr: 1  min: 35  sec: 4\n",
      "epoch: 0  batch: 318 / 787  loss: 0.16361171732299357  hr: 1  min: 34  sec: 59\n",
      "epoch: 0  batch: 319 / 787  loss: 0.16337199536003288  hr: 1  min: 34  sec: 55\n",
      "epoch: 0  batch: 320 / 787  loss: 0.16311325979186223  hr: 1  min: 34  sec: 52\n",
      "epoch: 0  batch: 321 / 787  loss: 0.1628017861779048  hr: 1  min: 34  sec: 49\n",
      "epoch: 0  batch: 322 / 787  loss: 0.1624494618533746  hr: 1  min: 34  sec: 45\n",
      "epoch: 0  batch: 323 / 787  loss: 0.1621599299689011  hr: 1  min: 34  sec: 39\n",
      "epoch: 0  batch: 324 / 787  loss: 0.1617119772946117  hr: 1  min: 34  sec: 40\n",
      "epoch: 0  batch: 325 / 787  loss: 0.16140070025164346  hr: 1  min: 34  sec: 39\n",
      "epoch: 0  batch: 326 / 787  loss: 0.1611497610567446  hr: 1  min: 34  sec: 31\n",
      "epoch: 0  batch: 327 / 787  loss: 0.16100889753652822  hr: 1  min: 34  sec: 27\n",
      "epoch: 0  batch: 328 / 787  loss: 0.16077950752976283  hr: 1  min: 34  sec: 20\n",
      "epoch: 0  batch: 329 / 787  loss: 0.1605432918946341  hr: 1  min: 34  sec: 30\n",
      "epoch: 0  batch: 330 / 787  loss: 0.1602166275352691  hr: 1  min: 34  sec: 28\n",
      "epoch: 0  batch: 331 / 787  loss: 0.15992213911137731  hr: 1  min: 34  sec: 26\n",
      "epoch: 0  batch: 332 / 787  loss: 0.15974442658003374  hr: 1  min: 34  sec: 21\n",
      "epoch: 0  batch: 333 / 787  loss: 0.15945978786561404  hr: 1  min: 34  sec: 22\n",
      "epoch: 0  batch: 334 / 787  loss: 0.15921458371630506  hr: 1  min: 34  sec: 18\n",
      "epoch: 0  batch: 335 / 787  loss: 0.15888690063868885  hr: 1  min: 34  sec: 16\n",
      "epoch: 0  batch: 336 / 787  loss: 0.15852763050878865  hr: 1  min: 34  sec: 10\n",
      "epoch: 0  batch: 337 / 787  loss: 0.1581695730513299  hr: 1  min: 34  sec: 5\n",
      "epoch: 0  batch: 338 / 787  loss: 0.1578669954895885  hr: 1  min: 34  sec: 1\n",
      "epoch: 0  batch: 339 / 787  loss: 0.15757549243668714  hr: 1  min: 33  sec: 56\n",
      "epoch: 0  batch: 340 / 787  loss: 0.15728704194702647  hr: 1  min: 33  sec: 51\n",
      "epoch: 0  batch: 341 / 787  loss: 0.1569152925207206  hr: 1  min: 33  sec: 47\n",
      "epoch: 0  batch: 342 / 787  loss: 0.1565490518015815  hr: 1  min: 33  sec: 43\n",
      "epoch: 0  batch: 343 / 787  loss: 0.15619861401356866  hr: 1  min: 33  sec: 42\n",
      "epoch: 0  batch: 344 / 787  loss: 0.1560823080443972  hr: 1  min: 33  sec: 37\n",
      "epoch: 0  batch: 345 / 787  loss: 0.15585841047914994  hr: 1  min: 33  sec: 31\n",
      "epoch: 0  batch: 346 / 787  loss: 0.1555321835616679  hr: 1  min: 33  sec: 31\n",
      "epoch: 0  batch: 347 / 787  loss: 0.15524100186437453  hr: 1  min: 33  sec: 27\n",
      "epoch: 0  batch: 348 / 787  loss: 0.15492285701885133  hr: 1  min: 33  sec: 32\n",
      "epoch: 0  batch: 349 / 787  loss: 0.15472524934968668  hr: 1  min: 33  sec: 28\n",
      "epoch: 0  batch: 350 / 787  loss: 0.15451982320951563  hr: 1  min: 33  sec: 20\n",
      "epoch: 0  batch: 351 / 787  loss: 0.15428933587742497  hr: 1  min: 33  sec: 13\n",
      "epoch: 0  batch: 352 / 787  loss: 0.1539669070149433  hr: 1  min: 33  sec: 8\n",
      "epoch: 0  batch: 353 / 787  loss: 0.15373552565808504  hr: 1  min: 33  sec: 3\n",
      "epoch: 0  batch: 354 / 787  loss: 0.15346991030320442  hr: 1  min: 33  sec: 5\n",
      "epoch: 0  batch: 355 / 787  loss: 0.1532171009869223  hr: 1  min: 33  sec: 2\n",
      "epoch: 0  batch: 356 / 787  loss: 0.15306013290445958  hr: 1  min: 33  sec: 0\n",
      "epoch: 0  batch: 357 / 787  loss: 0.15306533701425673  hr: 1  min: 32  sec: 54\n",
      "epoch: 0  batch: 358 / 787  loss: 0.15278688987311204  hr: 1  min: 32  sec: 47\n",
      "epoch: 0  batch: 359 / 787  loss: 0.1524864457869347  hr: 1  min: 33  sec: 1\n",
      "epoch: 0  batch: 360 / 787  loss: 0.1521194869134989  hr: 1  min: 33  sec: 3\n",
      "epoch: 0  batch: 361 / 787  loss: 0.1518213857697978  hr: 1  min: 32  sec: 58\n",
      "epoch: 0  batch: 362 / 787  loss: 0.1516580018343517  hr: 1  min: 32  sec: 52\n",
      "epoch: 0  batch: 363 / 787  loss: 0.15140283423842807  hr: 1  min: 32  sec: 50\n",
      "epoch: 0  batch: 364 / 787  loss: 0.1513549788230723  hr: 1  min: 32  sec: 47\n",
      "epoch: 0  batch: 365 / 787  loss: 0.1513026378742636  hr: 1  min: 32  sec: 42\n",
      "epoch: 0  batch: 366 / 787  loss: 0.15114747042782972  hr: 1  min: 32  sec: 40\n",
      "epoch: 0  batch: 367 / 787  loss: 0.15084569653551974  hr: 1  min: 32  sec: 40\n",
      "epoch: 0  batch: 368 / 787  loss: 0.15057551773750913  hr: 1  min: 32  sec: 40\n",
      "epoch: 0  batch: 369 / 787  loss: 0.1504712580705723  hr: 1  min: 32  sec: 38\n",
      "epoch: 0  batch: 370 / 787  loss: 0.15018213804106453  hr: 1  min: 32  sec: 34\n",
      "epoch: 0  batch: 371 / 787  loss: 0.1500594095600583  hr: 1  min: 32  sec: 31\n",
      "epoch: 0  batch: 372 / 787  loss: 0.15012570116068086  hr: 1  min: 32  sec: 27\n",
      "epoch: 0  batch: 373 / 787  loss: 0.14998995940183507  hr: 1  min: 32  sec: 24\n",
      "epoch: 0  batch: 374 / 787  loss: 0.14969094285870937  hr: 1  min: 32  sec: 18\n",
      "epoch: 0  batch: 375 / 787  loss: 0.14960625486572585  hr: 1  min: 32  sec: 15\n",
      "epoch: 0  batch: 376 / 787  loss: 0.14937962073476074  hr: 1  min: 32  sec: 12\n",
      "epoch: 0  batch: 377 / 787  loss: 0.1491124551911253  hr: 1  min: 32  sec: 14\n",
      "epoch: 0  batch: 378 / 787  loss: 0.1489323220910534  hr: 1  min: 32  sec: 9\n",
      "epoch: 0  batch: 379 / 787  loss: 0.14863207262982478  hr: 1  min: 32  sec: 4\n",
      "epoch: 0  batch: 380 / 787  loss: 0.1484233280055617  hr: 1  min: 32  sec: 1\n",
      "epoch: 0  batch: 381 / 787  loss: 0.14821656311317066  hr: 1  min: 31  sec: 57\n",
      "epoch: 0  batch: 382 / 787  loss: 0.14789901174487868  hr: 1  min: 31  sec: 53\n",
      "epoch: 0  batch: 383 / 787  loss: 0.14756364073828057  hr: 1  min: 31  sec: 57\n",
      "epoch: 0  batch: 384 / 787  loss: 0.14749810346014178  hr: 1  min: 31  sec: 54\n",
      "epoch: 0  batch: 385 / 787  loss: 0.14733038871706305  hr: 1  min: 31  sec: 52\n",
      "epoch: 0  batch: 386 / 787  loss: 0.14706837693760122  hr: 1  min: 31  sec: 59\n",
      "epoch: 0  batch: 387 / 787  loss: 0.14676747863356612  hr: 1  min: 31  sec: 55\n",
      "epoch: 0  batch: 388 / 787  loss: 0.14656758745748205  hr: 1  min: 31  sec: 51\n",
      "epoch: 0  batch: 389 / 787  loss: 0.14629338635251424  hr: 1  min: 31  sec: 48\n",
      "epoch: 0  batch: 390 / 787  loss: 0.1459703648606172  hr: 1  min: 31  sec: 47\n",
      "epoch: 0  batch: 391 / 787  loss: 0.14579246053114878  hr: 1  min: 31  sec: 45\n",
      "epoch: 0  batch: 392 / 787  loss: 0.14558995023787935  hr: 1  min: 31  sec: 41\n",
      "epoch: 0  batch: 393 / 787  loss: 0.14539019745459386  hr: 1  min: 31  sec: 37\n",
      "epoch: 0  batch: 394 / 787  loss: 0.14511020738763858  hr: 1  min: 31  sec: 36\n",
      "epoch: 0  batch: 395 / 787  loss: 0.14482489092440545  hr: 1  min: 31  sec: 35\n",
      "epoch: 0  batch: 396 / 787  loss: 0.1445640508330079  hr: 1  min: 31  sec: 32\n",
      "epoch: 0  batch: 397 / 787  loss: 0.1443257432065022  hr: 1  min: 31  sec: 25\n",
      "epoch: 0  batch: 398 / 787  loss: 0.14421007714828654  hr: 1  min: 31  sec: 18\n",
      "epoch: 0  batch: 399 / 787  loss: 0.14389836996849767  hr: 1  min: 31  sec: 15\n",
      "epoch: 0  batch: 400 / 787  loss: 0.14370909705292434  hr: 1  min: 31  sec: 12\n",
      "epoch: 0  batch: 401 / 787  loss: 0.14357150321422224  hr: 1  min: 31  sec: 8\n",
      "epoch: 0  batch: 402 / 787  loss: 0.14334051854519256  hr: 1  min: 31  sec: 6\n",
      "epoch: 0  batch: 403 / 787  loss: 0.14324664140106283  hr: 1  min: 31  sec: 4\n",
      "epoch: 0  batch: 404 / 787  loss: 0.14304370101596606  hr: 1  min: 31  sec: 1\n",
      "epoch: 0  batch: 405 / 787  loss: 0.14280672232953853  hr: 1  min: 30  sec: 59\n",
      "epoch: 0  batch: 406 / 787  loss: 0.14260356594794638  hr: 1  min: 30  sec: 55\n",
      "epoch: 0  batch: 407 / 787  loss: 0.14244842407103692  hr: 1  min: 30  sec: 53\n",
      "epoch: 0  batch: 408 / 787  loss: 0.14222839627615816  hr: 1  min: 30  sec: 52\n",
      "epoch: 0  batch: 409 / 787  loss: 0.1419539699299047  hr: 1  min: 30  sec: 50\n",
      "epoch: 0  batch: 410 / 787  loss: 0.14180220056143475  hr: 1  min: 30  sec: 43\n",
      "epoch: 0  batch: 411 / 787  loss: 0.14151164447014059  hr: 1  min: 30  sec: 41\n",
      "epoch: 0  batch: 412 / 787  loss: 0.14153850559615394  hr: 1  min: 30  sec: 35\n",
      "epoch: 0  batch: 413 / 787  loss: 0.14126955578441913  hr: 1  min: 30  sec: 33\n",
      "epoch: 0  batch: 414 / 787  loss: 0.14105796090052755  hr: 1  min: 30  sec: 30\n",
      "epoch: 0  batch: 415 / 787  loss: 0.1408530309661684  hr: 1  min: 30  sec: 27\n",
      "epoch: 0  batch: 416 / 787  loss: 0.1407271228202332  hr: 1  min: 30  sec: 26\n",
      "epoch: 0  batch: 417 / 787  loss: 0.14083813874573586  hr: 1  min: 30  sec: 22\n",
      "epoch: 0  batch: 418 / 787  loss: 0.1405454663934106  hr: 1  min: 30  sec: 17\n",
      "epoch: 0  batch: 419 / 787  loss: 0.14032950262519084  hr: 1  min: 30  sec: 16\n",
      "epoch: 0  batch: 420 / 787  loss: 0.14005960584839894  hr: 1  min: 30  sec: 15\n",
      "epoch: 0  batch: 421 / 787  loss: 0.13998687989797418  hr: 1  min: 30  sec: 10\n",
      "epoch: 0  batch: 422 / 787  loss: 0.13980800644781494  hr: 1  min: 30  sec: 5\n",
      "epoch: 0  batch: 423 / 787  loss: 0.13955235413889935  hr: 1  min: 30  sec: 4\n",
      "epoch: 0  batch: 424 / 787  loss: 0.13933749209193266  hr: 1  min: 30  sec: 0\n",
      "epoch: 0  batch: 425 / 787  loss: 0.13915192443220054  hr: 1  min: 29  sec: 57\n",
      "epoch: 0  batch: 426 / 787  loss: 0.13924463044747082  hr: 1  min: 29  sec: 54\n",
      "epoch: 0  batch: 427 / 787  loss: 0.13913599412617833  hr: 1  min: 29  sec: 51\n",
      "epoch: 0  batch: 428 / 787  loss: 0.1389208343358728  hr: 1  min: 29  sec: 47\n",
      "epoch: 0  batch: 429 / 787  loss: 0.1386752434222501  hr: 1  min: 29  sec: 43\n",
      "epoch: 0  batch: 430 / 787  loss: 0.1384059099133971  hr: 1  min: 29  sec: 46\n",
      "epoch: 0  batch: 431 / 787  loss: 0.13833292969988698  hr: 1  min: 29  sec: 43\n",
      "epoch: 0  batch: 432 / 787  loss: 0.13809181443260363  hr: 1  min: 29  sec: 41\n",
      "epoch: 0  batch: 433 / 787  loss: 0.13782443014977985  hr: 1  min: 29  sec: 39\n",
      "epoch: 0  batch: 434 / 787  loss: 0.1375875681518547  hr: 1  min: 29  sec: 34\n",
      "epoch: 0  batch: 435 / 787  loss: 0.1373964800231758  hr: 1  min: 29  sec: 30\n",
      "epoch: 0  batch: 436 / 787  loss: 0.13719709161156363  hr: 1  min: 29  sec: 26\n",
      "epoch: 0  batch: 437 / 787  loss: 0.1371282081820872  hr: 1  min: 29  sec: 23\n",
      "epoch: 0  batch: 438 / 787  loss: 0.1369948534966876  hr: 1  min: 29  sec: 17\n",
      "epoch: 0  batch: 439 / 787  loss: 0.13684894755516616  hr: 1  min: 29  sec: 12\n",
      "epoch: 0  batch: 440 / 787  loss: 0.13663805123757233  hr: 1  min: 29  sec: 13\n",
      "epoch: 0  batch: 441 / 787  loss: 0.1364418782902007  hr: 1  min: 29  sec: 12\n",
      "epoch: 0  batch: 442 / 787  loss: 0.13629017282294204  hr: 1  min: 29  sec: 9\n",
      "epoch: 0  batch: 443 / 787  loss: 0.13615141081587995  hr: 1  min: 29  sec: 6\n",
      "epoch: 0  batch: 444 / 787  loss: 0.13605861072973893  hr: 1  min: 29  sec: 1\n",
      "epoch: 0  batch: 445 / 787  loss: 0.13582802069990824  hr: 1  min: 28  sec: 58\n",
      "epoch: 0  batch: 446 / 787  loss: 0.13573112245231467  hr: 1  min: 28  sec: 54\n",
      "epoch: 0  batch: 447 / 787  loss: 0.13565863792261554  hr: 1  min: 28  sec: 53\n",
      "epoch: 0  batch: 448 / 787  loss: 0.13559526469491953  hr: 1  min: 28  sec: 51\n",
      "epoch: 0  batch: 449 / 787  loss: 0.1353542910212597  hr: 1  min: 28  sec: 49\n",
      "epoch: 0  batch: 450 / 787  loss: 0.1351767886388633  hr: 1  min: 28  sec: 45\n",
      "epoch: 0  batch: 451 / 787  loss: 0.13502146343715044  hr: 1  min: 28  sec: 43\n",
      "epoch: 0  batch: 452 / 787  loss: 0.1349503912930006  hr: 1  min: 28  sec: 40\n",
      "epoch: 0  batch: 453 / 787  loss: 0.13478308107300968  hr: 1  min: 28  sec: 36\n",
      "epoch: 0  batch: 454 / 787  loss: 0.13459819399788647  hr: 1  min: 28  sec: 29\n",
      "epoch: 0  batch: 455 / 787  loss: 0.13442033889938843  hr: 1  min: 28  sec: 27\n",
      "epoch: 0  batch: 456 / 787  loss: 0.13423761622898542  hr: 1  min: 28  sec: 23\n",
      "epoch: 0  batch: 457 / 787  loss: 0.13406006905048498  hr: 1  min: 28  sec: 20\n",
      "epoch: 0  batch: 458 / 787  loss: 0.13390365424748982  hr: 1  min: 28  sec: 14\n",
      "epoch: 0  batch: 459 / 787  loss: 0.13365370944174806  hr: 1  min: 28  sec: 12\n",
      "epoch: 0  batch: 460 / 787  loss: 0.1334445457095685  hr: 1  min: 28  sec: 7\n",
      "epoch: 0  batch: 461 / 787  loss: 0.13339593694616037  hr: 1  min: 28  sec: 2\n",
      "epoch: 0  batch: 462 / 787  loss: 0.13319849738149675  hr: 1  min: 27  sec: 58\n",
      "epoch: 0  batch: 463 / 787  loss: 0.13302364834377112  hr: 1  min: 27  sec: 55\n",
      "epoch: 0  batch: 464 / 787  loss: 0.13277638361163052  hr: 1  min: 27  sec: 50\n",
      "epoch: 0  batch: 465 / 787  loss: 0.13264980548091473  hr: 1  min: 27  sec: 46\n",
      "epoch: 0  batch: 466 / 787  loss: 0.13251229216916238  hr: 1  min: 27  sec: 42\n",
      "epoch: 0  batch: 467 / 787  loss: 0.13231044008578044  hr: 1  min: 27  sec: 39\n",
      "epoch: 0  batch: 468 / 787  loss: 0.13211282768931526  hr: 1  min: 27  sec: 35\n",
      "epoch: 0  batch: 469 / 787  loss: 0.13198747425111754  hr: 1  min: 27  sec: 32\n",
      "epoch: 0  batch: 470 / 787  loss: 0.13181074844633645  hr: 1  min: 27  sec: 28\n",
      "epoch: 0  batch: 471 / 787  loss: 0.13162213338107948  hr: 1  min: 27  sec: 26\n",
      "epoch: 0  batch: 472 / 787  loss: 0.13146483173230822  hr: 1  min: 27  sec: 22\n",
      "epoch: 0  batch: 473 / 787  loss: 0.13136045245284383  hr: 1  min: 27  sec: 19\n",
      "epoch: 0  batch: 474 / 787  loss: 0.13121621376335746  hr: 1  min: 27  sec: 16\n",
      "epoch: 0  batch: 475 / 787  loss: 0.1310125281661749  hr: 1  min: 27  sec: 25\n",
      "epoch: 0  batch: 476 / 787  loss: 0.13101915339352327  hr: 1  min: 27  sec: 23\n",
      "epoch: 0  batch: 477 / 787  loss: 0.13086855455655097  hr: 1  min: 27  sec: 18\n",
      "epoch: 0  batch: 478 / 787  loss: 0.13068498891221056  hr: 1  min: 27  sec: 16\n",
      "epoch: 0  batch: 479 / 787  loss: 0.13055908462588747  hr: 1  min: 27  sec: 12\n",
      "epoch: 0  batch: 480 / 787  loss: 0.13035538855862494  hr: 1  min: 27  sec: 10\n",
      "epoch: 0  batch: 481 / 787  loss: 0.13016057914033874  hr: 1  min: 27  sec: 9\n",
      "epoch: 0  batch: 482 / 787  loss: 0.13011726915944538  hr: 1  min: 27  sec: 7\n",
      "epoch: 0  batch: 483 / 787  loss: 0.13002769573057538  hr: 1  min: 27  sec: 3\n",
      "epoch: 0  batch: 484 / 787  loss: 0.12983369542870762  hr: 1  min: 27  sec: 4\n",
      "epoch: 0  batch: 485 / 787  loss: 0.12969006722044085  hr: 1  min: 26  sec: 59\n",
      "epoch: 0  batch: 486 / 787  loss: 0.12950812991125962  hr: 1  min: 26  sec: 56\n",
      "epoch: 0  batch: 487 / 787  loss: 0.12935688085395086  hr: 1  min: 26  sec: 54\n",
      "epoch: 0  batch: 488 / 787  loss: 0.1291431638366375  hr: 1  min: 26  sec: 51\n",
      "epoch: 0  batch: 489 / 787  loss: 0.12905397473364033  hr: 1  min: 26  sec: 50\n",
      "epoch: 0  batch: 490 / 787  loss: 0.12884248480656926  hr: 1  min: 26  sec: 45\n",
      "epoch: 0  batch: 491 / 787  loss: 0.12875835229538365  hr: 1  min: 26  sec: 41\n",
      "epoch: 0  batch: 492 / 787  loss: 0.12864919581518666  hr: 1  min: 26  sec: 38\n",
      "epoch: 0  batch: 493 / 787  loss: 0.12847896524786223  hr: 1  min: 26  sec: 34\n",
      "epoch: 0  batch: 494 / 787  loss: 0.1284173803049543  hr: 1  min: 26  sec: 32\n",
      "epoch: 0  batch: 495 / 787  loss: 0.1282404710999643  hr: 1  min: 26  sec: 31\n",
      "epoch: 0  batch: 496 / 787  loss: 0.12809147386841715  hr: 1  min: 26  sec: 27\n",
      "epoch: 0  batch: 497 / 787  loss: 0.1279432586351151  hr: 1  min: 26  sec: 22\n",
      "epoch: 0  batch: 498 / 787  loss: 0.12782038954726185  hr: 1  min: 26  sec: 19\n",
      "epoch: 0  batch: 499 / 787  loss: 0.12765949335180685  hr: 1  min: 26  sec: 18\n",
      "epoch: 0  batch: 500 / 787  loss: 0.12749297885596753  hr: 1  min: 26  sec: 15\n",
      "epoch: 0  batch: 501 / 787  loss: 0.12738157291255311  hr: 1  min: 26  sec: 11\n",
      "epoch: 0  batch: 502 / 787  loss: 0.1272322859362777  hr: 1  min: 26  sec: 9\n",
      "epoch: 0  batch: 503 / 787  loss: 0.12704466035215564  hr: 1  min: 26  sec: 11\n",
      "epoch: 0  batch: 504 / 787  loss: 0.12691516675321118  hr: 1  min: 26  sec: 8\n",
      "epoch: 0  batch: 505 / 787  loss: 0.12675193016334335  hr: 1  min: 26  sec: 6\n",
      "epoch: 0  batch: 506 / 787  loss: 0.12653346357521097  hr: 1  min: 26  sec: 5\n",
      "epoch: 0  batch: 507 / 787  loss: 0.1263592337465733  hr: 1  min: 26  sec: 1\n",
      "epoch: 0  batch: 508 / 787  loss: 0.12626977715875923  hr: 1  min: 25  sec: 55\n",
      "epoch: 0  batch: 509 / 787  loss: 0.12615012924217991  hr: 1  min: 25  sec: 50\n",
      "epoch: 0  batch: 510 / 787  loss: 0.12605776422310108  hr: 1  min: 25  sec: 47\n",
      "epoch: 0  batch: 511 / 787  loss: 0.12587068518969993  hr: 1  min: 25  sec: 45\n",
      "epoch: 0  batch: 512 / 787  loss: 0.12569259345400496  hr: 1  min: 25  sec: 43\n",
      "epoch: 0  batch: 513 / 787  loss: 0.1254764281133404  hr: 1  min: 25  sec: 38\n",
      "epoch: 0  batch: 514 / 787  loss: 0.12535258048079936  hr: 1  min: 25  sec: 33\n",
      "epoch: 0  batch: 515 / 787  loss: 0.12519407807984978  hr: 1  min: 25  sec: 30\n",
      "epoch: 0  batch: 516 / 787  loss: 0.12506604303295413  hr: 1  min: 25  sec: 28\n",
      "epoch: 0  batch: 517 / 787  loss: 0.12487250458758965  hr: 1  min: 25  sec: 26\n",
      "epoch: 0  batch: 518 / 787  loss: 0.12469104046009222  hr: 1  min: 25  sec: 22\n",
      "epoch: 0  batch: 519 / 787  loss: 0.12450376309827688  hr: 1  min: 25  sec: 21\n",
      "epoch: 0  batch: 520 / 787  loss: 0.1243152701940674  hr: 1  min: 25  sec: 19\n",
      "epoch: 0  batch: 521 / 787  loss: 0.12411673632655972  hr: 1  min: 25  sec: 26\n",
      "epoch: 0  batch: 522 / 787  loss: 0.12392179246178303  hr: 1  min: 25  sec: 22\n",
      "epoch: 0  batch: 523 / 787  loss: 0.1237397209543514  hr: 1  min: 25  sec: 18\n",
      "epoch: 0  batch: 524 / 787  loss: 0.12359829884349503  hr: 1  min: 25  sec: 14\n",
      "epoch: 0  batch: 525 / 787  loss: 0.12344303543014186  hr: 1  min: 25  sec: 9\n",
      "epoch: 0  batch: 526 / 787  loss: 0.12330721173180487  hr: 1  min: 25  sec: 6\n",
      "epoch: 0  batch: 527 / 787  loss: 0.12310907191665847  hr: 1  min: 25  sec: 5\n",
      "epoch: 0  batch: 528 / 787  loss: 0.12297419713330314  hr: 1  min: 25  sec: 1\n",
      "epoch: 0  batch: 529 / 787  loss: 0.122804628336531  hr: 1  min: 24  sec: 55\n",
      "epoch: 0  batch: 530 / 787  loss: 0.12260523725582181  hr: 1  min: 24  sec: 51\n",
      "epoch: 0  batch: 531 / 787  loss: 0.12240494127874657  hr: 1  min: 24  sec: 48\n",
      "epoch: 0  batch: 532 / 787  loss: 0.12220819093132938  hr: 1  min: 24  sec: 47\n",
      "epoch: 0  batch: 533 / 787  loss: 0.12200229977717096  hr: 1  min: 24  sec: 43\n",
      "epoch: 0  batch: 534 / 787  loss: 0.12180972647633445  hr: 1  min: 24  sec: 38\n",
      "epoch: 0  batch: 535 / 787  loss: 0.12184678047338379  hr: 1  min: 24  sec: 34\n",
      "epoch: 0  batch: 536 / 787  loss: 0.12183971334693593  hr: 1  min: 24  sec: 30\n",
      "epoch: 0  batch: 537 / 787  loss: 0.12176060202068456  hr: 1  min: 24  sec: 25\n",
      "epoch: 0  batch: 538 / 787  loss: 0.12163930133566758  hr: 1  min: 24  sec: 20\n",
      "epoch: 0  batch: 539 / 787  loss: 0.1214501134159669  hr: 1  min: 24  sec: 15\n",
      "epoch: 0  batch: 540 / 787  loss: 0.12128587948434331  hr: 1  min: 24  sec: 14\n",
      "epoch: 0  batch: 541 / 787  loss: 0.12111521267819317  hr: 1  min: 24  sec: 14\n",
      "epoch: 0  batch: 542 / 787  loss: 0.12096690354916882  hr: 1  min: 24  sec: 11\n",
      "epoch: 0  batch: 543 / 787  loss: 0.12081359191000132  hr: 1  min: 24  sec: 7\n",
      "epoch: 0  batch: 544 / 787  loss: 0.12062368957284729  hr: 1  min: 24  sec: 5\n",
      "epoch: 0  batch: 545 / 787  loss: 0.12053093046565122  hr: 1  min: 24  sec: 2\n",
      "epoch: 0  batch: 546 / 787  loss: 0.12034393565886187  hr: 1  min: 24  sec: 1\n",
      "epoch: 0  batch: 547 / 787  loss: 0.12023597162972856  hr: 1  min: 24  sec: 0\n",
      "epoch: 0  batch: 548 / 787  loss: 0.12005906019115099  hr: 1  min: 24  sec: 0\n",
      "epoch: 0  batch: 549 / 787  loss: 0.11993702792160497  hr: 1  min: 23  sec: 56\n",
      "epoch: 0  batch: 550 / 787  loss: 0.11983031591231173  hr: 1  min: 23  sec: 52\n",
      "epoch: 0  batch: 551 / 787  loss: 0.11965742159276498  hr: 1  min: 23  sec: 50\n",
      "epoch: 0  batch: 552 / 787  loss: 0.11955348754882057  hr: 1  min: 23  sec: 47\n",
      "epoch: 0  batch: 553 / 787  loss: 0.11939208761234826  hr: 1  min: 23  sec: 42\n",
      "epoch: 0  batch: 554 / 787  loss: 0.11925779899669683  hr: 1  min: 23  sec: 41\n",
      "epoch: 0  batch: 555 / 787  loss: 0.11907555175995505  hr: 1  min: 23  sec: 37\n",
      "epoch: 0  batch: 556 / 787  loss: 0.11899253383962263  hr: 1  min: 23  sec: 32\n",
      "epoch: 0  batch: 557 / 787  loss: 0.11888217309045085  hr: 1  min: 23  sec: 28\n",
      "epoch: 0  batch: 558 / 787  loss: 0.11880791100162652  hr: 1  min: 23  sec: 22\n",
      "epoch: 0  batch: 559 / 787  loss: 0.1186613862724285  hr: 1  min: 23  sec: 21\n",
      "epoch: 0  batch: 560 / 787  loss: 0.11846387595869601  hr: 1  min: 23  sec: 16\n",
      "epoch: 0  batch: 561 / 787  loss: 0.11834100603313583  hr: 1  min: 23  sec: 13\n",
      "epoch: 0  batch: 562 / 787  loss: 0.11816270762782716  hr: 1  min: 23  sec: 14\n",
      "epoch: 0  batch: 563 / 787  loss: 0.11804044641699926  hr: 1  min: 23  sec: 13\n",
      "epoch: 0  batch: 564 / 787  loss: 0.11784671241886491  hr: 1  min: 23  sec: 10\n",
      "epoch: 0  batch: 565 / 787  loss: 0.11782410668489417  hr: 1  min: 23  sec: 5\n",
      "epoch: 0  batch: 566 / 787  loss: 0.11769544782919987  hr: 1  min: 23  sec: 3\n",
      "epoch: 0  batch: 567 / 787  loss: 0.1176067737310573  hr: 1  min: 22  sec: 58\n",
      "epoch: 0  batch: 568 / 787  loss: 0.11760561712066525  hr: 1  min: 22  sec: 55\n",
      "epoch: 0  batch: 569 / 787  loss: 0.11744194511350356  hr: 1  min: 22  sec: 51\n",
      "epoch: 0  batch: 570 / 787  loss: 0.11729201340655747  hr: 1  min: 22  sec: 48\n",
      "epoch: 0  batch: 571 / 787  loss: 0.11721984169979281  hr: 1  min: 22  sec: 47\n",
      "epoch: 0  batch: 572 / 787  loss: 0.11705234541127887  hr: 1  min: 22  sec: 45\n",
      "epoch: 0  batch: 573 / 787  loss: 0.11688042094810731  hr: 1  min: 22  sec: 43\n",
      "epoch: 0  batch: 574 / 787  loss: 0.11673370865741517  hr: 1  min: 22  sec: 39\n",
      "epoch: 0  batch: 575 / 787  loss: 0.11663493834921847  hr: 1  min: 22  sec: 33\n",
      "epoch: 0  batch: 576 / 787  loss: 0.11645517097106979  hr: 1  min: 22  sec: 32\n",
      "epoch: 0  batch: 577 / 787  loss: 0.11629445046554457  hr: 1  min: 22  sec: 29\n",
      "epoch: 0  batch: 578 / 787  loss: 0.11621613107189846  hr: 1  min: 22  sec: 23\n",
      "epoch: 0  batch: 579 / 787  loss: 0.11603517589096578  hr: 1  min: 22  sec: 20\n",
      "epoch: 0  batch: 580 / 787  loss: 0.11596337476552561  hr: 1  min: 22  sec: 17\n",
      "epoch: 0  batch: 581 / 787  loss: 0.11588518107999213  hr: 1  min: 22  sec: 14\n",
      "epoch: 0  batch: 582 / 787  loss: 0.11574555777294938  hr: 1  min: 22  sec: 11\n",
      "epoch: 0  batch: 583 / 787  loss: 0.11561025289968806  hr: 1  min: 22  sec: 8\n",
      "epoch: 0  batch: 584 / 787  loss: 0.11552107159992399  hr: 1  min: 22  sec: 5\n",
      "epoch: 0  batch: 585 / 787  loss: 0.11544103196416146  hr: 1  min: 22  sec: 2\n",
      "epoch: 0  batch: 586 / 787  loss: 0.11525652821433219  hr: 1  min: 21  sec: 59\n",
      "epoch: 0  batch: 587 / 787  loss: 0.11514748997821313  hr: 1  min: 21  sec: 56\n",
      "epoch: 0  batch: 588 / 787  loss: 0.11500565475803248  hr: 1  min: 21  sec: 53\n",
      "epoch: 0  batch: 589 / 787  loss: 0.11487193123543404  hr: 1  min: 21  sec: 52\n",
      "epoch: 0  batch: 590 / 787  loss: 0.1147615445177939  hr: 1  min: 21  sec: 47\n",
      "epoch: 0  batch: 591 / 787  loss: 0.1145955044240415  hr: 1  min: 21  sec: 45\n",
      "epoch: 0  batch: 592 / 787  loss: 0.11443197016440634  hr: 1  min: 21  sec: 51\n",
      "epoch: 0  batch: 593 / 787  loss: 0.11431252873780655  hr: 1  min: 21  sec: 49\n",
      "epoch: 0  batch: 594 / 787  loss: 0.11415230437348085  hr: 1  min: 21  sec: 47\n",
      "epoch: 0  batch: 595 / 787  loss: 0.11403264217078686  hr: 1  min: 21  sec: 44\n",
      "epoch: 0  batch: 596 / 787  loss: 0.11387903277993502  hr: 1  min: 21  sec: 40\n",
      "epoch: 0  batch: 597 / 787  loss: 0.11371259102863034  hr: 1  min: 21  sec: 45\n",
      "epoch: 0  batch: 598 / 787  loss: 0.11361433119422766  hr: 1  min: 21  sec: 41\n",
      "epoch: 0  batch: 599 / 787  loss: 0.11349772234511048  hr: 1  min: 21  sec: 38\n",
      "epoch: 0  batch: 600 / 787  loss: 0.1133232743324091  hr: 1  min: 21  sec: 36\n",
      "epoch: 0  batch: 601 / 787  loss: 0.1131652150513874  hr: 1  min: 21  sec: 33\n",
      "epoch: 0  batch: 602 / 787  loss: 0.11301157507191464  hr: 1  min: 21  sec: 30\n",
      "epoch: 0  batch: 603 / 787  loss: 0.11285684589191555  hr: 1  min: 21  sec: 27\n",
      "epoch: 0  batch: 604 / 787  loss: 0.11273127925329354  hr: 1  min: 21  sec: 24\n",
      "epoch: 0  batch: 605 / 787  loss: 0.1125623612718518  hr: 1  min: 21  sec: 27\n",
      "epoch: 0  batch: 606 / 787  loss: 0.11251317693547595  hr: 1  min: 21  sec: 24\n",
      "epoch: 0  batch: 607 / 787  loss: 0.11241967633483192  hr: 1  min: 21  sec: 21\n",
      "epoch: 0  batch: 608 / 787  loss: 0.11227049304656439  hr: 1  min: 21  sec: 18\n",
      "epoch: 0  batch: 609 / 787  loss: 0.11213567338070307  hr: 1  min: 21  sec: 16\n",
      "epoch: 0  batch: 610 / 787  loss: 0.11203801393905868  hr: 1  min: 21  sec: 14\n",
      "epoch: 0  batch: 611 / 787  loss: 0.11185951814559973  hr: 1  min: 21  sec: 20\n",
      "epoch: 0  batch: 612 / 787  loss: 0.11173241362211545  hr: 1  min: 21  sec: 20\n",
      "epoch: 0  batch: 613 / 787  loss: 0.11160188780369792  hr: 1  min: 21  sec: 17\n",
      "epoch: 0  batch: 614 / 787  loss: 0.11143080428954068  hr: 1  min: 21  sec: 14\n",
      "epoch: 0  batch: 615 / 787  loss: 0.11127339230994202  hr: 1  min: 21  sec: 11\n",
      "epoch: 0  batch: 616 / 787  loss: 0.11114770075035668  hr: 1  min: 21  sec: 8\n",
      "epoch: 0  batch: 617 / 787  loss: 0.11100061746077186  hr: 1  min: 21  sec: 5\n",
      "epoch: 0  batch: 618 / 787  loss: 0.11085733038510429  hr: 1  min: 21  sec: 4\n",
      "epoch: 0  batch: 619 / 787  loss: 0.11071336078186714  hr: 1  min: 21  sec: 1\n",
      "epoch: 0  batch: 620 / 787  loss: 0.11058275143413114  hr: 1  min: 20  sec: 57\n",
      "epoch: 0  batch: 621 / 787  loss: 0.11060314036601324  hr: 1  min: 20  sec: 52\n",
      "epoch: 0  batch: 622 / 787  loss: 0.11045046832144464  hr: 1  min: 20  sec: 49\n",
      "epoch: 0  batch: 623 / 787  loss: 0.11032395840272678  hr: 1  min: 20  sec: 45\n",
      "epoch: 0  batch: 624 / 787  loss: 0.11017270252080216  hr: 1  min: 20  sec: 42\n",
      "epoch: 0  batch: 625 / 787  loss: 0.11003826011903584  hr: 1  min: 20  sec: 39\n",
      "epoch: 0  batch: 626 / 787  loss: 0.1099115016445452  hr: 1  min: 20  sec: 34\n",
      "epoch: 0  batch: 627 / 787  loss: 0.10982878304657005  hr: 1  min: 20  sec: 30\n",
      "epoch: 0  batch: 628 / 787  loss: 0.10972742059981379  hr: 1  min: 20  sec: 26\n",
      "epoch: 0  batch: 629 / 787  loss: 0.10957302141450465  hr: 1  min: 20  sec: 23\n",
      "epoch: 0  batch: 630 / 787  loss: 0.10945383957402396  hr: 1  min: 20  sec: 19\n",
      "epoch: 0  batch: 631 / 787  loss: 0.10930518228774698  hr: 1  min: 20  sec: 16\n",
      "epoch: 0  batch: 632 / 787  loss: 0.10919510981228639  hr: 1  min: 20  sec: 13\n",
      "epoch: 0  batch: 633 / 787  loss: 0.10904966520055787  hr: 1  min: 20  sec: 10\n",
      "epoch: 0  batch: 634 / 787  loss: 0.10894737876724005  hr: 1  min: 20  sec: 6\n",
      "epoch: 0  batch: 635 / 787  loss: 0.10879932447835275  hr: 1  min: 20  sec: 3\n",
      "epoch: 0  batch: 636 / 787  loss: 0.1087150379093569  hr: 1  min: 19  sec: 58\n",
      "epoch: 0  batch: 637 / 787  loss: 0.10856328863581845  hr: 1  min: 19  sec: 54\n",
      "epoch: 0  batch: 638 / 787  loss: 0.10843945060364704  hr: 1  min: 19  sec: 49\n",
      "epoch: 0  batch: 639 / 787  loss: 0.1083005719540527  hr: 1  min: 19  sec: 48\n",
      "epoch: 0  batch: 640 / 787  loss: 0.10819634020917875  hr: 1  min: 19  sec: 45\n",
      "epoch: 0  batch: 641 / 787  loss: 0.1080966565671541  hr: 1  min: 19  sec: 42\n",
      "epoch: 0  batch: 642 / 787  loss: 0.10800681754582593  hr: 1  min: 19  sec: 39\n",
      "epoch: 0  batch: 643 / 787  loss: 0.1079333177054356  hr: 1  min: 19  sec: 36\n",
      "epoch: 0  batch: 644 / 787  loss: 0.10782878844086255  hr: 1  min: 19  sec: 32\n",
      "epoch: 0  batch: 645 / 787  loss: 0.10769624160999765  hr: 1  min: 19  sec: 30\n",
      "epoch: 0  batch: 646 / 787  loss: 0.10760495022996026  hr: 1  min: 19  sec: 27\n",
      "epoch: 0  batch: 647 / 787  loss: 0.10748722457968943  hr: 1  min: 19  sec: 24\n",
      "epoch: 0  batch: 648 / 787  loss: 0.10734413623885987  hr: 1  min: 19  sec: 22\n",
      "epoch: 0  batch: 649 / 787  loss: 0.10723220789338676  hr: 1  min: 19  sec: 18\n",
      "epoch: 0  batch: 650 / 787  loss: 0.1071067025931552  hr: 1  min: 19  sec: 14\n",
      "epoch: 0  batch: 651 / 787  loss: 0.10702701672495339  hr: 1  min: 19  sec: 11\n",
      "epoch: 0  batch: 652 / 787  loss: 0.10692656440324681  hr: 1  min: 19  sec: 6\n",
      "epoch: 0  batch: 653 / 787  loss: 0.10681637223263715  hr: 1  min: 19  sec: 3\n",
      "epoch: 0  batch: 654 / 787  loss: 0.10675979932714595  hr: 1  min: 18  sec: 58\n",
      "epoch: 0  batch: 655 / 787  loss: 0.10668913292243565  hr: 1  min: 18  sec: 55\n",
      "epoch: 0  batch: 656 / 787  loss: 0.10664715580592533  hr: 1  min: 18  sec: 51\n",
      "epoch: 0  batch: 657 / 787  loss: 0.10651806017201071  hr: 1  min: 18  sec: 48\n",
      "epoch: 0  batch: 658 / 787  loss: 0.10637834362330419  hr: 1  min: 18  sec: 46\n",
      "epoch: 0  batch: 659 / 787  loss: 0.10623922343761553  hr: 1  min: 18  sec: 42\n",
      "epoch: 0  batch: 660 / 787  loss: 0.10609517368579735  hr: 1  min: 18  sec: 40\n",
      "epoch: 0  batch: 661 / 787  loss: 0.10596457746411067  hr: 1  min: 18  sec: 41\n",
      "epoch: 0  batch: 662 / 787  loss: 0.10582361350101883  hr: 1  min: 18  sec: 39\n",
      "epoch: 0  batch: 663 / 787  loss: 0.10571576399850866  hr: 1  min: 18  sec: 35\n",
      "epoch: 0  batch: 664 / 787  loss: 0.10557215614279407  hr: 1  min: 18  sec: 40\n",
      "epoch: 0  batch: 665 / 787  loss: 0.10547926236623267  hr: 1  min: 18  sec: 37\n",
      "epoch: 0  batch: 666 / 787  loss: 0.10537170601467527  hr: 1  min: 18  sec: 34\n",
      "epoch: 0  batch: 667 / 787  loss: 0.10529055448655301  hr: 1  min: 18  sec: 32\n",
      "epoch: 0  batch: 668 / 787  loss: 0.10517432308974704  hr: 1  min: 18  sec: 28\n",
      "epoch: 0  batch: 669 / 787  loss: 0.10504707569076842  hr: 1  min: 18  sec: 24\n",
      "epoch: 0  batch: 670 / 787  loss: 0.10494970042404454  hr: 1  min: 18  sec: 20\n",
      "epoch: 0  batch: 671 / 787  loss: 0.10483398795962034  hr: 1  min: 18  sec: 17\n",
      "epoch: 0  batch: 672 / 787  loss: 0.10469302059867741  hr: 1  min: 18  sec: 15\n",
      "epoch: 0  batch: 673 / 787  loss: 0.10455181463386304  hr: 1  min: 18  sec: 13\n",
      "epoch: 0  batch: 674 / 787  loss: 0.10443017355861413  hr: 1  min: 18  sec: 10\n",
      "epoch: 0  batch: 675 / 787  loss: 0.10429661730407841  hr: 1  min: 18  sec: 6\n",
      "epoch: 0  batch: 676 / 787  loss: 0.10416055516892295  hr: 1  min: 18  sec: 2\n",
      "epoch: 0  batch: 677 / 787  loss: 0.10409806926178979  hr: 1  min: 17  sec: 59\n",
      "epoch: 0  batch: 678 / 787  loss: 0.10404582727037008  hr: 1  min: 17  sec: 56\n",
      "epoch: 0  batch: 679 / 787  loss: 0.10398308923351407  hr: 1  min: 17  sec: 52\n",
      "epoch: 0  batch: 680 / 787  loss: 0.10393637528759428  hr: 1  min: 17  sec: 48\n",
      "epoch: 0  batch: 681 / 787  loss: 0.10389237557045525  hr: 1  min: 17  sec: 45\n",
      "epoch: 0  batch: 682 / 787  loss: 0.1037619523980581  hr: 1  min: 17  sec: 42\n",
      "epoch: 0  batch: 683 / 787  loss: 0.10365887262822998  hr: 1  min: 17  sec: 39\n",
      "epoch: 0  batch: 684 / 787  loss: 0.10358817070451624  hr: 1  min: 17  sec: 35\n",
      "epoch: 0  batch: 685 / 787  loss: 0.10348775484191294  hr: 1  min: 17  sec: 32\n",
      "epoch: 0  batch: 686 / 787  loss: 0.10335000210581483  hr: 1  min: 17  sec: 30\n",
      "epoch: 0  batch: 687 / 787  loss: 0.1032222715973057  hr: 1  min: 17  sec: 26\n",
      "epoch: 0  batch: 688 / 787  loss: 0.10307977762145022  hr: 1  min: 17  sec: 24\n",
      "epoch: 0  batch: 689 / 787  loss: 0.10296129882883102  hr: 1  min: 17  sec: 20\n",
      "epoch: 0  batch: 690 / 787  loss: 0.10284141436171974  hr: 1  min: 17  sec: 21\n",
      "epoch: 0  batch: 691 / 787  loss: 0.10270576606849653  hr: 1  min: 17  sec: 17\n",
      "epoch: 0  batch: 692 / 787  loss: 0.10257710505315948  hr: 1  min: 17  sec: 14\n",
      "epoch: 0  batch: 693 / 787  loss: 0.10246040566601289  hr: 1  min: 17  sec: 9\n",
      "epoch: 0  batch: 694 / 787  loss: 0.10232941444959191  hr: 1  min: 17  sec: 5\n",
      "epoch: 0  batch: 695 / 787  loss: 0.10231296488070499  hr: 1  min: 17  sec: 1\n",
      "epoch: 0  batch: 696 / 787  loss: 0.10218425960870106  hr: 1  min: 16  sec: 58\n",
      "epoch: 0  batch: 697 / 787  loss: 0.102074557766011  hr: 1  min: 16  sec: 55\n",
      "epoch: 0  batch: 698 / 787  loss: 0.1019475812584103  hr: 1  min: 16  sec: 52\n",
      "epoch: 0  batch: 699 / 787  loss: 0.10182464384082145  hr: 1  min: 16  sec: 49\n",
      "epoch: 0  batch: 700 / 787  loss: 0.10169149844009163  hr: 1  min: 16  sec: 46\n",
      "epoch: 0  batch: 701 / 787  loss: 0.10158098037102221  hr: 1  min: 16  sec: 42\n",
      "epoch: 0  batch: 702 / 787  loss: 0.10147619587627964  hr: 1  min: 16  sec: 37\n",
      "epoch: 0  batch: 703 / 787  loss: 0.10136850838811742  hr: 1  min: 16  sec: 34\n",
      "epoch: 0  batch: 704 / 787  loss: 0.10124402948878758  hr: 1  min: 16  sec: 31\n",
      "epoch: 0  batch: 705 / 787  loss: 0.10116027431952246  hr: 1  min: 16  sec: 27\n",
      "epoch: 0  batch: 706 / 787  loss: 0.10102043536336032  hr: 1  min: 16  sec: 25\n",
      "epoch: 0  batch: 707 / 787  loss: 0.10089915706822188  hr: 1  min: 16  sec: 22\n",
      "epoch: 0  batch: 708 / 787  loss: 0.1007785796750821  hr: 1  min: 16  sec: 21\n",
      "epoch: 0  batch: 709 / 787  loss: 0.10066321226590863  hr: 1  min: 16  sec: 17\n",
      "epoch: 0  batch: 710 / 787  loss: 0.10053656055054194  hr: 1  min: 16  sec: 14\n",
      "epoch: 0  batch: 711 / 787  loss: 0.10044086237245471  hr: 1  min: 16  sec: 11\n",
      "epoch: 0  batch: 712 / 787  loss: 0.1003455352456717  hr: 1  min: 16  sec: 8\n",
      "epoch: 0  batch: 713 / 787  loss: 0.10024299624089941  hr: 1  min: 16  sec: 5\n",
      "epoch: 0  batch: 714 / 787  loss: 0.1001374160281333  hr: 1  min: 16  sec: 2\n",
      "epoch: 0  batch: 715 / 787  loss: 0.10003587073171055  hr: 1  min: 15  sec: 59\n",
      "epoch: 0  batch: 716 / 787  loss: 0.09993147279536507  hr: 1  min: 15  sec: 57\n",
      "epoch: 0  batch: 717 / 787  loss: 0.09983754908013959  hr: 1  min: 15  sec: 53\n",
      "epoch: 0  batch: 718 / 787  loss: 0.0997777676656012  hr: 1  min: 15  sec: 49\n",
      "epoch: 0  batch: 719 / 787  loss: 0.09967119028851369  hr: 1  min: 15  sec: 46\n",
      "epoch: 0  batch: 720 / 787  loss: 0.09960294832320263  hr: 1  min: 15  sec: 44\n",
      "epoch: 0  batch: 721 / 787  loss: 0.09956530097939188  hr: 1  min: 15  sec: 40\n",
      "epoch: 0  batch: 722 / 787  loss: 0.09943314686345345  hr: 1  min: 15  sec: 38\n",
      "epoch: 0  batch: 723 / 787  loss: 0.0993494782081572  hr: 1  min: 15  sec: 35\n",
      "epoch: 0  batch: 724 / 787  loss: 0.09922988928509721  hr: 1  min: 15  sec: 31\n",
      "epoch: 0  batch: 725 / 787  loss: 0.0991169151984926  hr: 1  min: 15  sec: 27\n",
      "epoch: 0  batch: 726 / 787  loss: 0.09906614767438822  hr: 1  min: 15  sec: 25\n",
      "epoch: 0  batch: 727 / 787  loss: 0.09897903936749844  hr: 1  min: 15  sec: 21\n",
      "epoch: 0  batch: 728 / 787  loss: 0.09885795021833914  hr: 1  min: 15  sec: 21\n",
      "epoch: 0  batch: 729 / 787  loss: 0.09873190758921033  hr: 1  min: 15  sec: 19\n",
      "epoch: 0  batch: 730 / 787  loss: 0.09883525475272781  hr: 1  min: 15  sec: 15\n",
      "epoch: 0  batch: 731 / 787  loss: 0.09870704052913756  hr: 1  min: 15  sec: 12\n",
      "epoch: 0  batch: 732 / 787  loss: 0.09859791908430984  hr: 1  min: 15  sec: 8\n",
      "epoch: 0  batch: 733 / 787  loss: 0.09851352701161824  hr: 1  min: 15  sec: 5\n",
      "epoch: 0  batch: 734 / 787  loss: 0.09839677292033928  hr: 1  min: 15  sec: 2\n",
      "epoch: 0  batch: 735 / 787  loss: 0.09827271902642283  hr: 1  min: 14  sec: 59\n",
      "epoch: 0  batch: 736 / 787  loss: 0.09817366150673479  hr: 1  min: 14  sec: 55\n",
      "epoch: 0  batch: 737 / 787  loss: 0.09806101107047403  hr: 1  min: 14  sec: 53\n",
      "epoch: 0  batch: 738 / 787  loss: 0.09799043307721939  hr: 1  min: 14  sec: 50\n",
      "epoch: 0  batch: 739 / 787  loss: 0.09786900224514478  hr: 1  min: 14  sec: 52\n",
      "epoch: 0  batch: 740 / 787  loss: 0.0978072773300212  hr: 1  min: 14  sec: 51\n",
      "epoch: 0  batch: 741 / 787  loss: 0.0977259524777411  hr: 1  min: 14  sec: 48\n",
      "epoch: 0  batch: 742 / 787  loss: 0.09770165925413732  hr: 1  min: 14  sec: 45\n",
      "epoch: 0  batch: 743 / 787  loss: 0.09762594946515953  hr: 1  min: 14  sec: 43\n",
      "epoch: 0  batch: 744 / 787  loss: 0.09750783177448176  hr: 1  min: 14  sec: 41\n",
      "epoch: 0  batch: 745 / 787  loss: 0.09741241152979584  hr: 1  min: 14  sec: 38\n",
      "epoch: 0  batch: 746 / 787  loss: 0.09730424956061526  hr: 1  min: 14  sec: 35\n",
      "epoch: 0  batch: 747 / 787  loss: 0.09720537263072199  hr: 1  min: 14  sec: 34\n",
      "epoch: 0  batch: 748 / 787  loss: 0.09708802439870044  hr: 1  min: 14  sec: 31\n",
      "epoch: 0  batch: 749 / 787  loss: 0.09705909986163332  hr: 1  min: 14  sec: 27\n",
      "epoch: 0  batch: 750 / 787  loss: 0.09693525337055325  hr: 1  min: 14  sec: 31\n",
      "epoch: 0  batch: 751 / 787  loss: 0.09682852863030292  hr: 1  min: 14  sec: 28\n",
      "epoch: 0  batch: 752 / 787  loss: 0.0967098962121762  hr: 1  min: 14  sec: 25\n",
      "epoch: 0  batch: 753 / 787  loss: 0.09659421278329525  hr: 1  min: 14  sec: 25\n",
      "epoch: 0  batch: 754 / 787  loss: 0.09648202038901199  hr: 1  min: 14  sec: 23\n",
      "epoch: 0  batch: 755 / 787  loss: 0.09641543189495407  hr: 1  min: 14  sec: 22\n",
      "epoch: 0  batch: 756 / 787  loss: 0.0963521196085605  hr: 1  min: 14  sec: 17\n",
      "epoch: 0  batch: 757 / 787  loss: 0.09628396588433018  hr: 1  min: 14  sec: 15\n",
      "epoch: 0  batch: 758 / 787  loss: 0.09619425444453365  hr: 1  min: 14  sec: 11\n",
      "epoch: 0  batch: 759 / 787  loss: 0.09610476278195622  hr: 1  min: 14  sec: 9\n",
      "epoch: 0  batch: 760 / 787  loss: 0.09599693415098284  hr: 1  min: 14  sec: 5\n",
      "epoch: 0  batch: 761 / 787  loss: 0.09588772391731534  hr: 1  min: 14  sec: 1\n",
      "epoch: 0  batch: 762 / 787  loss: 0.09578460342619871  hr: 1  min: 13  sec: 58\n",
      "epoch: 0  batch: 763 / 787  loss: 0.09568741511270346  hr: 1  min: 13  sec: 56\n",
      "epoch: 0  batch: 764 / 787  loss: 0.09559918898499098  hr: 1  min: 13  sec: 52\n",
      "epoch: 0  batch: 765 / 787  loss: 0.09549981147610868  hr: 1  min: 13  sec: 50\n",
      "epoch: 0  batch: 766 / 787  loss: 0.09540039431863019  hr: 1  min: 13  sec: 47\n",
      "epoch: 0  batch: 767 / 787  loss: 0.09536811815892897  hr: 1  min: 13  sec: 45\n",
      "epoch: 0  batch: 768 / 787  loss: 0.09526667055494424  hr: 1  min: 13  sec: 41\n",
      "epoch: 0  batch: 769 / 787  loss: 0.09516708037521696  hr: 1  min: 13  sec: 37\n",
      "epoch: 0  batch: 770 / 787  loss: 0.09508335784488878  hr: 1  min: 13  sec: 34\n",
      "epoch: 0  batch: 771 / 787  loss: 0.09498462982441935  hr: 1  min: 13  sec: 31\n",
      "epoch: 0  batch: 772 / 787  loss: 0.09487926223860581  hr: 1  min: 13  sec: 29\n",
      "epoch: 0  batch: 773 / 787  loss: 0.0947653654695309  hr: 1  min: 13  sec: 27\n",
      "epoch: 0  batch: 774 / 787  loss: 0.09465561816420705  hr: 1  min: 13  sec: 23\n",
      "epoch: 0  batch: 775 / 787  loss: 0.09456776654648204  hr: 1  min: 13  sec: 19\n",
      "epoch: 0  batch: 776 / 787  loss: 0.09447548910135346  hr: 1  min: 13  sec: 16\n",
      "epoch: 0  batch: 777 / 787  loss: 0.09437725393693688  hr: 1  min: 13  sec: 13\n",
      "epoch: 0  batch: 778 / 787  loss: 0.09426641205067154  hr: 1  min: 13  sec: 10\n",
      "epoch: 0  batch: 779 / 787  loss: 0.09416155155332083  hr: 1  min: 13  sec: 7\n",
      "epoch: 0  batch: 780 / 787  loss: 0.09407175179952994  hr: 1  min: 13  sec: 4\n",
      "epoch: 0  batch: 781 / 787  loss: 0.09395914516267433  hr: 1  min: 13  sec: 1\n",
      "epoch: 0  batch: 782 / 787  loss: 0.09385487313091735  hr: 1  min: 12  sec: 58\n",
      "epoch: 0  batch: 783 / 787  loss: 0.09375456169112033  hr: 1  min: 12  sec: 54\n",
      "epoch: 0  batch: 784 / 787  loss: 0.09364026142652526  hr: 1  min: 12  sec: 52\n",
      "epoch: 0  batch: 785 / 787  loss: 0.0935492625252409  hr: 1  min: 12  sec: 49\n",
      "epoch: 0  batch: 786 / 787  loss: 0.0935422298741354  hr: 1  min: 12  sec: 45\n",
      "epoch: 0  batch: 787 / 787  loss: 0.09343426581531328  hr: 1  min: 12  sec: 42\n",
      "epoch: 1  batch: 1 / 787  loss: 0.005050524137914181  hr: 1  min: 15  sec: 8\n",
      "epoch: 1  batch: 2 / 787  loss: 0.0046804966405034065  hr: 1  min: 11  sec: 55\n",
      "epoch: 1  batch: 3 / 787  loss: 0.0074263618638118105  hr: 1  min: 14  sec: 48\n",
      "epoch: 1  batch: 4 / 787  loss: 0.008792988257482648  hr: 1  min: 14  sec: 24\n",
      "epoch: 1  batch: 5 / 787  loss: 0.007769027166068554  hr: 1  min: 17  sec: 9\n",
      "epoch: 1  batch: 6 / 787  loss: 0.01099360284085075  hr: 1  min: 15  sec: 46\n",
      "epoch: 1  batch: 7 / 787  loss: 0.02661325183830091  hr: 1  min: 13  sec: 12\n",
      "epoch: 1  batch: 8 / 787  loss: 0.023542833427200094  hr: 1  min: 12  sec: 35\n",
      "epoch: 1  batch: 9 / 787  loss: 0.023734189906261034  hr: 1  min: 12  sec: 3\n",
      "epoch: 1  batch: 10 / 787  loss: 0.02152583405841142  hr: 1  min: 11  sec: 45\n",
      "epoch: 1  batch: 11 / 787  loss: 0.019768047315830536  hr: 1  min: 12  sec: 39\n",
      "epoch: 1  batch: 12 / 787  loss: 0.018196722681750543  hr: 1  min: 13  sec: 58\n",
      "epoch: 1  batch: 13 / 787  loss: 0.017113390589097086  hr: 1  min: 19  sec: 56\n",
      "epoch: 1  batch: 14 / 787  loss: 0.016670820631718795  hr: 1  min: 19  sec: 53\n",
      "epoch: 1  batch: 15 / 787  loss: 0.0180265488064227  hr: 1  min: 18  sec: 58\n",
      "epoch: 1  batch: 16 / 787  loss: 0.018564458059699973  hr: 1  min: 17  sec: 53\n",
      "epoch: 1  batch: 17 / 787  loss: 0.018481191299061346  hr: 1  min: 17  sec: 10\n",
      "epoch: 1  batch: 18 / 787  loss: 0.020135652736320883  hr: 1  min: 15  sec: 48\n",
      "epoch: 1  batch: 19 / 787  loss: 0.01959428117353175  hr: 1  min: 15  sec: 22\n",
      "epoch: 1  batch: 20 / 787  loss: 0.018755528560723177  hr: 1  min: 15  sec: 3\n",
      "epoch: 1  batch: 21 / 787  loss: 0.018863343276149993  hr: 1  min: 14  sec: 44\n",
      "epoch: 1  batch: 22 / 787  loss: 0.019893644078613514  hr: 1  min: 14  sec: 11\n",
      "epoch: 1  batch: 23 / 787  loss: 0.01929523568833247  hr: 1  min: 13  sec: 30\n",
      "epoch: 1  batch: 24 / 787  loss: 0.01908055622819423  hr: 1  min: 13  sec: 44\n",
      "epoch: 1  batch: 25 / 787  loss: 0.018772638847585767  hr: 1  min: 13  sec: 41\n",
      "epoch: 1  batch: 26 / 787  loss: 0.018144864595692176  hr: 1  min: 13  sec: 15\n",
      "epoch: 1  batch: 27 / 787  loss: 0.018437202543416922  hr: 1  min: 13  sec: 14\n",
      "epoch: 1  batch: 28 / 787  loss: 0.017976311474090574  hr: 1  min: 13  sec: 12\n",
      "epoch: 1  batch: 29 / 787  loss: 0.01793564859866004  hr: 1  min: 13  sec: 20\n",
      "epoch: 1  batch: 30 / 787  loss: 0.018176307379811383  hr: 1  min: 13  sec: 16\n",
      "epoch: 1  batch: 31 / 787  loss: 0.017769380868788088  hr: 1  min: 13  sec: 23\n",
      "epoch: 1  batch: 32 / 787  loss: 0.017491174139649956  hr: 1  min: 13  sec: 33\n",
      "epoch: 1  batch: 33 / 787  loss: 0.017093422051843707  hr: 1  min: 12  sec: 53\n",
      "epoch: 1  batch: 34 / 787  loss: 0.01666522090946434  hr: 1  min: 12  sec: 44\n",
      "epoch: 1  batch: 35 / 787  loss: 0.016314953158143908  hr: 1  min: 12  sec: 42\n",
      "epoch: 1  batch: 36 / 787  loss: 0.015974969504491635  hr: 1  min: 12  sec: 15\n",
      "epoch: 1  batch: 37 / 787  loss: 0.01650972292299156  hr: 1  min: 11  sec: 29\n",
      "epoch: 1  batch: 38 / 787  loss: 0.016199265550025495  hr: 1  min: 11  sec: 12\n",
      "epoch: 1  batch: 39 / 787  loss: 0.017758105515848655  hr: 1  min: 10  sec: 59\n",
      "epoch: 1  batch: 40 / 787  loss: 0.017562688297766728  hr: 1  min: 10  sec: 52\n",
      "epoch: 1  batch: 41 / 787  loss: 0.01720759864163971  hr: 1  min: 10  sec: 43\n",
      "epoch: 1  batch: 42 / 787  loss: 0.016854597115612012  hr: 1  min: 10  sec: 29\n",
      "epoch: 1  batch: 43 / 787  loss: 0.016530186050235793  hr: 1  min: 10  sec: 15\n",
      "epoch: 1  batch: 44 / 787  loss: 0.01630441019369755  hr: 1  min: 10  sec: 14\n",
      "epoch: 1  batch: 45 / 787  loss: 0.016187687470422436  hr: 1  min: 10  sec: 8\n",
      "epoch: 1  batch: 46 / 787  loss: 0.015931206840120823  hr: 1  min: 9  sec: 39\n",
      "epoch: 1  batch: 47 / 787  loss: 0.015712860717804745  hr: 1  min: 9  sec: 36\n",
      "epoch: 1  batch: 48 / 787  loss: 0.01565162674887688  hr: 1  min: 9  sec: 37\n",
      "epoch: 1  batch: 49 / 787  loss: 0.015497151309652825  hr: 1  min: 9  sec: 41\n",
      "epoch: 1  batch: 50 / 787  loss: 0.015280093339970335  hr: 1  min: 9  sec: 29\n",
      "epoch: 1  batch: 51 / 787  loss: 0.015228270309949842  hr: 1  min: 9  sec: 31\n",
      "epoch: 1  batch: 52 / 787  loss: 0.014967672140650952  hr: 1  min: 9  sec: 27\n",
      "epoch: 1  batch: 53 / 787  loss: 0.01756855076561102  hr: 1  min: 9  sec: 17\n",
      "epoch: 1  batch: 54 / 787  loss: 0.017281614415373445  hr: 1  min: 9  sec: 25\n",
      "epoch: 1  batch: 55 / 787  loss: 0.017049052167302844  hr: 1  min: 10  sec: 12\n",
      "epoch: 1  batch: 56 / 787  loss: 0.016921292698368364  hr: 1  min: 10  sec: 47\n",
      "epoch: 1  batch: 57 / 787  loss: 0.016759174874893864  hr: 1  min: 12  sec: 3\n",
      "epoch: 1  batch: 58 / 787  loss: 0.01674190489474909  hr: 1  min: 11  sec: 58\n",
      "epoch: 1  batch: 59 / 787  loss: 0.0165383689711265  hr: 1  min: 11  sec: 54\n",
      "epoch: 1  batch: 60 / 787  loss: 0.016392641645506956  hr: 1  min: 11  sec: 35\n",
      "epoch: 1  batch: 61 / 787  loss: 0.01633123458710453  hr: 1  min: 11  sec: 36\n",
      "epoch: 1  batch: 62 / 787  loss: 0.01629015117519415  hr: 1  min: 11  sec: 39\n",
      "epoch: 1  batch: 63 / 787  loss: 0.016230754338995745  hr: 1  min: 12  sec: 47\n",
      "epoch: 1  batch: 64 / 787  loss: 0.016241612515841553  hr: 1  min: 12  sec: 26\n",
      "epoch: 1  batch: 65 / 787  loss: 0.016270013202805644  hr: 1  min: 12  sec: 22\n",
      "epoch: 1  batch: 66 / 787  loss: 0.01617815357431854  hr: 1  min: 12  sec: 6\n",
      "epoch: 1  batch: 67 / 787  loss: 0.016246803027556846  hr: 1  min: 11  sec: 37\n",
      "epoch: 1  batch: 68 / 787  loss: 0.01609915517431492  hr: 1  min: 11  sec: 36\n",
      "epoch: 1  batch: 69 / 787  loss: 0.01616558773304754  hr: 1  min: 11  sec: 22\n",
      "epoch: 1  batch: 70 / 787  loss: 0.016438656040034923  hr: 1  min: 11  sec: 17\n",
      "epoch: 1  batch: 71 / 787  loss: 0.016323995060766195  hr: 1  min: 11  sec: 14\n",
      "epoch: 1  batch: 72 / 787  loss: 0.01619491184747959  hr: 1  min: 11  sec: 2\n",
      "epoch: 1  batch: 73 / 787  loss: 0.015996169728667107  hr: 1  min: 11  sec: 5\n",
      "epoch: 1  batch: 74 / 787  loss: 0.0158202680279312  hr: 1  min: 10  sec: 59\n",
      "epoch: 1  batch: 75 / 787  loss: 0.01582270146890854  hr: 1  min: 10  sec: 43\n",
      "epoch: 1  batch: 76 / 787  loss: 0.015652183993982983  hr: 1  min: 10  sec: 37\n",
      "epoch: 1  batch: 77 / 787  loss: 0.015572079214684714  hr: 1  min: 10  sec: 21\n",
      "epoch: 1  batch: 78 / 787  loss: 0.015405709305270694  hr: 1  min: 10  sec: 6\n",
      "epoch: 1  batch: 79 / 787  loss: 0.015301618199718857  hr: 1  min: 10  sec: 4\n",
      "epoch: 1  batch: 80 / 787  loss: 0.015187279320525705  hr: 1  min: 9  sec: 44\n",
      "epoch: 1  batch: 81 / 787  loss: 0.01560098811176341  hr: 1  min: 9  sec: 33\n",
      "epoch: 1  batch: 82 / 787  loss: 0.015639176432514654  hr: 1  min: 9  sec: 38\n",
      "epoch: 1  batch: 83 / 787  loss: 0.01573044546327206  hr: 1  min: 9  sec: 32\n",
      "epoch: 1  batch: 84 / 787  loss: 0.015551814927935734  hr: 1  min: 9  sec: 53\n",
      "epoch: 1  batch: 85 / 787  loss: 0.01540327550336609  hr: 1  min: 9  sec: 52\n",
      "epoch: 1  batch: 86 / 787  loss: 0.015268007593180682  hr: 1  min: 9  sec: 52\n",
      "epoch: 1  batch: 87 / 787  loss: 0.015357840610213107  hr: 1  min: 9  sec: 45\n",
      "epoch: 1  batch: 88 / 787  loss: 0.015208476164182437  hr: 1  min: 9  sec: 42\n",
      "epoch: 1  batch: 89 / 787  loss: 0.015074676276168921  hr: 1  min: 9  sec: 45\n",
      "epoch: 1  batch: 90 / 787  loss: 0.014996549939193452  hr: 1  min: 9  sec: 44\n",
      "epoch: 1  batch: 91 / 787  loss: 0.014922247623262292  hr: 1  min: 9  sec: 45\n",
      "epoch: 1  batch: 92 / 787  loss: 0.014840743131809058  hr: 1  min: 9  sec: 46\n",
      "epoch: 1  batch: 93 / 787  loss: 0.014759702604859867  hr: 1  min: 9  sec: 48\n",
      "epoch: 1  batch: 94 / 787  loss: 0.014859917550719283  hr: 1  min: 9  sec: 41\n",
      "epoch: 1  batch: 95 / 787  loss: 0.014939984480090636  hr: 1  min: 9  sec: 23\n",
      "epoch: 1  batch: 96 / 787  loss: 0.014863845986231658  hr: 1  min: 9  sec: 22\n",
      "epoch: 1  batch: 97 / 787  loss: 0.014730382220430742  hr: 1  min: 9  sec: 14\n",
      "epoch: 1  batch: 98 / 787  loss: 0.014607981915885051  hr: 1  min: 9  sec: 10\n",
      "epoch: 1  batch: 99 / 787  loss: 0.014533161744828138  hr: 1  min: 9  sec: 10\n",
      "epoch: 1  batch: 100 / 787  loss: 0.014398186936159619  hr: 1  min: 9  sec: 20\n",
      "epoch: 1  batch: 101 / 787  loss: 0.01459864425844061  hr: 1  min: 9  sec: 9\n",
      "epoch: 1  batch: 102 / 787  loss: 0.01446822740413778  hr: 1  min: 9  sec: 8\n",
      "epoch: 1  batch: 103 / 787  loss: 0.014409295347116852  hr: 1  min: 9  sec: 5\n",
      "epoch: 1  batch: 104 / 787  loss: 0.014321523553427631  hr: 1  min: 8  sec: 58\n",
      "epoch: 1  batch: 105 / 787  loss: 0.014395921815386308  hr: 1  min: 8  sec: 51\n",
      "epoch: 1  batch: 106 / 787  loss: 0.01435018573801982  hr: 1  min: 8  sec: 56\n",
      "epoch: 1  batch: 107 / 787  loss: 0.014233612625698654  hr: 1  min: 8  sec: 48\n",
      "epoch: 1  batch: 108 / 787  loss: 0.014136773435489481  hr: 1  min: 8  sec: 48\n",
      "epoch: 1  batch: 109 / 787  loss: 0.014246031595500818  hr: 1  min: 8  sec: 42\n",
      "epoch: 1  batch: 110 / 787  loss: 0.014139033805854111  hr: 1  min: 8  sec: 31\n",
      "epoch: 1  batch: 111 / 787  loss: 0.014026788420459206  hr: 1  min: 8  sec: 25\n",
      "epoch: 1  batch: 112 / 787  loss: 0.01390828345392947  hr: 1  min: 8  sec: 15\n",
      "epoch: 1  batch: 113 / 787  loss: 0.01415403253363808  hr: 1  min: 8  sec: 10\n",
      "epoch: 1  batch: 114 / 787  loss: 0.014046345633687451  hr: 1  min: 8  sec: 13\n",
      "epoch: 1  batch: 115 / 787  loss: 0.013940724449576405  hr: 1  min: 8  sec: 7\n",
      "epoch: 1  batch: 116 / 787  loss: 0.013891668864305066  hr: 1  min: 8  sec: 6\n",
      "epoch: 1  batch: 117 / 787  loss: 0.013988034563381862  hr: 1  min: 7  sec: 53\n",
      "epoch: 1  batch: 118 / 787  loss: 0.013902068708853606  hr: 1  min: 8  sec: 6\n",
      "epoch: 1  batch: 119 / 787  loss: 0.01390996143591645  hr: 1  min: 7  sec: 58\n",
      "epoch: 1  batch: 120 / 787  loss: 0.013806847688101698  hr: 1  min: 8  sec: 11\n",
      "epoch: 1  batch: 121 / 787  loss: 0.01369714386730401  hr: 1  min: 8  sec: 7\n",
      "epoch: 1  batch: 122 / 787  loss: 0.013669524273117547  hr: 1  min: 8  sec: 10\n",
      "epoch: 1  batch: 123 / 787  loss: 0.01365674268908617  hr: 1  min: 8  sec: 4\n",
      "epoch: 1  batch: 124 / 787  loss: 0.0138109217007314  hr: 1  min: 7  sec: 59\n",
      "epoch: 1  batch: 125 / 787  loss: 0.013714842155575752  hr: 1  min: 7  sec: 53\n",
      "epoch: 1  batch: 126 / 787  loss: 0.013757982468676  hr: 1  min: 7  sec: 46\n",
      "epoch: 1  batch: 127 / 787  loss: 0.013663058470765965  hr: 1  min: 7  sec: 45\n",
      "epoch: 1  batch: 128 / 787  loss: 0.013597163198028284  hr: 1  min: 7  sec: 43\n",
      "epoch: 1  batch: 129 / 787  loss: 0.013534957796602343  hr: 1  min: 8  sec: 3\n",
      "epoch: 1  batch: 130 / 787  loss: 0.01345511409543598  hr: 1  min: 8  sec: 3\n",
      "epoch: 1  batch: 131 / 787  loss: 0.013519822425237422  hr: 1  min: 8  sec: 4\n",
      "epoch: 1  batch: 132 / 787  loss: 0.013436578334993541  hr: 1  min: 8  sec: 5\n",
      "epoch: 1  batch: 133 / 787  loss: 0.013354886135023395  hr: 1  min: 7  sec: 59\n",
      "epoch: 1  batch: 134 / 787  loss: 0.013337588070634641  hr: 1  min: 7  sec: 45\n",
      "epoch: 1  batch: 135 / 787  loss: 0.01326872740647997  hr: 1  min: 7  sec: 39\n",
      "epoch: 1  batch: 136 / 787  loss: 0.013202018006054191  hr: 1  min: 7  sec: 28\n",
      "epoch: 1  batch: 137 / 787  loss: 0.013603150638085483  hr: 1  min: 7  sec: 24\n",
      "epoch: 1  batch: 138 / 787  loss: 0.013523547069095344  hr: 1  min: 7  sec: 22\n",
      "epoch: 1  batch: 139 / 787  loss: 0.013439699681643262  hr: 1  min: 7  sec: 13\n",
      "epoch: 1  batch: 140 / 787  loss: 0.013479476085298562  hr: 1  min: 7  sec: 4\n",
      "epoch: 1  batch: 141 / 787  loss: 0.013401122809163168  hr: 1  min: 7  sec: 14\n",
      "epoch: 1  batch: 142 / 787  loss: 0.0138537550638561  hr: 1  min: 7  sec: 8\n",
      "epoch: 1  batch: 143 / 787  loss: 0.013921786952027856  hr: 1  min: 7  sec: 3\n",
      "epoch: 1  batch: 144 / 787  loss: 0.013948267706534049  hr: 1  min: 6  sec: 59\n",
      "epoch: 1  batch: 145 / 787  loss: 0.014345627938846833  hr: 1  min: 6  sec: 50\n",
      "epoch: 1  batch: 146 / 787  loss: 0.014279012350614297  hr: 1  min: 6  sec: 48\n",
      "epoch: 1  batch: 147 / 787  loss: 0.01420946604474036  hr: 1  min: 6  sec: 38\n",
      "epoch: 1  batch: 148 / 787  loss: 0.014122025048473498  hr: 1  min: 6  sec: 36\n",
      "epoch: 1  batch: 149 / 787  loss: 0.014228862105562482  hr: 1  min: 6  sec: 37\n",
      "epoch: 1  batch: 150 / 787  loss: 0.014172999804529051  hr: 1  min: 6  sec: 32\n",
      "epoch: 1  batch: 151 / 787  loss: 0.01409222378043939  hr: 1  min: 6  sec: 33\n",
      "epoch: 1  batch: 152 / 787  loss: 0.014088386816805914  hr: 1  min: 6  sec: 29\n",
      "epoch: 1  batch: 153 / 787  loss: 0.01405429993482197  hr: 1  min: 6  sec: 22\n",
      "epoch: 1  batch: 154 / 787  loss: 0.014008563834351378  hr: 1  min: 6  sec: 23\n",
      "epoch: 1  batch: 155 / 787  loss: 0.013931046749254869  hr: 1  min: 6  sec: 16\n",
      "epoch: 1  batch: 156 / 787  loss: 0.01385950545171419  hr: 1  min: 6  sec: 8\n",
      "epoch: 1  batch: 157 / 787  loss: 0.013802721031650806  hr: 1  min: 6  sec: 9\n",
      "epoch: 1  batch: 158 / 787  loss: 0.013749131493718375  hr: 1  min: 6  sec: 3\n",
      "epoch: 1  batch: 159 / 787  loss: 0.01369944844781509  hr: 1  min: 6  sec: 0\n",
      "epoch: 1  batch: 160 / 787  loss: 0.013645771465962752  hr: 1  min: 5  sec: 56\n",
      "epoch: 1  batch: 161 / 787  loss: 0.014270701110131623  hr: 1  min: 5  sec: 50\n",
      "epoch: 1  batch: 162 / 787  loss: 0.01430342273418734  hr: 1  min: 5  sec: 42\n",
      "epoch: 1  batch: 163 / 787  loss: 0.014258056784768594  hr: 1  min: 5  sec: 39\n",
      "epoch: 1  batch: 164 / 787  loss: 0.014345153799939265  hr: 1  min: 5  sec: 35\n",
      "epoch: 1  batch: 165 / 787  loss: 0.014269482140485762  hr: 1  min: 5  sec: 36\n",
      "epoch: 1  batch: 166 / 787  loss: 0.014193483080619565  hr: 1  min: 5  sec: 30\n",
      "epoch: 1  batch: 167 / 787  loss: 0.014167682442697  hr: 1  min: 5  sec: 26\n",
      "epoch: 1  batch: 168 / 787  loss: 0.01420750072256418  hr: 1  min: 5  sec: 17\n",
      "epoch: 1  batch: 169 / 787  loss: 0.014127449275782475  hr: 1  min: 5  sec: 28\n",
      "epoch: 1  batch: 170 / 787  loss: 0.014095603581517934  hr: 1  min: 5  sec: 24\n",
      "epoch: 1  batch: 171 / 787  loss: 0.01402224414050579  hr: 1  min: 5  sec: 21\n",
      "epoch: 1  batch: 172 / 787  loss: 0.013989669433253448  hr: 1  min: 5  sec: 19\n",
      "epoch: 1  batch: 173 / 787  loss: 0.014093360039970778  hr: 1  min: 5  sec: 15\n",
      "epoch: 1  batch: 174 / 787  loss: 0.01402008369225131  hr: 1  min: 5  sec: 9\n",
      "epoch: 1  batch: 175 / 787  loss: 0.014054773157861616  hr: 1  min: 5  sec: 13\n",
      "epoch: 1  batch: 176 / 787  loss: 0.014024539443860042  hr: 1  min: 5  sec: 10\n",
      "epoch: 1  batch: 177 / 787  loss: 0.013958408618950953  hr: 1  min: 5  sec: 5\n",
      "epoch: 1  batch: 178 / 787  loss: 0.013896454713605554  hr: 1  min: 5  sec: 6\n",
      "epoch: 1  batch: 179 / 787  loss: 0.013966205131194903  hr: 1  min: 4  sec: 59\n",
      "epoch: 1  batch: 180 / 787  loss: 0.01403391277611566  hr: 1  min: 4  sec: 51\n",
      "epoch: 1  batch: 181 / 787  loss: 0.014019616114783476  hr: 1  min: 4  sec: 51\n",
      "epoch: 1  batch: 182 / 787  loss: 0.014077133726159746  hr: 1  min: 4  sec: 47\n",
      "epoch: 1  batch: 183 / 787  loss: 0.014053292009506191  hr: 1  min: 4  sec: 43\n",
      "epoch: 1  batch: 184 / 787  loss: 0.014166860645712839  hr: 1  min: 4  sec: 41\n",
      "epoch: 1  batch: 185 / 787  loss: 0.014143737120478339  hr: 1  min: 4  sec: 33\n",
      "epoch: 1  batch: 186 / 787  loss: 0.014142007603915908  hr: 1  min: 4  sec: 28\n",
      "epoch: 1  batch: 187 / 787  loss: 0.014112878861627915  hr: 1  min: 4  sec: 20\n",
      "epoch: 1  batch: 188 / 787  loss: 0.014052429536798415  hr: 1  min: 4  sec: 13\n",
      "epoch: 1  batch: 189 / 787  loss: 0.014016072682804729  hr: 1  min: 4  sec: 8\n",
      "epoch: 1  batch: 190 / 787  loss: 0.014028951112107424  hr: 1  min: 4  sec: 5\n",
      "epoch: 1  batch: 191 / 787  loss: 0.014017065560947421  hr: 1  min: 4  sec: 3\n",
      "epoch: 1  batch: 192 / 787  loss: 0.013967692086225725  hr: 1  min: 3  sec: 56\n",
      "epoch: 1  batch: 193 / 787  loss: 0.013939771275856373  hr: 1  min: 3  sec: 54\n",
      "epoch: 1  batch: 194 / 787  loss: 0.013942851363945291  hr: 1  min: 3  sec: 49\n",
      "epoch: 1  batch: 195 / 787  loss: 0.013901522335930703  hr: 1  min: 3  sec: 50\n",
      "epoch: 1  batch: 196 / 787  loss: 0.01386175183306106  hr: 1  min: 3  sec: 44\n",
      "epoch: 1  batch: 197 / 787  loss: 0.013796968664501221  hr: 1  min: 3  sec: 42\n",
      "epoch: 1  batch: 198 / 787  loss: 0.01378713593338475  hr: 1  min: 3  sec: 35\n",
      "epoch: 1  batch: 199 / 787  loss: 0.013736731922675167  hr: 1  min: 3  sec: 38\n",
      "epoch: 1  batch: 200 / 787  loss: 0.013714905914384871  hr: 1  min: 3  sec: 33\n",
      "epoch: 1  batch: 201 / 787  loss: 0.013678570002298895  hr: 1  min: 3  sec: 32\n",
      "epoch: 1  batch: 202 / 787  loss: 0.01362887919202705  hr: 1  min: 3  sec: 38\n",
      "epoch: 1  batch: 203 / 787  loss: 0.01365297574783891  hr: 1  min: 3  sec: 36\n",
      "epoch: 1  batch: 204 / 787  loss: 0.013610498800499402  hr: 1  min: 3  sec: 30\n",
      "epoch: 1  batch: 205 / 787  loss: 0.013943880690806886  hr: 1  min: 3  sec: 24\n",
      "epoch: 1  batch: 206 / 787  loss: 0.013957874861871371  hr: 1  min: 3  sec: 20\n",
      "epoch: 1  batch: 207 / 787  loss: 0.013929138597829835  hr: 1  min: 3  sec: 16\n",
      "epoch: 1  batch: 208 / 787  loss: 0.013869408953514021  hr: 1  min: 3  sec: 9\n",
      "epoch: 1  batch: 209 / 787  loss: 0.013852090076833037  hr: 1  min: 3  sec: 4\n",
      "epoch: 1  batch: 210 / 787  loss: 0.013820527561585463  hr: 1  min: 3  sec: 2\n",
      "epoch: 1  batch: 211 / 787  loss: 0.013781436177200122  hr: 1  min: 2  sec: 55\n",
      "epoch: 1  batch: 212 / 787  loss: 0.0137777579300334  hr: 1  min: 2  sec: 50\n",
      "epoch: 1  batch: 213 / 787  loss: 0.013811151900420517  hr: 1  min: 2  sec: 46\n",
      "epoch: 1  batch: 214 / 787  loss: 0.01376729019347439  hr: 1  min: 2  sec: 46\n",
      "epoch: 1  batch: 215 / 787  loss: 0.013708682775194216  hr: 1  min: 2  sec: 42\n",
      "epoch: 1  batch: 216 / 787  loss: 0.013685476398138606  hr: 1  min: 2  sec: 41\n",
      "epoch: 1  batch: 217 / 787  loss: 0.013628193090373676  hr: 1  min: 2  sec: 37\n",
      "epoch: 1  batch: 218 / 787  loss: 0.013779026160332794  hr: 1  min: 2  sec: 36\n",
      "epoch: 1  batch: 219 / 787  loss: 0.013750827527864407  hr: 1  min: 2  sec: 51\n",
      "epoch: 1  batch: 220 / 787  loss: 0.013709705201273953  hr: 1  min: 2  sec: 50\n",
      "epoch: 1  batch: 221 / 787  loss: 0.01373172186133852  hr: 1  min: 2  sec: 47\n",
      "epoch: 1  batch: 222 / 787  loss: 0.013685899454185756  hr: 1  min: 2  sec: 46\n",
      "epoch: 1  batch: 223 / 787  loss: 0.013633732149385817  hr: 1  min: 2  sec: 44\n",
      "epoch: 1  batch: 224 / 787  loss: 0.013848602551401459  hr: 1  min: 2  sec: 38\n",
      "epoch: 1  batch: 225 / 787  loss: 0.013889756571087573  hr: 1  min: 2  sec: 36\n",
      "epoch: 1  batch: 226 / 787  loss: 0.01387147758716503  hr: 1  min: 2  sec: 27\n",
      "epoch: 1  batch: 227 / 787  loss: 0.013898602034486339  hr: 1  min: 2  sec: 21\n",
      "epoch: 1  batch: 228 / 787  loss: 0.01386364269593175  hr: 1  min: 2  sec: 16\n",
      "epoch: 1  batch: 229 / 787  loss: 0.013822340030288201  hr: 1  min: 2  sec: 10\n",
      "epoch: 1  batch: 230 / 787  loss: 0.013851869173104997  hr: 1  min: 2  sec: 2\n",
      "epoch: 1  batch: 231 / 787  loss: 0.013796393734854504  hr: 1  min: 1  sec: 58\n",
      "epoch: 1  batch: 232 / 787  loss: 0.013806564704783999  hr: 1  min: 1  sec: 54\n",
      "epoch: 1  batch: 233 / 787  loss: 0.013766153683074522  hr: 1  min: 1  sec: 51\n",
      "epoch: 1  batch: 234 / 787  loss: 0.014092516340613842  hr: 1  min: 1  sec: 48\n",
      "epoch: 1  batch: 235 / 787  loss: 0.014039922895980009  hr: 1  min: 1  sec: 47\n",
      "epoch: 1  batch: 236 / 787  loss: 0.01398790933160206  hr: 1  min: 1  sec: 45\n",
      "epoch: 1  batch: 237 / 787  loss: 0.014058114976508204  hr: 1  min: 1  sec: 40\n",
      "epoch: 1  batch: 238 / 787  loss: 0.014137436447962493  hr: 1  min: 1  sec: 37\n",
      "epoch: 1  batch: 239 / 787  loss: 0.014109160451054823  hr: 1  min: 1  sec: 34\n",
      "epoch: 1  batch: 240 / 787  loss: 0.014106409309897571  hr: 1  min: 1  sec: 33\n",
      "epoch: 1  batch: 241 / 787  loss: 0.014058555592299744  hr: 1  min: 1  sec: 34\n",
      "epoch: 1  batch: 242 / 787  loss: 0.014029438120566985  hr: 1  min: 1  sec: 29\n",
      "epoch: 1  batch: 243 / 787  loss: 0.013981560995388363  hr: 1  min: 1  sec: 27\n",
      "epoch: 1  batch: 244 / 787  loss: 0.013961207399289811  hr: 1  min: 1  sec: 22\n",
      "epoch: 1  batch: 245 / 787  loss: 0.014136792281262425  hr: 1  min: 1  sec: 16\n",
      "epoch: 1  batch: 246 / 787  loss: 0.014095117299196621  hr: 1  min: 1  sec: 15\n",
      "epoch: 1  batch: 247 / 787  loss: 0.01406209176002817  hr: 1  min: 1  sec: 13\n",
      "epoch: 1  batch: 248 / 787  loss: 0.01401804466525303  hr: 1  min: 1  sec: 9\n",
      "epoch: 1  batch: 249 / 787  loss: 0.014018804931743857  hr: 1  min: 1  sec: 7\n",
      "epoch: 1  batch: 250 / 787  loss: 0.014059473005123436  hr: 1  min: 1  sec: 3\n",
      "epoch: 1  batch: 251 / 787  loss: 0.014097255922712949  hr: 1  min: 0  sec: 59\n",
      "epoch: 1  batch: 252 / 787  loss: 0.014099472269807602  hr: 1  min: 0  sec: 56\n",
      "epoch: 1  batch: 253 / 787  loss: 0.014081344300174254  hr: 1  min: 0  sec: 56\n",
      "epoch: 1  batch: 254 / 787  loss: 0.014166877110943374  hr: 1  min: 0  sec: 52\n",
      "epoch: 1  batch: 255 / 787  loss: 0.014192414375971638  hr: 1  min: 0  sec: 47\n",
      "epoch: 1  batch: 256 / 787  loss: 0.014152252927488007  hr: 1  min: 0  sec: 46\n",
      "epoch: 1  batch: 257 / 787  loss: 0.014195793889035337  hr: 1  min: 0  sec: 44\n",
      "epoch: 1  batch: 258 / 787  loss: 0.014165555389576354  hr: 1  min: 0  sec: 37\n",
      "epoch: 1  batch: 259 / 787  loss: 0.01412700506333535  hr: 1  min: 0  sec: 31\n",
      "epoch: 1  batch: 260 / 787  loss: 0.014098100259434431  hr: 1  min: 0  sec: 27\n",
      "epoch: 1  batch: 261 / 787  loss: 0.014084556064179517  hr: 1  min: 0  sec: 40\n",
      "epoch: 1  batch: 262 / 787  loss: 0.014060160329117518  hr: 1  min: 0  sec: 36\n",
      "epoch: 1  batch: 263 / 787  loss: 0.014031394525562119  hr: 1  min: 0  sec: 32\n",
      "epoch: 1  batch: 264 / 787  loss: 0.014000789354957471  hr: 1  min: 0  sec: 29\n",
      "epoch: 1  batch: 265 / 787  loss: 0.013954427023857552  hr: 1  min: 0  sec: 23\n",
      "epoch: 1  batch: 266 / 787  loss: 0.013936233515080932  hr: 1  min: 0  sec: 19\n",
      "epoch: 1  batch: 267 / 787  loss: 0.013975200809238388  hr: 1  min: 0  sec: 16\n",
      "epoch: 1  batch: 268 / 787  loss: 0.01393442263708575  hr: 1  min: 0  sec: 11\n",
      "epoch: 1  batch: 269 / 787  loss: 0.01396174632449077  hr: 1  min: 0  sec: 7\n",
      "epoch: 1  batch: 270 / 787  loss: 0.013930470920685264  hr: 1  min: 0  sec: 5\n",
      "epoch: 1  batch: 271 / 787  loss: 0.013891326952588184  hr: 1  min: 0  sec: 2\n",
      "epoch: 1  batch: 272 / 787  loss: 0.013848222978755502  hr: 0  min: 59  sec: 57\n",
      "epoch: 1  batch: 273 / 787  loss: 0.013827841563470961  hr: 0  min: 59  sec: 54\n",
      "epoch: 1  batch: 274 / 787  loss: 0.01382236628806776  hr: 0  min: 59  sec: 52\n",
      "epoch: 1  batch: 275 / 787  loss: 0.013778526551428843  hr: 0  min: 59  sec: 47\n",
      "epoch: 1  batch: 276 / 787  loss: 0.013768183849026462  hr: 0  min: 59  sec: 45\n",
      "epoch: 1  batch: 277 / 787  loss: 0.01373085278693237  hr: 0  min: 59  sec: 48\n",
      "epoch: 1  batch: 278 / 787  loss: 0.013702616569160638  hr: 0  min: 59  sec: 45\n",
      "epoch: 1  batch: 279 / 787  loss: 0.013700054602540602  hr: 0  min: 59  sec: 40\n",
      "epoch: 1  batch: 280 / 787  loss: 0.013663957587726015  hr: 0  min: 59  sec: 39\n",
      "epoch: 1  batch: 281 / 787  loss: 0.013626366848328226  hr: 0  min: 59  sec: 33\n",
      "epoch: 1  batch: 282 / 787  loss: 0.013749484667564406  hr: 0  min: 59  sec: 29\n",
      "epoch: 1  batch: 283 / 787  loss: 0.01372516287002028  hr: 0  min: 59  sec: 29\n",
      "epoch: 1  batch: 284 / 787  loss: 0.013723277599586857  hr: 0  min: 59  sec: 23\n",
      "epoch: 1  batch: 285 / 787  loss: 0.013955668446183074  hr: 0  min: 59  sec: 20\n",
      "epoch: 1  batch: 286 / 787  loss: 0.013924272176956146  hr: 0  min: 59  sec: 15\n",
      "epoch: 1  batch: 287 / 787  loss: 0.013908716654875527  hr: 0  min: 59  sec: 13\n",
      "epoch: 1  batch: 288 / 787  loss: 0.013877965659857081  hr: 0  min: 59  sec: 10\n",
      "epoch: 1  batch: 289 / 787  loss: 0.013870553875255003  hr: 0  min: 59  sec: 6\n",
      "epoch: 1  batch: 290 / 787  loss: 0.013898683241006501  hr: 0  min: 59  sec: 7\n",
      "epoch: 1  batch: 291 / 787  loss: 0.013891717522641919  hr: 0  min: 59  sec: 6\n",
      "epoch: 1  batch: 292 / 787  loss: 0.013869910084320335  hr: 0  min: 59  sec: 4\n",
      "epoch: 1  batch: 293 / 787  loss: 0.013878809553157831  hr: 0  min: 59  sec: 2\n",
      "epoch: 1  batch: 294 / 787  loss: 0.013867891850947802  hr: 0  min: 59  sec: 1\n",
      "epoch: 1  batch: 295 / 787  loss: 0.013826259583140077  hr: 0  min: 59  sec: 0\n",
      "epoch: 1  batch: 296 / 787  loss: 0.0137832224879546  hr: 0  min: 58  sec: 57\n",
      "epoch: 1  batch: 297 / 787  loss: 0.013744316915573821  hr: 0  min: 58  sec: 56\n",
      "epoch: 1  batch: 298 / 787  loss: 0.013754194610574026  hr: 0  min: 58  sec: 52\n",
      "epoch: 1  batch: 299 / 787  loss: 0.013809377024258584  hr: 0  min: 58  sec: 51\n",
      "epoch: 1  batch: 300 / 787  loss: 0.013795491756948953  hr: 0  min: 58  sec: 49\n",
      "epoch: 1  batch: 301 / 787  loss: 0.013757177683110178  hr: 0  min: 58  sec: 43\n",
      "epoch: 1  batch: 302 / 787  loss: 0.01380887280552901  hr: 0  min: 58  sec: 38\n",
      "epoch: 1  batch: 303 / 787  loss: 0.013787272471163065  hr: 0  min: 58  sec: 35\n",
      "epoch: 1  batch: 304 / 787  loss: 0.013754871456752115  hr: 0  min: 58  sec: 33\n",
      "epoch: 1  batch: 305 / 787  loss: 0.013742376470434494  hr: 0  min: 58  sec: 32\n",
      "epoch: 1  batch: 306 / 787  loss: 0.013722038914957784  hr: 0  min: 58  sec: 29\n",
      "epoch: 1  batch: 307 / 787  loss: 0.013695239602491041  hr: 0  min: 58  sec: 28\n",
      "epoch: 1  batch: 308 / 787  loss: 0.01367990658414405  hr: 0  min: 58  sec: 25\n",
      "epoch: 1  batch: 309 / 787  loss: 0.01364617226200787  hr: 0  min: 58  sec: 22\n",
      "epoch: 1  batch: 310 / 787  loss: 0.013612402227872442  hr: 0  min: 58  sec: 18\n",
      "epoch: 1  batch: 311 / 787  loss: 0.013975553499411947  hr: 0  min: 58  sec: 14\n",
      "epoch: 1  batch: 312 / 787  loss: 0.013937126245507851  hr: 0  min: 58  sec: 12\n",
      "epoch: 1  batch: 313 / 787  loss: 0.013899193197777406  hr: 0  min: 58  sec: 7\n",
      "epoch: 1  batch: 314 / 787  loss: 0.01387715860081623  hr: 0  min: 58  sec: 1\n",
      "epoch: 1  batch: 315 / 787  loss: 0.013848965411399683  hr: 0  min: 57  sec: 58\n",
      "epoch: 1  batch: 316 / 787  loss: 0.013840461695416466  hr: 0  min: 57  sec: 55\n",
      "epoch: 1  batch: 317 / 787  loss: 0.013912483547778935  hr: 0  min: 57  sec: 51\n",
      "epoch: 1  batch: 318 / 787  loss: 0.013943224510180898  hr: 0  min: 57  sec: 48\n",
      "epoch: 1  batch: 319 / 787  loss: 0.013922319147913925  hr: 0  min: 57  sec: 44\n",
      "epoch: 1  batch: 320 / 787  loss: 0.01390452976229426  hr: 0  min: 57  sec: 41\n",
      "epoch: 1  batch: 321 / 787  loss: 0.013889635546420153  hr: 0  min: 57  sec: 36\n",
      "epoch: 1  batch: 322 / 787  loss: 0.01388447404505037  hr: 0  min: 57  sec: 36\n",
      "epoch: 1  batch: 323 / 787  loss: 0.013928825106693795  hr: 0  min: 57  sec: 34\n",
      "epoch: 1  batch: 324 / 787  loss: 0.013900009253030688  hr: 0  min: 57  sec: 33\n",
      "epoch: 1  batch: 325 / 787  loss: 0.013864010215307084  hr: 0  min: 57  sec: 30\n",
      "epoch: 1  batch: 326 / 787  loss: 0.013898253404789609  hr: 0  min: 57  sec: 27\n",
      "epoch: 1  batch: 327 / 787  loss: 0.013877840831929915  hr: 0  min: 57  sec: 23\n",
      "epoch: 1  batch: 328 / 787  loss: 0.013869982414024247  hr: 0  min: 57  sec: 21\n",
      "epoch: 1  batch: 329 / 787  loss: 0.013838647207667015  hr: 0  min: 57  sec: 16\n",
      "epoch: 1  batch: 330 / 787  loss: 0.013833574812054973  hr: 0  min: 57  sec: 16\n",
      "epoch: 1  batch: 331 / 787  loss: 0.013818650970339573  hr: 0  min: 57  sec: 13\n",
      "epoch: 1  batch: 332 / 787  loss: 0.01380214089189701  hr: 0  min: 57  sec: 8\n",
      "epoch: 1  batch: 333 / 787  loss: 0.013771650199946184  hr: 0  min: 57  sec: 3\n",
      "epoch: 1  batch: 334 / 787  loss: 0.013734458388710494  hr: 0  min: 56  sec: 58\n",
      "epoch: 1  batch: 335 / 787  loss: 0.013700382993904067  hr: 0  min: 57  sec: 7\n",
      "epoch: 1  batch: 336 / 787  loss: 0.013665488785980935  hr: 0  min: 57  sec: 15\n",
      "epoch: 1  batch: 337 / 787  loss: 0.013653937926880728  hr: 0  min: 57  sec: 12\n",
      "epoch: 1  batch: 338 / 787  loss: 0.013629343753994702  hr: 0  min: 57  sec: 11\n",
      "epoch: 1  batch: 339 / 787  loss: 0.013600875298660194  hr: 0  min: 57  sec: 7\n",
      "epoch: 1  batch: 340 / 787  loss: 0.013582899344071527  hr: 0  min: 57  sec: 3\n",
      "epoch: 1  batch: 341 / 787  loss: 0.013546928933062358  hr: 0  min: 56  sec: 59\n",
      "epoch: 1  batch: 342 / 787  loss: 0.013577037218928185  hr: 0  min: 56  sec: 57\n",
      "epoch: 1  batch: 343 / 787  loss: 0.013539358232925177  hr: 0  min: 56  sec: 54\n",
      "epoch: 1  batch: 344 / 787  loss: 0.013514538477635184  hr: 0  min: 56  sec: 50\n",
      "epoch: 1  batch: 345 / 787  loss: 0.013550776095412996  hr: 0  min: 56  sec: 48\n",
      "epoch: 1  batch: 346 / 787  loss: 0.013607182398013168  hr: 0  min: 56  sec: 43\n",
      "epoch: 1  batch: 347 / 787  loss: 0.013609931031939833  hr: 0  min: 56  sec: 40\n",
      "epoch: 1  batch: 348 / 787  loss: 0.013591385966343484  hr: 0  min: 56  sec: 35\n",
      "epoch: 1  batch: 349 / 787  loss: 0.013618742727559797  hr: 0  min: 56  sec: 32\n",
      "epoch: 1  batch: 350 / 787  loss: 0.01359199878427067  hr: 0  min: 56  sec: 27\n",
      "epoch: 1  batch: 351 / 787  loss: 0.01356976862765976  hr: 0  min: 56  sec: 26\n",
      "epoch: 1  batch: 352 / 787  loss: 0.013544373245333025  hr: 0  min: 56  sec: 23\n",
      "epoch: 1  batch: 353 / 787  loss: 0.013511437001946796  hr: 0  min: 56  sec: 19\n",
      "epoch: 1  batch: 354 / 787  loss: 0.013561845917549342  hr: 0  min: 56  sec: 14\n",
      "epoch: 1  batch: 355 / 787  loss: 0.013568827344603102  hr: 0  min: 56  sec: 11\n",
      "epoch: 1  batch: 356 / 787  loss: 0.013537403529645938  hr: 0  min: 56  sec: 10\n",
      "epoch: 1  batch: 357 / 787  loss: 0.013526518486116697  hr: 0  min: 56  sec: 6\n",
      "epoch: 1  batch: 358 / 787  loss: 0.013493515021368007  hr: 0  min: 56  sec: 4\n",
      "epoch: 1  batch: 359 / 787  loss: 0.013475296482856289  hr: 0  min: 56  sec: 0\n",
      "epoch: 1  batch: 360 / 787  loss: 0.0134857783905722  hr: 0  min: 55  sec: 56\n",
      "epoch: 1  batch: 361 / 787  loss: 0.013452965683959053  hr: 0  min: 55  sec: 51\n",
      "epoch: 1  batch: 362 / 787  loss: 0.01342799147566554  hr: 0  min: 55  sec: 51\n",
      "epoch: 1  batch: 363 / 787  loss: 0.013472344981924664  hr: 0  min: 55  sec: 47\n",
      "epoch: 1  batch: 364 / 787  loss: 0.013438364023606262  hr: 0  min: 55  sec: 43\n",
      "epoch: 1  batch: 365 / 787  loss: 0.013408214414862228  hr: 0  min: 55  sec: 41\n",
      "epoch: 1  batch: 366 / 787  loss: 0.013374330874473936  hr: 0  min: 55  sec: 39\n",
      "epoch: 1  batch: 367 / 787  loss: 0.013345847677436913  hr: 0  min: 55  sec: 36\n",
      "epoch: 1  batch: 368 / 787  loss: 0.01331345987368284  hr: 0  min: 55  sec: 35\n",
      "epoch: 1  batch: 369 / 787  loss: 0.013291562630714177  hr: 0  min: 55  sec: 31\n",
      "epoch: 1  batch: 370 / 787  loss: 0.013259299388910467  hr: 0  min: 55  sec: 28\n",
      "epoch: 1  batch: 371 / 787  loss: 0.013231589547371005  hr: 0  min: 55  sec: 26\n",
      "epoch: 1  batch: 372 / 787  loss: 0.013197693459924952  hr: 0  min: 55  sec: 23\n",
      "epoch: 1  batch: 373 / 787  loss: 0.013209508409244246  hr: 0  min: 55  sec: 20\n",
      "epoch: 1  batch: 374 / 787  loss: 0.013186852359910182  hr: 0  min: 55  sec: 17\n",
      "epoch: 1  batch: 375 / 787  loss: 0.013347580906624596  hr: 0  min: 55  sec: 14\n",
      "epoch: 1  batch: 376 / 787  loss: 0.013323617346728458  hr: 0  min: 55  sec: 13\n",
      "epoch: 1  batch: 377 / 787  loss: 0.013305936423093278  hr: 0  min: 55  sec: 15\n",
      "epoch: 1  batch: 378 / 787  loss: 0.013309026483776512  hr: 0  min: 55  sec: 12\n",
      "epoch: 1  batch: 379 / 787  loss: 0.013275378188953826  hr: 0  min: 55  sec: 9\n",
      "epoch: 1  batch: 380 / 787  loss: 0.013267780812824832  hr: 0  min: 55  sec: 6\n",
      "epoch: 1  batch: 381 / 787  loss: 0.013262018367643236  hr: 0  min: 55  sec: 4\n",
      "epoch: 1  batch: 382 / 787  loss: 0.013230954559888028  hr: 0  min: 54  sec: 59\n",
      "epoch: 1  batch: 383 / 787  loss: 0.013286438942935522  hr: 0  min: 54  sec: 54\n",
      "epoch: 1  batch: 384 / 787  loss: 0.013260355412967328  hr: 0  min: 54  sec: 50\n",
      "epoch: 1  batch: 385 / 787  loss: 0.013237315563593883  hr: 0  min: 54  sec: 46\n",
      "epoch: 1  batch: 386 / 787  loss: 0.013225404477806115  hr: 0  min: 54  sec: 44\n",
      "epoch: 1  batch: 387 / 787  loss: 0.01323075255575212  hr: 0  min: 54  sec: 40\n",
      "epoch: 1  batch: 388 / 787  loss: 0.01320864670740719  hr: 0  min: 54  sec: 41\n",
      "epoch: 1  batch: 389 / 787  loss: 0.01323351366080315  hr: 0  min: 54  sec: 39\n",
      "epoch: 1  batch: 390 / 787  loss: 0.013221462389442306  hr: 0  min: 54  sec: 34\n",
      "epoch: 1  batch: 391 / 787  loss: 0.013199922406886373  hr: 0  min: 54  sec: 30\n",
      "epoch: 1  batch: 392 / 787  loss: 0.013191849717630396  hr: 0  min: 54  sec: 26\n",
      "epoch: 1  batch: 393 / 787  loss: 0.013359015852238974  hr: 0  min: 54  sec: 21\n",
      "epoch: 1  batch: 394 / 787  loss: 0.013329406257075857  hr: 0  min: 54  sec: 22\n",
      "epoch: 1  batch: 395 / 787  loss: 0.013333181930776638  hr: 0  min: 54  sec: 18\n",
      "epoch: 1  batch: 396 / 787  loss: 0.013351951666985346  hr: 0  min: 54  sec: 15\n",
      "epoch: 1  batch: 397 / 787  loss: 0.013354258030014915  hr: 0  min: 54  sec: 10\n",
      "epoch: 1  batch: 398 / 787  loss: 0.013388382303692287  hr: 0  min: 54  sec: 6\n",
      "epoch: 1  batch: 399 / 787  loss: 0.01341802361222885  hr: 0  min: 54  sec: 3\n",
      "epoch: 1  batch: 400 / 787  loss: 0.013453832054510713  hr: 0  min: 53  sec: 59\n",
      "epoch: 1  batch: 401 / 787  loss: 0.013449134809102054  hr: 0  min: 53  sec: 55\n",
      "epoch: 1  batch: 402 / 787  loss: 0.01342990467398988  hr: 0  min: 53  sec: 52\n",
      "epoch: 1  batch: 403 / 787  loss: 0.013447053815993867  hr: 0  min: 53  sec: 49\n",
      "epoch: 1  batch: 404 / 787  loss: 0.013500347219426001  hr: 0  min: 53  sec: 45\n",
      "epoch: 1  batch: 405 / 787  loss: 0.013496986293682345  hr: 0  min: 53  sec: 41\n",
      "epoch: 1  batch: 406 / 787  loss: 0.013505624533800656  hr: 0  min: 53  sec: 38\n",
      "epoch: 1  batch: 407 / 787  loss: 0.013518782190667324  hr: 0  min: 53  sec: 34\n",
      "epoch: 1  batch: 408 / 787  loss: 0.013492280321182026  hr: 0  min: 53  sec: 34\n",
      "epoch: 1  batch: 409 / 787  loss: 0.013479802849153762  hr: 0  min: 53  sec: 29\n",
      "epoch: 1  batch: 410 / 787  loss: 0.013461660256995479  hr: 0  min: 53  sec: 26\n",
      "epoch: 1  batch: 411 / 787  loss: 0.01349969760179429  hr: 0  min: 53  sec: 23\n",
      "epoch: 1  batch: 412 / 787  loss: 0.013472638590243256  hr: 0  min: 53  sec: 19\n",
      "epoch: 1  batch: 413 / 787  loss: 0.01347443239749849  hr: 0  min: 53  sec: 15\n",
      "epoch: 1  batch: 414 / 787  loss: 0.013531310971970287  hr: 0  min: 53  sec: 9\n",
      "epoch: 1  batch: 415 / 787  loss: 0.01350256016517215  hr: 0  min: 53  sec: 6\n",
      "epoch: 1  batch: 416 / 787  loss: 0.013484407944745796  hr: 0  min: 53  sec: 3\n",
      "epoch: 1  batch: 417 / 787  loss: 0.013694281784462468  hr: 0  min: 52  sec: 59\n",
      "epoch: 1  batch: 418 / 787  loss: 0.013687045530373738  hr: 0  min: 52  sec: 56\n",
      "epoch: 1  batch: 419 / 787  loss: 0.013659736714748164  hr: 0  min: 53  sec: 1\n",
      "epoch: 1  batch: 420 / 787  loss: 0.013632814812618086  hr: 0  min: 53  sec: 6\n",
      "epoch: 1  batch: 421 / 787  loss: 0.01362751226572457  hr: 0  min: 53  sec: 4\n",
      "epoch: 1  batch: 422 / 787  loss: 0.013620331241491018  hr: 0  min: 53  sec: 0\n",
      "epoch: 1  batch: 423 / 787  loss: 0.013597025815342459  hr: 0  min: 52  sec: 56\n",
      "epoch: 1  batch: 424 / 787  loss: 0.013576587243390023  hr: 0  min: 52  sec: 53\n",
      "epoch: 1  batch: 425 / 787  loss: 0.013558774200158523  hr: 0  min: 52  sec: 52\n",
      "epoch: 1  batch: 426 / 787  loss: 0.013537445427803978  hr: 0  min: 52  sec: 48\n",
      "epoch: 1  batch: 427 / 787  loss: 0.013528884779738232  hr: 0  min: 52  sec: 44\n",
      "epoch: 1  batch: 428 / 787  loss: 0.013513869339128017  hr: 0  min: 52  sec: 42\n",
      "epoch: 1  batch: 429 / 787  loss: 0.013510880697386162  hr: 0  min: 52  sec: 38\n",
      "epoch: 1  batch: 430 / 787  loss: 0.013493112078055652  hr: 0  min: 52  sec: 35\n",
      "epoch: 1  batch: 431 / 787  loss: 0.013477196731915797  hr: 0  min: 52  sec: 32\n",
      "epoch: 1  batch: 432 / 787  loss: 0.013451943313325668  hr: 0  min: 52  sec: 30\n",
      "epoch: 1  batch: 433 / 787  loss: 0.013433211300507033  hr: 0  min: 52  sec: 27\n",
      "epoch: 1  batch: 434 / 787  loss: 0.013408921653359768  hr: 0  min: 52  sec: 24\n",
      "epoch: 1  batch: 435 / 787  loss: 0.01340714911742928  hr: 0  min: 52  sec: 21\n",
      "epoch: 1  batch: 436 / 787  loss: 0.013389061670483373  hr: 0  min: 52  sec: 19\n",
      "epoch: 1  batch: 437 / 787  loss: 0.013361460448283216  hr: 0  min: 52  sec: 16\n",
      "epoch: 1  batch: 438 / 787  loss: 0.013345375703860364  hr: 0  min: 52  sec: 14\n",
      "epoch: 1  batch: 439 / 787  loss: 0.013355716948391093  hr: 0  min: 52  sec: 14\n",
      "epoch: 1  batch: 440 / 787  loss: 0.01332994860660454  hr: 0  min: 52  sec: 11\n",
      "epoch: 1  batch: 441 / 787  loss: 0.013308511334186306  hr: 0  min: 52  sec: 6\n",
      "epoch: 1  batch: 442 / 787  loss: 0.013281171333110086  hr: 0  min: 52  sec: 3\n",
      "epoch: 1  batch: 443 / 787  loss: 0.013276633294351803  hr: 0  min: 52  sec: 0\n",
      "epoch: 1  batch: 444 / 787  loss: 0.013299911952869094  hr: 0  min: 51  sec: 56\n",
      "epoch: 1  batch: 445 / 787  loss: 0.013280440156159692  hr: 0  min: 51  sec: 53\n",
      "epoch: 1  batch: 446 / 787  loss: 0.013273712511502223  hr: 0  min: 51  sec: 50\n",
      "epoch: 1  batch: 447 / 787  loss: 0.013246804058824726  hr: 0  min: 51  sec: 47\n",
      "epoch: 1  batch: 448 / 787  loss: 0.013220398588860658  hr: 0  min: 51  sec: 46\n",
      "epoch: 1  batch: 449 / 787  loss: 0.013193528681271658  hr: 0  min: 51  sec: 43\n",
      "epoch: 1  batch: 450 / 787  loss: 0.013165600427762708  hr: 0  min: 51  sec: 48\n",
      "epoch: 1  batch: 451 / 787  loss: 0.01315577196418318  hr: 0  min: 51  sec: 44\n",
      "epoch: 1  batch: 452 / 787  loss: 0.01313728403953724  hr: 0  min: 51  sec: 41\n",
      "epoch: 1  batch: 453 / 787  loss: 0.01311020601888251  hr: 0  min: 51  sec: 40\n",
      "epoch: 1  batch: 454 / 787  loss: 0.013116521718255388  hr: 0  min: 51  sec: 35\n",
      "epoch: 1  batch: 455 / 787  loss: 0.013169142659651217  hr: 0  min: 51  sec: 32\n",
      "epoch: 1  batch: 456 / 787  loss: 0.01314238566721957  hr: 0  min: 51  sec: 30\n",
      "epoch: 1  batch: 457 / 787  loss: 0.013130928122889886  hr: 0  min: 51  sec: 27\n",
      "epoch: 1  batch: 458 / 787  loss: 0.013103333243634552  hr: 0  min: 51  sec: 23\n",
      "epoch: 1  batch: 459 / 787  loss: 0.013075942980644977  hr: 0  min: 51  sec: 19\n",
      "epoch: 1  batch: 460 / 787  loss: 0.013052397668766586  hr: 0  min: 51  sec: 16\n",
      "epoch: 1  batch: 461 / 787  loss: 0.013036775689596339  hr: 0  min: 51  sec: 14\n",
      "epoch: 1  batch: 462 / 787  loss: 0.013047451550519956  hr: 0  min: 51  sec: 9\n",
      "epoch: 1  batch: 463 / 787  loss: 0.013329396143024606  hr: 0  min: 51  sec: 4\n",
      "epoch: 1  batch: 464 / 787  loss: 0.013307590169555123  hr: 0  min: 51  sec: 1\n",
      "epoch: 1  batch: 465 / 787  loss: 0.013284233520408311  hr: 0  min: 50  sec: 59\n",
      "epoch: 1  batch: 466 / 787  loss: 0.013261836523773716  hr: 0  min: 50  sec: 56\n",
      "epoch: 1  batch: 467 / 787  loss: 0.013236753434906025  hr: 0  min: 50  sec: 53\n",
      "epoch: 1  batch: 468 / 787  loss: 0.013210125949206905  hr: 0  min: 50  sec: 51\n",
      "epoch: 1  batch: 469 / 787  loss: 0.013331113670240723  hr: 0  min: 50  sec: 49\n",
      "epoch: 1  batch: 470 / 787  loss: 0.013308980791121127  hr: 0  min: 50  sec: 49\n",
      "epoch: 1  batch: 471 / 787  loss: 0.0132960193942997  hr: 0  min: 50  sec: 46\n",
      "epoch: 1  batch: 472 / 787  loss: 0.013287540045354745  hr: 0  min: 50  sec: 42\n",
      "epoch: 1  batch: 473 / 787  loss: 0.013262983473942242  hr: 0  min: 50  sec: 39\n",
      "epoch: 1  batch: 474 / 787  loss: 0.01324925312164774  hr: 0  min: 50  sec: 35\n",
      "epoch: 1  batch: 475 / 787  loss: 0.013229295322058821  hr: 0  min: 50  sec: 33\n",
      "epoch: 1  batch: 476 / 787  loss: 0.013318050504899818  hr: 0  min: 50  sec: 30\n",
      "epoch: 1  batch: 477 / 787  loss: 0.013305117009940755  hr: 0  min: 50  sec: 28\n",
      "epoch: 1  batch: 478 / 787  loss: 0.013288168359239036  hr: 0  min: 50  sec: 25\n",
      "epoch: 1  batch: 479 / 787  loss: 0.013292360663805779  hr: 0  min: 50  sec: 22\n",
      "epoch: 1  batch: 480 / 787  loss: 0.013267679179625702  hr: 0  min: 50  sec: 20\n",
      "epoch: 1  batch: 481 / 787  loss: 0.013286168937665254  hr: 0  min: 50  sec: 16\n",
      "epoch: 1  batch: 482 / 787  loss: 0.013316367943051438  hr: 0  min: 50  sec: 13\n",
      "epoch: 1  batch: 483 / 787  loss: 0.013294164665350902  hr: 0  min: 50  sec: 10\n",
      "epoch: 1  batch: 484 / 787  loss: 0.013270999177831441  hr: 0  min: 50  sec: 7\n",
      "epoch: 1  batch: 485 / 787  loss: 0.013255581892330738  hr: 0  min: 50  sec: 4\n",
      "epoch: 1  batch: 486 / 787  loss: 0.013260727866007582  hr: 0  min: 50  sec: 1\n",
      "epoch: 1  batch: 487 / 787  loss: 0.01323939681096668  hr: 0  min: 49  sec: 59\n",
      "epoch: 1  batch: 488 / 787  loss: 0.01322326205084463  hr: 0  min: 49  sec: 55\n",
      "epoch: 1  batch: 489 / 787  loss: 0.013213143915729312  hr: 0  min: 49  sec: 52\n",
      "epoch: 1  batch: 490 / 787  loss: 0.013189102337833455  hr: 0  min: 49  sec: 49\n",
      "epoch: 1  batch: 491 / 787  loss: 0.01316576985162901  hr: 0  min: 49  sec: 45\n",
      "epoch: 1  batch: 492 / 787  loss: 0.013143033133389057  hr: 0  min: 49  sec: 42\n",
      "epoch: 1  batch: 493 / 787  loss: 0.013125011244619691  hr: 0  min: 49  sec: 38\n",
      "epoch: 1  batch: 494 / 787  loss: 0.013103221820385539  hr: 0  min: 49  sec: 35\n",
      "epoch: 1  batch: 495 / 787  loss: 0.013079417406727155  hr: 0  min: 49  sec: 33\n",
      "epoch: 1  batch: 496 / 787  loss: 0.013066008964092508  hr: 0  min: 49  sec: 31\n",
      "epoch: 1  batch: 497 / 787  loss: 0.013041687684140208  hr: 0  min: 49  sec: 28\n",
      "epoch: 1  batch: 498 / 787  loss: 0.013021060135977596  hr: 0  min: 49  sec: 26\n",
      "epoch: 1  batch: 499 / 787  loss: 0.012997788958132962  hr: 0  min: 49  sec: 23\n",
      "epoch: 1  batch: 500 / 787  loss: 0.012996684650541284  hr: 0  min: 49  sec: 19\n",
      "epoch: 1  batch: 501 / 787  loss: 0.013009917424706382  hr: 0  min: 49  sec: 15\n",
      "epoch: 1  batch: 502 / 787  loss: 0.013146317228735183  hr: 0  min: 49  sec: 13\n",
      "epoch: 1  batch: 503 / 787  loss: 0.013169798113717688  hr: 0  min: 49  sec: 9\n",
      "epoch: 1  batch: 504 / 787  loss: 0.013148015678794648  hr: 0  min: 49  sec: 4\n",
      "epoch: 1  batch: 505 / 787  loss: 0.013160424800889364  hr: 0  min: 49  sec: 1\n",
      "epoch: 1  batch: 506 / 787  loss: 0.01313876473998721  hr: 0  min: 48  sec: 57\n",
      "epoch: 1  batch: 507 / 787  loss: 0.013121277876992764  hr: 0  min: 48  sec: 54\n",
      "epoch: 1  batch: 508 / 787  loss: 0.013098669623040345  hr: 0  min: 48  sec: 51\n",
      "epoch: 1  batch: 509 / 787  loss: 0.013131906891041316  hr: 0  min: 48  sec: 49\n",
      "epoch: 1  batch: 510 / 787  loss: 0.013118933759031672  hr: 0  min: 48  sec: 46\n",
      "epoch: 1  batch: 511 / 787  loss: 0.013107750571762082  hr: 0  min: 48  sec: 43\n",
      "epoch: 1  batch: 512 / 787  loss: 0.013092523682303181  hr: 0  min: 48  sec: 45\n",
      "epoch: 1  batch: 513 / 787  loss: 0.013106338844655471  hr: 0  min: 48  sec: 41\n",
      "epoch: 1  batch: 514 / 787  loss: 0.013107079314634182  hr: 0  min: 48  sec: 38\n",
      "epoch: 1  batch: 515 / 787  loss: 0.01308659086268377  hr: 0  min: 48  sec: 36\n",
      "epoch: 1  batch: 516 / 787  loss: 0.013070236225616974  hr: 0  min: 48  sec: 33\n",
      "epoch: 1  batch: 517 / 787  loss: 0.013061000392803782  hr: 0  min: 48  sec: 31\n",
      "epoch: 1  batch: 518 / 787  loss: 0.013037516069814247  hr: 0  min: 48  sec: 28\n",
      "epoch: 1  batch: 519 / 787  loss: 0.01301517503009481  hr: 0  min: 48  sec: 25\n",
      "epoch: 1  batch: 520 / 787  loss: 0.013003447890066756  hr: 0  min: 48  sec: 22\n",
      "epoch: 1  batch: 521 / 787  loss: 0.012992270964764473  hr: 0  min: 48  sec: 19\n",
      "epoch: 1  batch: 522 / 787  loss: 0.012983754341458452  hr: 0  min: 48  sec: 16\n",
      "epoch: 1  batch: 523 / 787  loss: 0.012960383330648107  hr: 0  min: 48  sec: 12\n",
      "epoch: 1  batch: 524 / 787  loss: 0.012938189475338565  hr: 0  min: 48  sec: 9\n",
      "epoch: 1  batch: 525 / 787  loss: 0.012919692759446445  hr: 0  min: 48  sec: 6\n",
      "epoch: 1  batch: 526 / 787  loss: 0.012913385858075842  hr: 0  min: 48  sec: 3\n",
      "epoch: 1  batch: 527 / 787  loss: 0.012891351146600519  hr: 0  min: 48  sec: 0\n",
      "epoch: 1  batch: 528 / 787  loss: 0.012872316020630851  hr: 0  min: 48  sec: 3\n",
      "epoch: 1  batch: 529 / 787  loss: 0.012856988329396342  hr: 0  min: 47  sec: 59\n",
      "epoch: 1  batch: 530 / 787  loss: 0.012867960728977298  hr: 0  min: 47  sec: 55\n",
      "epoch: 1  batch: 531 / 787  loss: 0.012856253850529858  hr: 0  min: 47  sec: 53\n",
      "epoch: 1  batch: 532 / 787  loss: 0.012833635002169828  hr: 0  min: 47  sec: 50\n",
      "epoch: 1  batch: 533 / 787  loss: 0.01284568500814271  hr: 0  min: 47  sec: 47\n",
      "epoch: 1  batch: 534 / 787  loss: 0.012836129285791045  hr: 0  min: 47  sec: 46\n",
      "epoch: 1  batch: 535 / 787  loss: 0.012816009572358959  hr: 0  min: 47  sec: 42\n",
      "epoch: 1  batch: 536 / 787  loss: 0.01280835141195853  hr: 0  min: 47  sec: 39\n",
      "epoch: 1  batch: 537 / 787  loss: 0.012800617319760668  hr: 0  min: 47  sec: 36\n",
      "epoch: 1  batch: 538 / 787  loss: 0.012784741855474001  hr: 0  min: 47  sec: 33\n",
      "epoch: 1  batch: 539 / 787  loss: 0.012764062667878929  hr: 0  min: 47  sec: 30\n",
      "epoch: 1  batch: 540 / 787  loss: 0.0128816725317544  hr: 0  min: 47  sec: 26\n",
      "epoch: 1  batch: 541 / 787  loss: 0.012893256221126675  hr: 0  min: 47  sec: 24\n",
      "epoch: 1  batch: 542 / 787  loss: 0.012872247097754294  hr: 0  min: 47  sec: 20\n",
      "epoch: 1  batch: 543 / 787  loss: 0.012855807519312023  hr: 0  min: 47  sec: 15\n",
      "epoch: 1  batch: 544 / 787  loss: 0.01283806665592505  hr: 0  min: 47  sec: 13\n",
      "epoch: 1  batch: 545 / 787  loss: 0.012815839094494325  hr: 0  min: 47  sec: 11\n",
      "epoch: 1  batch: 546 / 787  loss: 0.012795612428357804  hr: 0  min: 47  sec: 8\n",
      "epoch: 1  batch: 547 / 787  loss: 0.012776464629506308  hr: 0  min: 47  sec: 13\n",
      "epoch: 1  batch: 548 / 787  loss: 0.012833372068193735  hr: 0  min: 47  sec: 12\n",
      "epoch: 1  batch: 549 / 787  loss: 0.01281630448303194  hr: 0  min: 47  sec: 12\n",
      "epoch: 1  batch: 550 / 787  loss: 0.012845205650664866  hr: 0  min: 47  sec: 8\n",
      "epoch: 1  batch: 551 / 787  loss: 0.012823739884541236  hr: 0  min: 47  sec: 4\n",
      "epoch: 1  batch: 552 / 787  loss: 0.012808795040852958  hr: 0  min: 47  sec: 2\n",
      "epoch: 1  batch: 553 / 787  loss: 0.012789902808843602  hr: 0  min: 47  sec: 0\n",
      "epoch: 1  batch: 554 / 787  loss: 0.012787088690643343  hr: 0  min: 46  sec: 57\n",
      "epoch: 1  batch: 555 / 787  loss: 0.012765235607596199  hr: 0  min: 46  sec: 54\n",
      "epoch: 1  batch: 556 / 787  loss: 0.012794817867563207  hr: 0  min: 46  sec: 51\n",
      "epoch: 1  batch: 557 / 787  loss: 0.012773831287579513  hr: 0  min: 46  sec: 47\n",
      "epoch: 1  batch: 558 / 787  loss: 0.012759250498169826  hr: 0  min: 46  sec: 45\n",
      "epoch: 1  batch: 559 / 787  loss: 0.01283756164335754  hr: 0  min: 46  sec: 41\n",
      "epoch: 1  batch: 560 / 787  loss: 0.012827328812480637  hr: 0  min: 46  sec: 39\n",
      "epoch: 1  batch: 561 / 787  loss: 0.012822311782027444  hr: 0  min: 46  sec: 37\n",
      "epoch: 1  batch: 562 / 787  loss: 0.012805424720542738  hr: 0  min: 46  sec: 33\n",
      "epoch: 1  batch: 563 / 787  loss: 0.012867094406719133  hr: 0  min: 46  sec: 30\n",
      "epoch: 1  batch: 564 / 787  loss: 0.012864508618623784  hr: 0  min: 46  sec: 26\n",
      "epoch: 1  batch: 565 / 787  loss: 0.012869649240747095  hr: 0  min: 46  sec: 23\n",
      "epoch: 1  batch: 566 / 787  loss: 0.012867445210505  hr: 0  min: 46  sec: 20\n",
      "epoch: 1  batch: 567 / 787  loss: 0.01285566786686829  hr: 0  min: 46  sec: 17\n",
      "epoch: 1  batch: 568 / 787  loss: 0.01284389795930597  hr: 0  min: 46  sec: 14\n",
      "epoch: 1  batch: 569 / 787  loss: 0.012839374976618925  hr: 0  min: 46  sec: 14\n",
      "epoch: 1  batch: 570 / 787  loss: 0.012828232661265423  hr: 0  min: 46  sec: 12\n",
      "epoch: 1  batch: 571 / 787  loss: 0.012820959593138401  hr: 0  min: 46  sec: 11\n",
      "epoch: 1  batch: 572 / 787  loss: 0.012832876493082988  hr: 0  min: 46  sec: 7\n",
      "epoch: 1  batch: 573 / 787  loss: 0.012946067019942725  hr: 0  min: 46  sec: 4\n",
      "epoch: 1  batch: 574 / 787  loss: 0.01292609713504331  hr: 0  min: 46  sec: 1\n",
      "epoch: 1  batch: 575 / 787  loss: 0.012907140181278405  hr: 0  min: 46  sec: 0\n",
      "epoch: 1  batch: 576 / 787  loss: 0.012887436876553693  hr: 0  min: 45  sec: 58\n",
      "epoch: 1  batch: 577 / 787  loss: 0.012871853098316857  hr: 0  min: 45  sec: 55\n",
      "epoch: 1  batch: 578 / 787  loss: 0.012852640274491574  hr: 0  min: 45  sec: 52\n",
      "epoch: 1  batch: 579 / 787  loss: 0.012837490127807955  hr: 0  min: 45  sec: 49\n",
      "epoch: 1  batch: 580 / 787  loss: 0.012818723897136555  hr: 0  min: 45  sec: 46\n",
      "epoch: 1  batch: 581 / 787  loss: 0.012800531920082483  hr: 0  min: 45  sec: 42\n",
      "epoch: 1  batch: 582 / 787  loss: 0.012812067072676923  hr: 0  min: 45  sec: 40\n",
      "epoch: 1  batch: 583 / 787  loss: 0.012801215854332835  hr: 0  min: 45  sec: 38\n",
      "epoch: 1  batch: 584 / 787  loss: 0.01292701816638415  hr: 0  min: 45  sec: 34\n",
      "epoch: 1  batch: 585 / 787  loss: 0.01291563299456691  hr: 0  min: 45  sec: 31\n",
      "epoch: 1  batch: 586 / 787  loss: 0.012902502993832155  hr: 0  min: 45  sec: 29\n",
      "epoch: 1  batch: 587 / 787  loss: 0.012894532189298313  hr: 0  min: 45  sec: 26\n",
      "epoch: 1  batch: 588 / 787  loss: 0.012891524174958658  hr: 0  min: 45  sec: 24\n",
      "epoch: 1  batch: 589 / 787  loss: 0.012891549068180519  hr: 0  min: 45  sec: 20\n",
      "epoch: 1  batch: 590 / 787  loss: 0.01292710399109144  hr: 0  min: 45  sec: 17\n",
      "epoch: 1  batch: 591 / 787  loss: 0.012953751026920646  hr: 0  min: 45  sec: 14\n",
      "epoch: 1  batch: 592 / 787  loss: 0.012938280059913538  hr: 0  min: 45  sec: 10\n",
      "epoch: 1  batch: 593 / 787  loss: 0.012918465039896987  hr: 0  min: 45  sec: 8\n",
      "epoch: 1  batch: 594 / 787  loss: 0.012903240265329416  hr: 0  min: 45  sec: 5\n",
      "epoch: 1  batch: 595 / 787  loss: 0.01288718495287095  hr: 0  min: 45  sec: 3\n",
      "epoch: 1  batch: 596 / 787  loss: 0.012879334432549537  hr: 0  min: 45  sec: 0\n",
      "epoch: 1  batch: 597 / 787  loss: 0.012859764739783664  hr: 0  min: 44  sec: 58\n",
      "epoch: 1  batch: 598 / 787  loss: 0.012852076576542994  hr: 0  min: 44  sec: 55\n",
      "epoch: 1  batch: 599 / 787  loss: 0.012833127201548802  hr: 0  min: 44  sec: 53\n",
      "epoch: 1  batch: 600 / 787  loss: 0.01281730067760994  hr: 0  min: 44  sec: 50\n",
      "epoch: 1  batch: 601 / 787  loss: 0.01281585075897157  hr: 0  min: 44  sec: 47\n",
      "epoch: 1  batch: 602 / 787  loss: 0.012802715388519622  hr: 0  min: 44  sec: 44\n",
      "epoch: 1  batch: 603 / 787  loss: 0.01283613885528907  hr: 0  min: 44  sec: 40\n",
      "epoch: 1  batch: 604 / 787  loss: 0.012816269865736753  hr: 0  min: 44  sec: 38\n",
      "epoch: 1  batch: 605 / 787  loss: 0.012797880836489249  hr: 0  min: 44  sec: 39\n",
      "epoch: 1  batch: 606 / 787  loss: 0.012787985501226142  hr: 0  min: 44  sec: 39\n",
      "epoch: 1  batch: 607 / 787  loss: 0.012782539717705009  hr: 0  min: 44  sec: 37\n",
      "epoch: 1  batch: 608 / 787  loss: 0.012778386119335712  hr: 0  min: 44  sec: 36\n",
      "epoch: 1  batch: 609 / 787  loss: 0.012772031839037  hr: 0  min: 44  sec: 34\n",
      "epoch: 1  batch: 610 / 787  loss: 0.012823006469034972  hr: 0  min: 44  sec: 33\n",
      "epoch: 1  batch: 611 / 787  loss: 0.012804195447843949  hr: 0  min: 44  sec: 37\n",
      "epoch: 1  batch: 612 / 787  loss: 0.012785831207901794  hr: 0  min: 44  sec: 36\n",
      "epoch: 1  batch: 613 / 787  loss: 0.012819358860510955  hr: 0  min: 44  sec: 34\n",
      "epoch: 1  batch: 614 / 787  loss: 0.01280139457991465  hr: 0  min: 44  sec: 37\n",
      "epoch: 1  batch: 615 / 787  loss: 0.012804318115406678  hr: 0  min: 44  sec: 35\n",
      "epoch: 1  batch: 616 / 787  loss: 0.012811452146281651  hr: 0  min: 44  sec: 32\n",
      "epoch: 1  batch: 617 / 787  loss: 0.012794757338947913  hr: 0  min: 44  sec: 30\n",
      "epoch: 1  batch: 618 / 787  loss: 0.012776869554227086  hr: 0  min: 44  sec: 31\n",
      "epoch: 1  batch: 619 / 787  loss: 0.012762302787909028  hr: 0  min: 44  sec: 30\n",
      "epoch: 1  batch: 620 / 787  loss: 0.012745044458255682  hr: 0  min: 44  sec: 28\n",
      "epoch: 1  batch: 621 / 787  loss: 0.012731322510735598  hr: 0  min: 44  sec: 25\n",
      "epoch: 1  batch: 622 / 787  loss: 0.012747910124241521  hr: 0  min: 44  sec: 26\n",
      "epoch: 1  batch: 623 / 787  loss: 0.012770223016174208  hr: 0  min: 44  sec: 23\n",
      "epoch: 1  batch: 624 / 787  loss: 0.012751996331644477  hr: 0  min: 44  sec: 19\n",
      "epoch: 1  batch: 625 / 787  loss: 0.012744560694228857  hr: 0  min: 44  sec: 15\n",
      "epoch: 1  batch: 626 / 787  loss: 0.012731588003281415  hr: 0  min: 44  sec: 13\n",
      "epoch: 1  batch: 627 / 787  loss: 0.012713492628980867  hr: 0  min: 44  sec: 10\n",
      "epoch: 1  batch: 628 / 787  loss: 0.012730942439279745  hr: 0  min: 44  sec: 7\n",
      "epoch: 1  batch: 629 / 787  loss: 0.012750982061944256  hr: 0  min: 44  sec: 4\n",
      "epoch: 1  batch: 630 / 787  loss: 0.012734929016617585  hr: 0  min: 44  sec: 1\n",
      "epoch: 1  batch: 631 / 787  loss: 0.012730497177324971  hr: 0  min: 43  sec: 59\n",
      "epoch: 1  batch: 632 / 787  loss: 0.012737426389310113  hr: 0  min: 43  sec: 55\n",
      "epoch: 1  batch: 633 / 787  loss: 0.0127192532694619  hr: 0  min: 43  sec: 53\n",
      "epoch: 1  batch: 634 / 787  loss: 0.012713962653302956  hr: 0  min: 43  sec: 51\n",
      "epoch: 1  batch: 635 / 787  loss: 0.012745335538354592  hr: 0  min: 43  sec: 48\n",
      "epoch: 1  batch: 636 / 787  loss: 0.012730459228056224  hr: 0  min: 43  sec: 46\n",
      "epoch: 1  batch: 637 / 787  loss: 0.012712053470207551  hr: 0  min: 43  sec: 43\n",
      "epoch: 1  batch: 638 / 787  loss: 0.012695117655329166  hr: 0  min: 43  sec: 41\n",
      "epoch: 1  batch: 639 / 787  loss: 0.012679158801672738  hr: 0  min: 43  sec: 38\n",
      "epoch: 1  batch: 640 / 787  loss: 0.0126630086003388  hr: 0  min: 43  sec: 36\n",
      "epoch: 1  batch: 641 / 787  loss: 0.012656667726256052  hr: 0  min: 43  sec: 32\n",
      "epoch: 1  batch: 642 / 787  loss: 0.012643184299153534  hr: 0  min: 43  sec: 29\n",
      "epoch: 1  batch: 643 / 787  loss: 0.01262509448641714  hr: 0  min: 43  sec: 26\n",
      "epoch: 1  batch: 644 / 787  loss: 0.012606865354117312  hr: 0  min: 43  sec: 28\n",
      "epoch: 1  batch: 645 / 787  loss: 0.012592930895503784  hr: 0  min: 43  sec: 24\n",
      "epoch: 1  batch: 646 / 787  loss: 0.012575240324856136  hr: 0  min: 43  sec: 21\n",
      "epoch: 1  batch: 647 / 787  loss: 0.012571353011083008  hr: 0  min: 43  sec: 20\n",
      "epoch: 1  batch: 648 / 787  loss: 0.012648962346752847  hr: 0  min: 43  sec: 18\n",
      "epoch: 1  batch: 649 / 787  loss: 0.012630324831461313  hr: 0  min: 43  sec: 15\n",
      "epoch: 1  batch: 650 / 787  loss: 0.012647565856056574  hr: 0  min: 43  sec: 13\n",
      "epoch: 1  batch: 651 / 787  loss: 0.0126300134318566  hr: 0  min: 43  sec: 12\n",
      "epoch: 1  batch: 652 / 787  loss: 0.012626865267788598  hr: 0  min: 43  sec: 10\n",
      "epoch: 1  batch: 653 / 787  loss: 0.01260982286019228  hr: 0  min: 43  sec: 7\n",
      "epoch: 1  batch: 654 / 787  loss: 0.012634047562600361  hr: 0  min: 43  sec: 3\n",
      "epoch: 1  batch: 655 / 787  loss: 0.012616681467425362  hr: 0  min: 43  sec: 0\n",
      "epoch: 1  batch: 656 / 787  loss: 0.012602929494655322  hr: 0  min: 42  sec: 57\n",
      "epoch: 1  batch: 657 / 787  loss: 0.01258664608082863  hr: 0  min: 42  sec: 55\n",
      "epoch: 1  batch: 658 / 787  loss: 0.012570996553722718  hr: 0  min: 42  sec: 52\n",
      "epoch: 1  batch: 659 / 787  loss: 0.012587348123668228  hr: 0  min: 42  sec: 49\n",
      "epoch: 1  batch: 660 / 787  loss: 0.0125881765574873  hr: 0  min: 42  sec: 46\n",
      "epoch: 1  batch: 661 / 787  loss: 0.01257002478819087  hr: 0  min: 42  sec: 43\n",
      "epoch: 1  batch: 662 / 787  loss: 0.012564812017691933  hr: 0  min: 42  sec: 40\n",
      "epoch: 1  batch: 663 / 787  loss: 0.012558550665542753  hr: 0  min: 42  sec: 37\n",
      "epoch: 1  batch: 664 / 787  loss: 0.012545796824094603  hr: 0  min: 42  sec: 34\n",
      "epoch: 1  batch: 665 / 787  loss: 0.012540535748186976  hr: 0  min: 42  sec: 30\n",
      "epoch: 1  batch: 666 / 787  loss: 0.01252885196356567  hr: 0  min: 42  sec: 28\n",
      "epoch: 1  batch: 667 / 787  loss: 0.012511649147908983  hr: 0  min: 42  sec: 25\n",
      "epoch: 1  batch: 668 / 787  loss: 0.012497637019538694  hr: 0  min: 42  sec: 22\n",
      "epoch: 1  batch: 669 / 787  loss: 0.012480045422116053  hr: 0  min: 42  sec: 20\n",
      "epoch: 1  batch: 670 / 787  loss: 0.01246292567612089  hr: 0  min: 42  sec: 19\n",
      "epoch: 1  batch: 671 / 787  loss: 0.012449040918124508  hr: 0  min: 42  sec: 18\n",
      "epoch: 1  batch: 672 / 787  loss: 0.012452914739263915  hr: 0  min: 42  sec: 15\n",
      "epoch: 1  batch: 673 / 787  loss: 0.012438605101256705  hr: 0  min: 42  sec: 12\n",
      "epoch: 1  batch: 674 / 787  loss: 0.012437155662751853  hr: 0  min: 42  sec: 10\n",
      "epoch: 1  batch: 675 / 787  loss: 0.01242455359108539  hr: 0  min: 42  sec: 8\n",
      "epoch: 1  batch: 676 / 787  loss: 0.012422336041999667  hr: 0  min: 42  sec: 5\n",
      "epoch: 1  batch: 677 / 787  loss: 0.012407590759016348  hr: 0  min: 42  sec: 4\n",
      "epoch: 1  batch: 678 / 787  loss: 0.012394189923623258  hr: 0  min: 42  sec: 1\n",
      "epoch: 1  batch: 679 / 787  loss: 0.012408479769485803  hr: 0  min: 41  sec: 57\n",
      "epoch: 1  batch: 680 / 787  loss: 0.012393719612651587  hr: 0  min: 41  sec: 53\n",
      "epoch: 1  batch: 681 / 787  loss: 0.01238496056882659  hr: 0  min: 41  sec: 52\n",
      "epoch: 1  batch: 682 / 787  loss: 0.012456247245819825  hr: 0  min: 41  sec: 49\n",
      "epoch: 1  batch: 683 / 787  loss: 0.01244578951511922  hr: 0  min: 41  sec: 47\n",
      "epoch: 1  batch: 684 / 787  loss: 0.012440096087326916  hr: 0  min: 41  sec: 43\n",
      "epoch: 1  batch: 685 / 787  loss: 0.012424428808336302  hr: 0  min: 41  sec: 41\n",
      "epoch: 1  batch: 686 / 787  loss: 0.012414022925552172  hr: 0  min: 41  sec: 38\n",
      "epoch: 1  batch: 687 / 787  loss: 0.01240164955328605  hr: 0  min: 41  sec: 35\n",
      "epoch: 1  batch: 688 / 787  loss: 0.012387846830807936  hr: 0  min: 41  sec: 32\n",
      "epoch: 1  batch: 689 / 787  loss: 0.012370687419601347  hr: 0  min: 41  sec: 29\n",
      "epoch: 1  batch: 690 / 787  loss: 0.012359764397421015  hr: 0  min: 41  sec: 27\n",
      "epoch: 1  batch: 691 / 787  loss: 0.012342603031196126  hr: 0  min: 41  sec: 25\n",
      "epoch: 1  batch: 692 / 787  loss: 0.012467018961573336  hr: 0  min: 41  sec: 22\n",
      "epoch: 1  batch: 693 / 787  loss: 0.012449830740590783  hr: 0  min: 41  sec: 19\n",
      "epoch: 1  batch: 694 / 787  loss: 0.012435017240177357  hr: 0  min: 41  sec: 15\n",
      "epoch: 1  batch: 695 / 787  loss: 0.012419420244547424  hr: 0  min: 41  sec: 14\n",
      "epoch: 1  batch: 696 / 787  loss: 0.01240286151314858  hr: 0  min: 41  sec: 10\n",
      "epoch: 1  batch: 697 / 787  loss: 0.012420458339907662  hr: 0  min: 41  sec: 7\n",
      "epoch: 1  batch: 698 / 787  loss: 0.012407203386641136  hr: 0  min: 41  sec: 5\n",
      "epoch: 1  batch: 699 / 787  loss: 0.012397108918894615  hr: 0  min: 41  sec: 1\n",
      "epoch: 1  batch: 700 / 787  loss: 0.01238187971716148  hr: 0  min: 40  sec: 59\n",
      "epoch: 1  batch: 701 / 787  loss: 0.012369436932878486  hr: 0  min: 40  sec: 56\n",
      "epoch: 1  batch: 702 / 787  loss: 0.012354591627458663  hr: 0  min: 40  sec: 52\n",
      "epoch: 1  batch: 703 / 787  loss: 0.012357945496578744  hr: 0  min: 40  sec: 49\n",
      "epoch: 1  batch: 704 / 787  loss: 0.012372647483144565  hr: 0  min: 40  sec: 46\n",
      "epoch: 1  batch: 705 / 787  loss: 0.012357151052660262  hr: 0  min: 40  sec: 42\n",
      "epoch: 1  batch: 706 / 787  loss: 0.012406112245041956  hr: 0  min: 40  sec: 38\n",
      "epoch: 1  batch: 707 / 787  loss: 0.012545285400090794  hr: 0  min: 40  sec: 35\n",
      "epoch: 1  batch: 708 / 787  loss: 0.012535028471456877  hr: 0  min: 40  sec: 31\n",
      "epoch: 1  batch: 709 / 787  loss: 0.01251971642399727  hr: 0  min: 40  sec: 28\n",
      "epoch: 1  batch: 710 / 787  loss: 0.012515311817635297  hr: 0  min: 40  sec: 25\n",
      "epoch: 1  batch: 711 / 787  loss: 0.012507921620746931  hr: 0  min: 40  sec: 22\n",
      "epoch: 1  batch: 712 / 787  loss: 0.012553343768058256  hr: 0  min: 40  sec: 20\n",
      "epoch: 1  batch: 713 / 787  loss: 0.012564200300806258  hr: 0  min: 40  sec: 17\n",
      "epoch: 1  batch: 714 / 787  loss: 0.012555164943957253  hr: 0  min: 40  sec: 13\n",
      "epoch: 1  batch: 715 / 787  loss: 0.012574288789817895  hr: 0  min: 40  sec: 10\n",
      "epoch: 1  batch: 716 / 787  loss: 0.012577217054282127  hr: 0  min: 40  sec: 7\n",
      "epoch: 1  batch: 717 / 787  loss: 0.012570084859386496  hr: 0  min: 40  sec: 4\n",
      "epoch: 1  batch: 718 / 787  loss: 0.012557711004014295  hr: 0  min: 40  sec: 1\n",
      "epoch: 1  batch: 719 / 787  loss: 0.0125444750511217  hr: 0  min: 39  sec: 58\n",
      "epoch: 1  batch: 720 / 787  loss: 0.012579344193202106  hr: 0  min: 39  sec: 55\n",
      "epoch: 1  batch: 721 / 787  loss: 0.012612990777530732  hr: 0  min: 39  sec: 53\n",
      "epoch: 1  batch: 722 / 787  loss: 0.012605769569657284  hr: 0  min: 39  sec: 50\n",
      "epoch: 1  batch: 723 / 787  loss: 0.012592973900790795  hr: 0  min: 39  sec: 49\n",
      "epoch: 1  batch: 724 / 787  loss: 0.012588306833994421  hr: 0  min: 39  sec: 47\n",
      "epoch: 1  batch: 725 / 787  loss: 0.012572908184635613  hr: 0  min: 39  sec: 47\n",
      "epoch: 1  batch: 726 / 787  loss: 0.012592103731476628  hr: 0  min: 39  sec: 44\n",
      "epoch: 1  batch: 727 / 787  loss: 0.01259006084219336  hr: 0  min: 39  sec: 41\n",
      "epoch: 1  batch: 728 / 787  loss: 0.012575998528688445  hr: 0  min: 39  sec: 39\n",
      "epoch: 1  batch: 729 / 787  loss: 0.012562847484732928  hr: 0  min: 39  sec: 35\n",
      "epoch: 1  batch: 730 / 787  loss: 0.012553017281477454  hr: 0  min: 39  sec: 32\n",
      "epoch: 1  batch: 731 / 787  loss: 0.012575990910194025  hr: 0  min: 39  sec: 29\n",
      "epoch: 1  batch: 732 / 787  loss: 0.01259507213283745  hr: 0  min: 39  sec: 26\n",
      "epoch: 1  batch: 733 / 787  loss: 0.012588067421683911  hr: 0  min: 39  sec: 24\n",
      "epoch: 1  batch: 734 / 787  loss: 0.01258041769478975  hr: 0  min: 39  sec: 21\n",
      "epoch: 1  batch: 735 / 787  loss: 0.012630535205303483  hr: 0  min: 39  sec: 19\n",
      "epoch: 1  batch: 736 / 787  loss: 0.012620895146186662  hr: 0  min: 39  sec: 16\n",
      "epoch: 1  batch: 737 / 787  loss: 0.01260827146709064  hr: 0  min: 39  sec: 13\n",
      "epoch: 1  batch: 738 / 787  loss: 0.012653732794348962  hr: 0  min: 39  sec: 10\n",
      "epoch: 1  batch: 739 / 787  loss: 0.012664625212831829  hr: 0  min: 39  sec: 6\n",
      "epoch: 1  batch: 740 / 787  loss: 0.012663394325598445  hr: 0  min: 39  sec: 3\n",
      "epoch: 1  batch: 741 / 787  loss: 0.012649884855851667  hr: 0  min: 39  sec: 0\n",
      "epoch: 1  batch: 742 / 787  loss: 0.012676994031282002  hr: 0  min: 38  sec: 57\n",
      "epoch: 1  batch: 743 / 787  loss: 0.01266615708321922  hr: 0  min: 38  sec: 54\n",
      "epoch: 1  batch: 744 / 787  loss: 0.012657325778589008  hr: 0  min: 38  sec: 52\n",
      "epoch: 1  batch: 745 / 787  loss: 0.012655367042116146  hr: 0  min: 38  sec: 49\n",
      "epoch: 1  batch: 746 / 787  loss: 0.01265772774023748  hr: 0  min: 38  sec: 46\n",
      "epoch: 1  batch: 747 / 787  loss: 0.012645085418202267  hr: 0  min: 38  sec: 43\n",
      "epoch: 1  batch: 748 / 787  loss: 0.012638765885192717  hr: 0  min: 38  sec: 40\n",
      "epoch: 1  batch: 749 / 787  loss: 0.01264205892924381  hr: 0  min: 38  sec: 37\n",
      "epoch: 1  batch: 750 / 787  loss: 0.012635985354504858  hr: 0  min: 38  sec: 34\n",
      "epoch: 1  batch: 751 / 787  loss: 0.012625917799683405  hr: 0  min: 38  sec: 31\n",
      "epoch: 1  batch: 752 / 787  loss: 0.012617297871134298  hr: 0  min: 38  sec: 32\n",
      "epoch: 1  batch: 753 / 787  loss: 0.012602262131441236  hr: 0  min: 38  sec: 29\n",
      "epoch: 1  batch: 754 / 787  loss: 0.012597862592097298  hr: 0  min: 38  sec: 26\n",
      "epoch: 1  batch: 755 / 787  loss: 0.0125916241396015  hr: 0  min: 38  sec: 23\n",
      "epoch: 1  batch: 756 / 787  loss: 0.012579195187768958  hr: 0  min: 38  sec: 20\n",
      "epoch: 1  batch: 757 / 787  loss: 0.012580824206300852  hr: 0  min: 38  sec: 17\n",
      "epoch: 1  batch: 758 / 787  loss: 0.012569741261087845  hr: 0  min: 38  sec: 13\n",
      "epoch: 1  batch: 759 / 787  loss: 0.012601803675246681  hr: 0  min: 38  sec: 10\n",
      "epoch: 1  batch: 760 / 787  loss: 0.01258617037927156  hr: 0  min: 38  sec: 8\n",
      "epoch: 1  batch: 761 / 787  loss: 0.012574097904136206  hr: 0  min: 38  sec: 5\n",
      "epoch: 1  batch: 762 / 787  loss: 0.012559357457571196  hr: 0  min: 38  sec: 2\n",
      "epoch: 1  batch: 763 / 787  loss: 0.012546425355646545  hr: 0  min: 38  sec: 1\n",
      "epoch: 1  batch: 764 / 787  loss: 0.012534880517791171  hr: 0  min: 37  sec: 58\n",
      "epoch: 1  batch: 765 / 787  loss: 0.012521391014912427  hr: 0  min: 37  sec: 55\n",
      "epoch: 1  batch: 766 / 787  loss: 0.012506160469739102  hr: 0  min: 37  sec: 53\n",
      "epoch: 1  batch: 767 / 787  loss: 0.012492354652350173  hr: 0  min: 37  sec: 50\n",
      "epoch: 1  batch: 768 / 787  loss: 0.012477300046612072  hr: 0  min: 37  sec: 48\n",
      "epoch: 1  batch: 769 / 787  loss: 0.012462785081927009  hr: 0  min: 37  sec: 44\n",
      "epoch: 1  batch: 770 / 787  loss: 0.012447056396450098  hr: 0  min: 37  sec: 41\n",
      "epoch: 1  batch: 771 / 787  loss: 0.012433720104158982  hr: 0  min: 37  sec: 39\n",
      "epoch: 1  batch: 772 / 787  loss: 0.012421109732539017  hr: 0  min: 37  sec: 37\n",
      "epoch: 1  batch: 773 / 787  loss: 0.012406726114685017  hr: 0  min: 37  sec: 34\n",
      "epoch: 1  batch: 774 / 787  loss: 0.012391696045056674  hr: 0  min: 37  sec: 32\n",
      "epoch: 1  batch: 775 / 787  loss: 0.012376953681870815  hr: 0  min: 37  sec: 29\n",
      "epoch: 1  batch: 776 / 787  loss: 0.01238746897043833  hr: 0  min: 37  sec: 26\n",
      "epoch: 1  batch: 777 / 787  loss: 0.012372591635997394  hr: 0  min: 37  sec: 23\n",
      "epoch: 1  batch: 778 / 787  loss: 0.0123720165345983  hr: 0  min: 37  sec: 20\n",
      "epoch: 1  batch: 779 / 787  loss: 0.012360207658931756  hr: 0  min: 37  sec: 21\n",
      "epoch: 1  batch: 780 / 787  loss: 0.012372990770605751  hr: 0  min: 37  sec: 18\n",
      "epoch: 1  batch: 781 / 787  loss: 0.012361376607586676  hr: 0  min: 37  sec: 15\n",
      "epoch: 1  batch: 782 / 787  loss: 0.01234786814365112  hr: 0  min: 37  sec: 12\n",
      "epoch: 1  batch: 783 / 787  loss: 0.012352714793355767  hr: 0  min: 37  sec: 8\n",
      "epoch: 1  batch: 784 / 787  loss: 0.012342754934530714  hr: 0  min: 37  sec: 6\n",
      "epoch: 1  batch: 785 / 787  loss: 0.012331612970958802  hr: 0  min: 37  sec: 4\n",
      "epoch: 1  batch: 786 / 787  loss: 0.01232965248764277  hr: 0  min: 37  sec: 2\n",
      "epoch: 1  batch: 787 / 787  loss: 0.012318378950296421  hr: 0  min: 36  sec: 59\n",
      "epoch: 2  batch: 1 / 787  loss: 0.0005491410847753286  hr: 0  min: 37  sec: 10\n",
      "epoch: 2  batch: 2 / 787  loss: 0.0017695608548820019  hr: 0  min: 41  sec: 1\n",
      "epoch: 2  batch: 3 / 787  loss: 0.005830561742186546  hr: 0  min: 38  sec: 46\n",
      "epoch: 2  batch: 4 / 787  loss: 0.004641495324904099  hr: 0  min: 37  sec: 10\n",
      "epoch: 2  batch: 5 / 787  loss: 0.004049980989657342  hr: 0  min: 37  sec: 32\n",
      "epoch: 2  batch: 6 / 787  loss: 0.004204421672814836  hr: 0  min: 37  sec: 10\n",
      "epoch: 2  batch: 7 / 787  loss: 0.003709764369497342  hr: 0  min: 36  sec: 56\n",
      "epoch: 2  batch: 8 / 787  loss: 0.003567573949112557  hr: 0  min: 37  sec: 42\n",
      "epoch: 2  batch: 9 / 787  loss: 0.0032487537698923713  hr: 0  min: 37  sec: 21\n",
      "epoch: 2  batch: 10 / 787  loss: 0.0033632153819780795  hr: 0  min: 36  sec: 51\n",
      "epoch: 2  batch: 11 / 787  loss: 0.0032206571142358534  hr: 0  min: 36  sec: 32\n",
      "epoch: 2  batch: 12 / 787  loss: 0.0029992038859442496  hr: 0  min: 36  sec: 12\n",
      "epoch: 2  batch: 13 / 787  loss: 0.0028420831569327186  hr: 0  min: 36  sec: 38\n",
      "epoch: 2  batch: 14 / 787  loss: 0.0027842040996932027  hr: 0  min: 36  sec: 27\n",
      "epoch: 2  batch: 15 / 787  loss: 0.002696576384672274  hr: 0  min: 35  sec: 45\n",
      "epoch: 2  batch: 16 / 787  loss: 0.0025792927481234074  hr: 0  min: 35  sec: 26\n",
      "epoch: 2  batch: 17 / 787  loss: 0.002481713764342096  hr: 0  min: 35  sec: 7\n",
      "epoch: 2  batch: 18 / 787  loss: 0.0024414309656195757  hr: 0  min: 35  sec: 18\n",
      "epoch: 2  batch: 19 / 787  loss: 0.0024277989256293758  hr: 0  min: 35  sec: 29\n",
      "epoch: 2  batch: 20 / 787  loss: 0.0023313058220082892  hr: 0  min: 37  sec: 30\n",
      "epoch: 2  batch: 21 / 787  loss: 0.0022461307484523524  hr: 0  min: 37  sec: 5\n",
      "epoch: 2  batch: 22 / 787  loss: 0.0023052616346500476  hr: 0  min: 36  sec: 57\n",
      "epoch: 2  batch: 23 / 787  loss: 0.0022586488200392087  hr: 0  min: 36  sec: 54\n",
      "epoch: 2  batch: 24 / 787  loss: 0.0021928262285655364  hr: 0  min: 36  sec: 30\n",
      "epoch: 2  batch: 25 / 787  loss: 0.0028218782553449274  hr: 0  min: 36  sec: 16\n",
      "epoch: 2  batch: 26 / 787  loss: 0.002776824819962852  hr: 0  min: 36  sec: 20\n",
      "epoch: 2  batch: 27 / 787  loss: 0.0027439540243466144  hr: 0  min: 36  sec: 37\n",
      "epoch: 2  batch: 28 / 787  loss: 0.0026845429175799446  hr: 0  min: 36  sec: 41\n",
      "epoch: 2  batch: 29 / 787  loss: 0.0026392251180870265  hr: 0  min: 36  sec: 26\n",
      "epoch: 2  batch: 30 / 787  loss: 0.0030469448422081767  hr: 0  min: 36  sec: 23\n",
      "epoch: 2  batch: 31 / 787  loss: 0.0029638000733910068  hr: 0  min: 37  sec: 37\n",
      "epoch: 2  batch: 32 / 787  loss: 0.00301147616119124  hr: 0  min: 38  sec: 22\n",
      "epoch: 2  batch: 33 / 787  loss: 0.0029632442870713544  hr: 0  min: 38  sec: 31\n",
      "epoch: 2  batch: 34 / 787  loss: 0.0029062420978923052  hr: 0  min: 38  sec: 17\n",
      "epoch: 2  batch: 35 / 787  loss: 0.0028762831685266326  hr: 0  min: 37  sec: 53\n",
      "epoch: 2  batch: 36 / 787  loss: 0.002815683530772933  hr: 0  min: 37  sec: 51\n",
      "epoch: 2  batch: 37 / 787  loss: 0.002848963369615376  hr: 0  min: 37  sec: 43\n",
      "epoch: 2  batch: 38 / 787  loss: 0.0027949421936155935  hr: 0  min: 37  sec: 30\n",
      "epoch: 2  batch: 39 / 787  loss: 0.002755677969290469  hr: 0  min: 37  sec: 28\n",
      "epoch: 2  batch: 40 / 787  loss: 0.002701676201832015  hr: 0  min: 37  sec: 18\n",
      "epoch: 2  batch: 41 / 787  loss: 0.002686999699884526  hr: 0  min: 37  sec: 17\n",
      "epoch: 2  batch: 42 / 787  loss: 0.0026341535613894286  hr: 0  min: 37  sec: 1\n",
      "epoch: 2  batch: 43 / 787  loss: 0.0025838238957370525  hr: 0  min: 37  sec: 1\n",
      "epoch: 2  batch: 44 / 787  loss: 0.0026011629677007227  hr: 0  min: 37  sec: 4\n",
      "epoch: 2  batch: 45 / 787  loss: 0.002564502042433661  hr: 0  min: 36  sec: 55\n",
      "epoch: 2  batch: 46 / 787  loss: 0.0025242407440521715  hr: 0  min: 36  sec: 58\n",
      "epoch: 2  batch: 47 / 787  loss: 0.002476240755950517  hr: 0  min: 36  sec: 56\n",
      "epoch: 2  batch: 48 / 787  loss: 0.0024336266784909335  hr: 0  min: 36  sec: 55\n",
      "epoch: 2  batch: 49 / 787  loss: 0.002444617111682512  hr: 0  min: 36  sec: 43\n",
      "epoch: 2  batch: 50 / 787  loss: 0.0024883295630570503  hr: 0  min: 36  sec: 49\n",
      "epoch: 2  batch: 51 / 787  loss: 0.0024451517360741457  hr: 0  min: 36  sec: 49\n",
      "epoch: 2  batch: 52 / 787  loss: 0.002876990115440164  hr: 0  min: 36  sec: 49\n",
      "epoch: 2  batch: 53 / 787  loss: 0.0028415178267267656  hr: 0  min: 36  sec: 43\n",
      "epoch: 2  batch: 54 / 787  loss: 0.0027975048497965974  hr: 0  min: 36  sec: 38\n",
      "epoch: 2  batch: 55 / 787  loss: 0.0027743949930564586  hr: 0  min: 36  sec: 36\n",
      "epoch: 2  batch: 56 / 787  loss: 0.002729259932363805  hr: 0  min: 36  sec: 26\n",
      "epoch: 2  batch: 57 / 787  loss: 0.002698535048566236  hr: 0  min: 36  sec: 40\n",
      "epoch: 2  batch: 58 / 787  loss: 0.00267156969418685  hr: 0  min: 36  sec: 29\n",
      "epoch: 2  batch: 59 / 787  loss: 0.002631311595412287  hr: 0  min: 36  sec: 42\n",
      "epoch: 2  batch: 60 / 787  loss: 0.0025925699306147483  hr: 0  min: 36  sec: 36\n",
      "epoch: 2  batch: 61 / 787  loss: 0.0025902136575361927  hr: 0  min: 36  sec: 30\n",
      "epoch: 2  batch: 62 / 787  loss: 0.002554142540658734  hr: 0  min: 36  sec: 26\n",
      "epoch: 2  batch: 63 / 787  loss: 0.002594865549037913  hr: 0  min: 36  sec: 24\n",
      "epoch: 2  batch: 64 / 787  loss: 0.0025704124718686217  hr: 0  min: 36  sec: 18\n",
      "epoch: 2  batch: 65 / 787  loss: 0.002533780066117358  hr: 0  min: 36  sec: 9\n",
      "epoch: 2  batch: 66 / 787  loss: 0.0025000713459589997  hr: 0  min: 36  sec: 8\n",
      "epoch: 2  batch: 67 / 787  loss: 0.0025333895338455967  hr: 0  min: 36  sec: 16\n",
      "epoch: 2  batch: 68 / 787  loss: 0.0025103895146885943  hr: 0  min: 36  sec: 14\n",
      "epoch: 2  batch: 69 / 787  loss: 0.002477140162491064  hr: 0  min: 36  sec: 7\n",
      "epoch: 2  batch: 70 / 787  loss: 0.002452173062400626  hr: 0  min: 35  sec: 57\n",
      "epoch: 2  batch: 71 / 787  loss: 0.002420823055960592  hr: 0  min: 35  sec: 48\n",
      "epoch: 2  batch: 72 / 787  loss: 0.002425727944379711  hr: 0  min: 36  sec: 17\n",
      "epoch: 2  batch: 73 / 787  loss: 0.0023982111025324143  hr: 0  min: 36  sec: 13\n",
      "epoch: 2  batch: 74 / 787  loss: 0.0025892578636853  hr: 0  min: 36  sec: 18\n",
      "epoch: 2  batch: 75 / 787  loss: 0.002556961053633131  hr: 0  min: 36  sec: 25\n",
      "epoch: 2  batch: 76 / 787  loss: 0.0025310342866042015  hr: 0  min: 36  sec: 27\n",
      "epoch: 2  batch: 77 / 787  loss: 0.002503546255878052  hr: 0  min: 36  sec: 19\n",
      "epoch: 2  batch: 78 / 787  loss: 0.0024743328926538546  hr: 0  min: 36  sec: 9\n",
      "epoch: 2  batch: 79 / 787  loss: 0.00244667656668845  hr: 0  min: 36  sec: 6\n",
      "epoch: 2  batch: 80 / 787  loss: 0.0024201027934395826  hr: 0  min: 36  sec: 0\n",
      "epoch: 2  batch: 81 / 787  loss: 0.0023940328092179205  hr: 0  min: 36  sec: 11\n",
      "epoch: 2  batch: 82 / 787  loss: 0.0023744472257420986  hr: 0  min: 36  sec: 18\n",
      "epoch: 2  batch: 83 / 787  loss: 0.002497862768675066  hr: 0  min: 36  sec: 15\n",
      "epoch: 2  batch: 84 / 787  loss: 0.002480908058108374  hr: 0  min: 36  sec: 15\n",
      "epoch: 2  batch: 85 / 787  loss: 0.002506920152432833  hr: 0  min: 36  sec: 7\n",
      "epoch: 2  batch: 86 / 787  loss: 0.0024889946658351067  hr: 0  min: 36  sec: 3\n",
      "epoch: 2  batch: 87 / 787  loss: 0.002464543498655107  hr: 0  min: 36  sec: 3\n",
      "epoch: 2  batch: 88 / 787  loss: 0.002440781749762457  hr: 0  min: 35  sec: 57\n",
      "epoch: 2  batch: 89 / 787  loss: 0.0024153870969273023  hr: 0  min: 35  sec: 56\n",
      "epoch: 2  batch: 90 / 787  loss: 0.0026297699773244355  hr: 0  min: 35  sec: 48\n",
      "epoch: 2  batch: 91 / 787  loss: 0.0026147756351357593  hr: 0  min: 35  sec: 38\n",
      "epoch: 2  batch: 92 / 787  loss: 0.002592260095495832  hr: 0  min: 35  sec: 32\n",
      "epoch: 2  batch: 93 / 787  loss: 0.0025729124784164173  hr: 0  min: 35  sec: 25\n",
      "epoch: 2  batch: 94 / 787  loss: 0.002988314687985273  hr: 0  min: 35  sec: 26\n",
      "epoch: 2  batch: 95 / 787  loss: 0.0029796814898298564  hr: 0  min: 35  sec: 21\n",
      "epoch: 2  batch: 96 / 787  loss: 0.0029572623895243546  hr: 0  min: 35  sec: 12\n",
      "epoch: 2  batch: 97 / 787  loss: 0.0029316668686116135  hr: 0  min: 35  sec: 6\n",
      "epoch: 2  batch: 98 / 787  loss: 0.0029182249156886482  hr: 0  min: 35  sec: 0\n",
      "epoch: 2  batch: 99 / 787  loss: 0.00292704990833311  hr: 0  min: 34  sec: 58\n",
      "epoch: 2  batch: 100 / 787  loss: 0.002909399238124024  hr: 0  min: 35  sec: 6\n",
      "epoch: 2  batch: 101 / 787  loss: 0.0028892268929837993  hr: 0  min: 35  sec: 7\n",
      "epoch: 2  batch: 102 / 787  loss: 0.0028769192049010457  hr: 0  min: 34  sec: 59\n",
      "epoch: 2  batch: 103 / 787  loss: 0.0029720119463429034  hr: 0  min: 34  sec: 50\n",
      "epoch: 2  batch: 104 / 787  loss: 0.0029460470155194905  hr: 0  min: 34  sec: 46\n",
      "epoch: 2  batch: 105 / 787  loss: 0.0029373071860477684  hr: 0  min: 34  sec: 37\n",
      "epoch: 2  batch: 106 / 787  loss: 0.002914589440410535  hr: 0  min: 34  sec: 34\n",
      "epoch: 2  batch: 107 / 787  loss: 0.0029141986106139407  hr: 0  min: 34  sec: 24\n",
      "epoch: 2  batch: 108 / 787  loss: 0.00288990620821197  hr: 0  min: 34  sec: 18\n",
      "epoch: 2  batch: 109 / 787  loss: 0.0029477347852662206  hr: 0  min: 34  sec: 10\n",
      "epoch: 2  batch: 110 / 787  loss: 0.002923837407830764  hr: 0  min: 34  sec: 4\n",
      "epoch: 2  batch: 111 / 787  loss: 0.00290418016097114  hr: 0  min: 33  sec: 58\n",
      "epoch: 2  batch: 112 / 787  loss: 0.002984068639697008  hr: 0  min: 33  sec: 51\n",
      "epoch: 2  batch: 113 / 787  loss: 0.0029615831357402214  hr: 0  min: 33  sec: 44\n",
      "epoch: 2  batch: 114 / 787  loss: 0.0029444862882149194  hr: 0  min: 33  sec: 38\n",
      "epoch: 2  batch: 115 / 787  loss: 0.0029216923396629484  hr: 0  min: 33  sec: 31\n",
      "epoch: 2  batch: 116 / 787  loss: 0.002900966091868307  hr: 0  min: 33  sec: 23\n",
      "epoch: 2  batch: 117 / 787  loss: 0.002878979470831557  hr: 0  min: 33  sec: 18\n",
      "epoch: 2  batch: 118 / 787  loss: 0.0028592611917578692  hr: 0  min: 33  sec: 10\n",
      "epoch: 2  batch: 119 / 787  loss: 0.0028509001165409296  hr: 0  min: 33  sec: 3\n",
      "epoch: 2  batch: 120 / 787  loss: 0.0029617631516885012  hr: 0  min: 32  sec: 54\n",
      "epoch: 2  batch: 121 / 787  loss: 0.0030288329397997827  hr: 0  min: 32  sec: 47\n",
      "epoch: 2  batch: 122 / 787  loss: 0.0030059376051630366  hr: 0  min: 32  sec: 42\n",
      "epoch: 2  batch: 123 / 787  loss: 0.00298654007923229  hr: 0  min: 32  sec: 36\n",
      "epoch: 2  batch: 124 / 787  loss: 0.0029772068432536007  hr: 0  min: 32  sec: 29\n",
      "epoch: 2  batch: 125 / 787  loss: 0.0029621797187719496  hr: 0  min: 32  sec: 36\n",
      "epoch: 2  batch: 126 / 787  loss: 0.003047169487607399  hr: 0  min: 32  sec: 32\n",
      "epoch: 2  batch: 127 / 787  loss: 0.0030444421455737085  hr: 0  min: 32  sec: 28\n",
      "epoch: 2  batch: 128 / 787  loss: 0.0030597700135786  hr: 0  min: 32  sec: 21\n",
      "epoch: 2  batch: 129 / 787  loss: 0.003043799270567116  hr: 0  min: 32  sec: 18\n",
      "epoch: 2  batch: 130 / 787  loss: 0.003037774177098002  hr: 0  min: 32  sec: 11\n",
      "epoch: 2  batch: 131 / 787  loss: 0.0030377291221587513  hr: 0  min: 32  sec: 6\n",
      "epoch: 2  batch: 132 / 787  loss: 0.0031050519630133003  hr: 0  min: 32  sec: 0\n",
      "epoch: 2  batch: 133 / 787  loss: 0.003133760712020225  hr: 0  min: 31  sec: 54\n",
      "epoch: 2  batch: 134 / 787  loss: 0.0031251375827983496  hr: 0  min: 31  sec: 49\n",
      "epoch: 2  batch: 135 / 787  loss: 0.003107847291674396  hr: 0  min: 31  sec: 55\n",
      "epoch: 2  batch: 136 / 787  loss: 0.003099277679960805  hr: 0  min: 31  sec: 52\n",
      "epoch: 2  batch: 137 / 787  loss: 0.0031075061455685103  hr: 0  min: 31  sec: 45\n",
      "epoch: 2  batch: 138 / 787  loss: 0.0030965735113831993  hr: 0  min: 31  sec: 38\n",
      "epoch: 2  batch: 139 / 787  loss: 0.0030763305475761137  hr: 0  min: 31  sec: 37\n",
      "epoch: 2  batch: 140 / 787  loss: 0.003072212834376842  hr: 0  min: 31  sec: 35\n",
      "epoch: 2  batch: 141 / 787  loss: 0.003054284048101581  hr: 0  min: 31  sec: 32\n",
      "epoch: 2  batch: 142 / 787  loss: 0.0030460683063236655  hr: 0  min: 31  sec: 34\n",
      "epoch: 2  batch: 143 / 787  loss: 0.003064003156765097  hr: 0  min: 31  sec: 32\n",
      "epoch: 2  batch: 144 / 787  loss: 0.0030479624159246064  hr: 0  min: 31  sec: 27\n",
      "epoch: 2  batch: 145 / 787  loss: 0.003030449274831034  hr: 0  min: 31  sec: 22\n",
      "epoch: 2  batch: 146 / 787  loss: 0.003028310270029541  hr: 0  min: 31  sec: 18\n",
      "epoch: 2  batch: 147 / 787  loss: 0.0030094435728364147  hr: 0  min: 31  sec: 17\n",
      "epoch: 2  batch: 148 / 787  loss: 0.002991673174570629  hr: 0  min: 31  sec: 17\n",
      "epoch: 2  batch: 149 / 787  loss: 0.002985012690785135  hr: 0  min: 31  sec: 13\n",
      "epoch: 2  batch: 150 / 787  loss: 0.0029710702336160468  hr: 0  min: 31  sec: 9\n",
      "epoch: 2  batch: 151 / 787  loss: 0.002955365371898076  hr: 0  min: 31  sec: 7\n",
      "epoch: 2  batch: 152 / 787  loss: 0.002942447700900773  hr: 0  min: 31  sec: 6\n",
      "epoch: 2  batch: 153 / 787  loss: 0.0029268247279676373  hr: 0  min: 31  sec: 9\n",
      "epoch: 2  batch: 154 / 787  loss: 0.0029100939170714443  hr: 0  min: 31  sec: 8\n",
      "epoch: 2  batch: 155 / 787  loss: 0.0028944846958057174  hr: 0  min: 31  sec: 5\n",
      "epoch: 2  batch: 156 / 787  loss: 0.002878285839053719  hr: 0  min: 31  sec: 3\n",
      "epoch: 2  batch: 157 / 787  loss: 0.0028733104535239116  hr: 0  min: 30  sec: 58\n",
      "epoch: 2  batch: 158 / 787  loss: 0.0028593670471816005  hr: 0  min: 30  sec: 54\n",
      "epoch: 2  batch: 159 / 787  loss: 0.0028577742044910578  hr: 0  min: 30  sec: 50\n",
      "epoch: 2  batch: 160 / 787  loss: 0.002841064302720042  hr: 0  min: 30  sec: 49\n",
      "epoch: 2  batch: 161 / 787  loss: 0.0029109602784831946  hr: 0  min: 30  sec: 44\n",
      "epoch: 2  batch: 162 / 787  loss: 0.0028969009740480774  hr: 0  min: 30  sec: 39\n",
      "epoch: 2  batch: 163 / 787  loss: 0.002957862368774132  hr: 0  min: 30  sec: 34\n",
      "epoch: 2  batch: 164 / 787  loss: 0.0029673507369141856  hr: 0  min: 30  sec: 35\n",
      "epoch: 2  batch: 165 / 787  loss: 0.002953132231822096  hr: 0  min: 30  sec: 32\n",
      "epoch: 2  batch: 166 / 787  loss: 0.0029409540255577863  hr: 0  min: 30  sec: 27\n",
      "epoch: 2  batch: 167 / 787  loss: 0.00292536895591445  hr: 0  min: 30  sec: 21\n",
      "epoch: 2  batch: 168 / 787  loss: 0.002913301875499504  hr: 0  min: 30  sec: 16\n",
      "epoch: 2  batch: 169 / 787  loss: 0.002901109629920298  hr: 0  min: 30  sec: 12\n",
      "epoch: 2  batch: 170 / 787  loss: 0.002889526770398815  hr: 0  min: 30  sec: 19\n",
      "epoch: 2  batch: 171 / 787  loss: 0.0028765383051410154  hr: 0  min: 30  sec: 14\n",
      "epoch: 2  batch: 172 / 787  loss: 0.0028685610617008766  hr: 0  min: 30  sec: 11\n",
      "epoch: 2  batch: 173 / 787  loss: 0.0028543575289840892  hr: 0  min: 30  sec: 8\n",
      "epoch: 2  batch: 174 / 787  loss: 0.002841430848271013  hr: 0  min: 30  sec: 6\n",
      "epoch: 2  batch: 175 / 787  loss: 0.0028262949906223056  hr: 0  min: 30  sec: 11\n",
      "epoch: 2  batch: 176 / 787  loss: 0.002813807604814594  hr: 0  min: 30  sec: 8\n",
      "epoch: 2  batch: 177 / 787  loss: 0.00282266597729834  hr: 0  min: 30  sec: 3\n",
      "epoch: 2  batch: 178 / 787  loss: 0.0028124007900321353  hr: 0  min: 29  sec: 59\n",
      "epoch: 2  batch: 179 / 787  loss: 0.0027991100851779377  hr: 0  min: 29  sec: 56\n",
      "epoch: 2  batch: 180 / 787  loss: 0.0027964700711891055  hr: 0  min: 29  sec: 53\n",
      "epoch: 2  batch: 181 / 787  loss: 0.0027843127110767185  hr: 0  min: 29  sec: 50\n",
      "epoch: 2  batch: 182 / 787  loss: 0.0027758148863430616  hr: 0  min: 29  sec: 45\n",
      "epoch: 2  batch: 183 / 787  loss: 0.0027623448320193713  hr: 0  min: 29  sec: 43\n",
      "epoch: 2  batch: 184 / 787  loss: 0.0027573321101005167  hr: 0  min: 29  sec: 41\n",
      "epoch: 2  batch: 185 / 787  loss: 0.0027444320282508693  hr: 0  min: 29  sec: 39\n",
      "epoch: 2  batch: 186 / 787  loss: 0.002732069299009288  hr: 0  min: 29  sec: 37\n",
      "epoch: 2  batch: 187 / 787  loss: 0.002719984146116431  hr: 0  min: 29  sec: 32\n",
      "epoch: 2  batch: 188 / 787  loss: 0.002706621329784963  hr: 0  min: 29  sec: 29\n",
      "epoch: 2  batch: 189 / 787  loss: 0.002823770856444734  hr: 0  min: 29  sec: 25\n",
      "epoch: 2  batch: 190 / 787  loss: 0.002887767628922838  hr: 0  min: 29  sec: 21\n",
      "epoch: 2  batch: 191 / 787  loss: 0.002873800539671253  hr: 0  min: 29  sec: 17\n",
      "epoch: 2  batch: 192 / 787  loss: 0.00286005678268945  hr: 0  min: 29  sec: 23\n",
      "epoch: 2  batch: 193 / 787  loss: 0.0028491252273263245  hr: 0  min: 29  sec: 20\n",
      "epoch: 2  batch: 194 / 787  loss: 0.0028564730483904175  hr: 0  min: 29  sec: 19\n",
      "epoch: 2  batch: 195 / 787  loss: 0.0028460285524861554  hr: 0  min: 29  sec: 16\n",
      "epoch: 2  batch: 196 / 787  loss: 0.002848609353591479  hr: 0  min: 29  sec: 14\n",
      "epoch: 2  batch: 197 / 787  loss: 0.0028456565728978383  hr: 0  min: 29  sec: 10\n",
      "epoch: 2  batch: 198 / 787  loss: 0.002833844181632322  hr: 0  min: 29  sec: 7\n",
      "epoch: 2  batch: 199 / 787  loss: 0.0028267868384414694  hr: 0  min: 29  sec: 3\n",
      "epoch: 2  batch: 200 / 787  loss: 0.0028588080263580195  hr: 0  min: 28  sec: 59\n",
      "epoch: 2  batch: 201 / 787  loss: 0.002845905853095997  hr: 0  min: 29  sec: 5\n",
      "epoch: 2  batch: 202 / 787  loss: 0.0028331388636102604  hr: 0  min: 29  sec: 2\n",
      "epoch: 2  batch: 203 / 787  loss: 0.0028753535381318735  hr: 0  min: 28  sec: 57\n",
      "epoch: 2  batch: 204 / 787  loss: 0.0028626842911619985  hr: 0  min: 28  sec: 55\n",
      "epoch: 2  batch: 205 / 787  loss: 0.0028508114767624294  hr: 0  min: 28  sec: 53\n",
      "epoch: 2  batch: 206 / 787  loss: 0.0028390293044899366  hr: 0  min: 28  sec: 49\n",
      "epoch: 2  batch: 207 / 787  loss: 0.0028329518021046125  hr: 0  min: 28  sec: 44\n",
      "epoch: 2  batch: 208 / 787  loss: 0.0028366095767313917  hr: 0  min: 28  sec: 42\n",
      "epoch: 2  batch: 209 / 787  loss: 0.0028262828683099624  hr: 0  min: 28  sec: 38\n",
      "epoch: 2  batch: 210 / 787  loss: 0.0028139301405947967  hr: 0  min: 28  sec: 36\n",
      "epoch: 2  batch: 211 / 787  loss: 0.002811448086564588  hr: 0  min: 28  sec: 32\n",
      "epoch: 2  batch: 212 / 787  loss: 0.0028005815280644215  hr: 0  min: 28  sec: 26\n",
      "epoch: 2  batch: 213 / 787  loss: 0.0027890188096581954  hr: 0  min: 28  sec: 23\n",
      "epoch: 2  batch: 214 / 787  loss: 0.0027772763597253476  hr: 0  min: 28  sec: 20\n",
      "epoch: 2  batch: 215 / 787  loss: 0.002765885179626842  hr: 0  min: 28  sec: 15\n",
      "epoch: 2  batch: 216 / 787  loss: 0.0027543891807268934  hr: 0  min: 28  sec: 12\n",
      "epoch: 2  batch: 217 / 787  loss: 0.002759079451628205  hr: 0  min: 28  sec: 7\n",
      "epoch: 2  batch: 218 / 787  loss: 0.0027587266565848425  hr: 0  min: 28  sec: 4\n",
      "epoch: 2  batch: 219 / 787  loss: 0.0027485926916368825  hr: 0  min: 28  sec: 0\n",
      "epoch: 2  batch: 220 / 787  loss: 0.002737262726705839  hr: 0  min: 27  sec: 56\n",
      "epoch: 2  batch: 221 / 787  loss: 0.002733750073057473  hr: 0  min: 27  sec: 54\n",
      "epoch: 2  batch: 222 / 787  loss: 0.0027234541239276704  hr: 0  min: 27  sec: 51\n",
      "epoch: 2  batch: 223 / 787  loss: 0.0027130510082605166  hr: 0  min: 27  sec: 48\n",
      "epoch: 2  batch: 224 / 787  loss: 0.0027018623097449434  hr: 0  min: 27  sec: 48\n",
      "epoch: 2  batch: 225 / 787  loss: 0.002690678797144857  hr: 0  min: 27  sec: 45\n",
      "epoch: 2  batch: 226 / 787  loss: 0.002679926207787728  hr: 0  min: 27  sec: 43\n",
      "epoch: 2  batch: 227 / 787  loss: 0.0026931159166487893  hr: 0  min: 27  sec: 38\n",
      "epoch: 2  batch: 228 / 787  loss: 0.0026819139547705276  hr: 0  min: 27  sec: 36\n",
      "epoch: 2  batch: 229 / 787  loss: 0.002671175784094353  hr: 0  min: 27  sec: 32\n",
      "epoch: 2  batch: 230 / 787  loss: 0.0026610485128121977  hr: 0  min: 27  sec: 31\n",
      "epoch: 2  batch: 231 / 787  loss: 0.0026521495768565533  hr: 0  min: 27  sec: 26\n",
      "epoch: 2  batch: 232 / 787  loss: 0.00265493790476316  hr: 0  min: 27  sec: 23\n",
      "epoch: 2  batch: 233 / 787  loss: 0.0026451121067529336  hr: 0  min: 27  sec: 20\n",
      "epoch: 2  batch: 234 / 787  loss: 0.0026466971859420473  hr: 0  min: 27  sec: 17\n",
      "epoch: 2  batch: 235 / 787  loss: 0.002637081209785662  hr: 0  min: 27  sec: 14\n",
      "epoch: 2  batch: 236 / 787  loss: 0.0026266622477678692  hr: 0  min: 27  sec: 11\n",
      "epoch: 2  batch: 237 / 787  loss: 0.0026169392301893563  hr: 0  min: 27  sec: 8\n",
      "epoch: 2  batch: 238 / 787  loss: 0.0026065135356200227  hr: 0  min: 27  sec: 5\n",
      "epoch: 2  batch: 239 / 787  loss: 0.002596841270026603  hr: 0  min: 27  sec: 1\n",
      "epoch: 2  batch: 240 / 787  loss: 0.002613351377961711  hr: 0  min: 26  sec: 58\n",
      "epoch: 2  batch: 241 / 787  loss: 0.002603166913398719  hr: 0  min: 26  sec: 55\n",
      "epoch: 2  batch: 242 / 787  loss: 0.0025931729244555095  hr: 0  min: 26  sec: 51\n",
      "epoch: 2  batch: 243 / 787  loss: 0.002618565169137178  hr: 0  min: 26  sec: 48\n",
      "epoch: 2  batch: 244 / 787  loss: 0.0026087872125780914  hr: 0  min: 26  sec: 44\n",
      "epoch: 2  batch: 245 / 787  loss: 0.0026720330551532764  hr: 0  min: 26  sec: 40\n",
      "epoch: 2  batch: 246 / 787  loss: 0.0026622303318832355  hr: 0  min: 26  sec: 37\n",
      "epoch: 2  batch: 247 / 787  loss: 0.002652037227212881  hr: 0  min: 26  sec: 35\n",
      "epoch: 2  batch: 248 / 787  loss: 0.0026436831687474734  hr: 0  min: 26  sec: 32\n",
      "epoch: 2  batch: 249 / 787  loss: 0.0026339156430772877  hr: 0  min: 26  sec: 28\n",
      "epoch: 2  batch: 250 / 787  loss: 0.0026253506462671793  hr: 0  min: 26  sec: 25\n",
      "epoch: 2  batch: 251 / 787  loss: 0.002654384832378881  hr: 0  min: 26  sec: 22\n",
      "epoch: 2  batch: 252 / 787  loss: 0.0026482350973339405  hr: 0  min: 26  sec: 17\n",
      "epoch: 2  batch: 253 / 787  loss: 0.002639365707408626  hr: 0  min: 26  sec: 14\n",
      "epoch: 2  batch: 254 / 787  loss: 0.002630703132262721  hr: 0  min: 26  sec: 11\n",
      "epoch: 2  batch: 255 / 787  loss: 0.0026214906567474825  hr: 0  min: 26  sec: 8\n",
      "epoch: 2  batch: 256 / 787  loss: 0.0026134618981927815  hr: 0  min: 26  sec: 5\n",
      "epoch: 2  batch: 257 / 787  loss: 0.0026049048386506033  hr: 0  min: 26  sec: 3\n",
      "epoch: 2  batch: 258 / 787  loss: 0.002597605877196283  hr: 0  min: 26  sec: 0\n",
      "epoch: 2  batch: 259 / 787  loss: 0.0026606012766252174  hr: 0  min: 25  sec: 58\n",
      "epoch: 2  batch: 260 / 787  loss: 0.002651677566049889  hr: 0  min: 25  sec: 54\n",
      "epoch: 2  batch: 261 / 787  loss: 0.0026944624962016647  hr: 0  min: 25  sec: 50\n",
      "epoch: 2  batch: 262 / 787  loss: 0.002695937061276667  hr: 0  min: 25  sec: 45\n",
      "epoch: 2  batch: 263 / 787  loss: 0.0026869524629169563  hr: 0  min: 25  sec: 42\n",
      "epoch: 2  batch: 264 / 787  loss: 0.002678151142500803  hr: 0  min: 25  sec: 40\n",
      "epoch: 2  batch: 265 / 787  loss: 0.0026705377519389895  hr: 0  min: 25  sec: 36\n",
      "epoch: 2  batch: 266 / 787  loss: 0.0026612332146207776  hr: 0  min: 25  sec: 33\n",
      "epoch: 2  batch: 267 / 787  loss: 0.00265223071772007  hr: 0  min: 25  sec: 28\n",
      "epoch: 2  batch: 268 / 787  loss: 0.002646460894023072  hr: 0  min: 25  sec: 24\n",
      "epoch: 2  batch: 269 / 787  loss: 0.002919434879044829  hr: 0  min: 25  sec: 21\n",
      "epoch: 2  batch: 270 / 787  loss: 0.00291039395869356  hr: 0  min: 25  sec: 18\n",
      "epoch: 2  batch: 271 / 787  loss: 0.002901542998111044  hr: 0  min: 25  sec: 13\n",
      "epoch: 2  batch: 272 / 787  loss: 0.0028931537990501965  hr: 0  min: 25  sec: 10\n",
      "epoch: 2  batch: 273 / 787  loss: 0.002886773805778334  hr: 0  min: 25  sec: 8\n",
      "epoch: 2  batch: 274 / 787  loss: 0.0028836092965088505  hr: 0  min: 25  sec: 4\n",
      "epoch: 2  batch: 275 / 787  loss: 0.0028783228012881326  hr: 0  min: 25  sec: 0\n",
      "epoch: 2  batch: 276 / 787  loss: 0.0028719825189171756  hr: 0  min: 24  sec: 57\n",
      "epoch: 2  batch: 277 / 787  loss: 0.0028700593798333066  hr: 0  min: 24  sec: 53\n",
      "epoch: 2  batch: 278 / 787  loss: 0.0028989754590308865  hr: 0  min: 24  sec: 49\n",
      "epoch: 2  batch: 279 / 787  loss: 0.0028894903567743967  hr: 0  min: 24  sec: 46\n",
      "epoch: 2  batch: 280 / 787  loss: 0.002880187751569403  hr: 0  min: 24  sec: 42\n",
      "epoch: 2  batch: 281 / 787  loss: 0.0028755402475122357  hr: 0  min: 24  sec: 39\n",
      "epoch: 2  batch: 282 / 787  loss: 0.0028966058664133686  hr: 0  min: 24  sec: 35\n",
      "epoch: 2  batch: 283 / 787  loss: 0.0029269081712457725  hr: 0  min: 24  sec: 31\n",
      "epoch: 2  batch: 284 / 787  loss: 0.0030126723891364337  hr: 0  min: 24  sec: 28\n",
      "epoch: 2  batch: 285 / 787  loss: 0.0030077784475890856  hr: 0  min: 24  sec: 24\n",
      "epoch: 2  batch: 286 / 787  loss: 0.003019720795281456  hr: 0  min: 24  sec: 20\n",
      "epoch: 2  batch: 287 / 787  loss: 0.003218493366604655  hr: 0  min: 24  sec: 17\n",
      "epoch: 2  batch: 288 / 787  loss: 0.0032090159398750176  hr: 0  min: 24  sec: 13\n",
      "epoch: 2  batch: 289 / 787  loss: 0.0032010973867928855  hr: 0  min: 24  sec: 11\n",
      "epoch: 2  batch: 290 / 787  loss: 0.0032608851098459503  hr: 0  min: 24  sec: 7\n",
      "epoch: 2  batch: 291 / 787  loss: 0.0033293483646116874  hr: 0  min: 24  sec: 3\n",
      "epoch: 2  batch: 292 / 787  loss: 0.003330815635944082  hr: 0  min: 23  sec: 59\n",
      "epoch: 2  batch: 293 / 787  loss: 0.00332479049284178  hr: 0  min: 23  sec: 56\n",
      "epoch: 2  batch: 294 / 787  loss: 0.0033300767269489975  hr: 0  min: 23  sec: 53\n",
      "epoch: 2  batch: 295 / 787  loss: 0.0033285357245989933  hr: 0  min: 23  sec: 50\n",
      "epoch: 2  batch: 296 / 787  loss: 0.00334163438167928  hr: 0  min: 23  sec: 48\n",
      "epoch: 2  batch: 297 / 787  loss: 0.0033561640044439877  hr: 0  min: 23  sec: 45\n",
      "epoch: 2  batch: 298 / 787  loss: 0.0033776292688429714  hr: 0  min: 23  sec: 42\n",
      "epoch: 2  batch: 299 / 787  loss: 0.0033845405488443967  hr: 0  min: 23  sec: 39\n",
      "epoch: 2  batch: 300 / 787  loss: 0.0035713542127147474  hr: 0  min: 23  sec: 36\n",
      "epoch: 2  batch: 301 / 787  loss: 0.003562532743043421  hr: 0  min: 23  sec: 33\n",
      "epoch: 2  batch: 302 / 787  loss: 0.003563103005715988  hr: 0  min: 23  sec: 31\n",
      "epoch: 2  batch: 303 / 787  loss: 0.0035549537826689856  hr: 0  min: 23  sec: 28\n",
      "epoch: 2  batch: 304 / 787  loss: 0.0035564618480126036  hr: 0  min: 23  sec: 25\n",
      "epoch: 2  batch: 305 / 787  loss: 0.003806470398128903  hr: 0  min: 23  sec: 22\n",
      "epoch: 2  batch: 306 / 787  loss: 0.0038033429318007988  hr: 0  min: 23  sec: 19\n",
      "epoch: 2  batch: 307 / 787  loss: 0.003801522995900827  hr: 0  min: 23  sec: 16\n",
      "epoch: 2  batch: 308 / 787  loss: 0.003824918970320897  hr: 0  min: 23  sec: 13\n",
      "epoch: 2  batch: 309 / 787  loss: 0.0038529296406461105  hr: 0  min: 23  sec: 11\n",
      "epoch: 2  batch: 310 / 787  loss: 0.003851215979469294  hr: 0  min: 23  sec: 9\n",
      "epoch: 2  batch: 311 / 787  loss: 0.003845963037396494  hr: 0  min: 23  sec: 6\n",
      "epoch: 2  batch: 312 / 787  loss: 0.003867761130701789  hr: 0  min: 23  sec: 3\n",
      "epoch: 2  batch: 313 / 787  loss: 0.00386001783432982  hr: 0  min: 23  sec: 0\n",
      "epoch: 2  batch: 314 / 787  loss: 0.003850855441791062  hr: 0  min: 22  sec: 59\n",
      "epoch: 2  batch: 315 / 787  loss: 0.0038417774872089336  hr: 0  min: 22  sec: 56\n",
      "epoch: 2  batch: 316 / 787  loss: 0.0039044069228019664  hr: 0  min: 22  sec: 53\n",
      "epoch: 2  batch: 317 / 787  loss: 0.0039030177486500017  hr: 0  min: 22  sec: 49\n",
      "epoch: 2  batch: 318 / 787  loss: 0.004008149232361364  hr: 0  min: 22  sec: 46\n",
      "epoch: 2  batch: 319 / 787  loss: 0.004035987639312522  hr: 0  min: 22  sec: 42\n",
      "epoch: 2  batch: 320 / 787  loss: 0.004053361686828794  hr: 0  min: 22  sec: 39\n",
      "epoch: 2  batch: 321 / 787  loss: 0.004061012601958968  hr: 0  min: 22  sec: 37\n",
      "epoch: 2  batch: 322 / 787  loss: 0.004050939226234173  hr: 0  min: 22  sec: 34\n",
      "epoch: 2  batch: 323 / 787  loss: 0.00404789904371263  hr: 0  min: 22  sec: 32\n",
      "epoch: 2  batch: 324 / 787  loss: 0.004043487500175504  hr: 0  min: 22  sec: 29\n",
      "epoch: 2  batch: 325 / 787  loss: 0.004035000668775935  hr: 0  min: 22  sec: 26\n",
      "epoch: 2  batch: 326 / 787  loss: 0.004029399129225332  hr: 0  min: 22  sec: 23\n",
      "epoch: 2  batch: 327 / 787  loss: 0.004033114828978557  hr: 0  min: 22  sec: 21\n",
      "epoch: 2  batch: 328 / 787  loss: 0.004049594524941512  hr: 0  min: 22  sec: 18\n",
      "epoch: 2  batch: 329 / 787  loss: 0.004060491692569563  hr: 0  min: 22  sec: 16\n",
      "epoch: 2  batch: 330 / 787  loss: 0.004049227051533914  hr: 0  min: 22  sec: 14\n",
      "epoch: 2  batch: 331 / 787  loss: 0.004144710179773902  hr: 0  min: 22  sec: 12\n",
      "epoch: 2  batch: 332 / 787  loss: 0.004265635580773653  hr: 0  min: 22  sec: 9\n",
      "epoch: 2  batch: 333 / 787  loss: 0.004258826363776872  hr: 0  min: 22  sec: 8\n",
      "epoch: 2  batch: 334 / 787  loss: 0.004297574552591307  hr: 0  min: 22  sec: 4\n",
      "epoch: 2  batch: 335 / 787  loss: 0.00428875471676401  hr: 0  min: 22  sec: 1\n",
      "epoch: 2  batch: 336 / 787  loss: 0.004278435227466356  hr: 0  min: 21  sec: 58\n",
      "epoch: 2  batch: 337 / 787  loss: 0.0042796786209117225  hr: 0  min: 21  sec: 55\n",
      "epoch: 2  batch: 338 / 787  loss: 0.004270558681994464  hr: 0  min: 21  sec: 51\n",
      "epoch: 2  batch: 339 / 787  loss: 0.004261086643170387  hr: 0  min: 21  sec: 47\n",
      "epoch: 2  batch: 340 / 787  loss: 0.004271053797214442  hr: 0  min: 21  sec: 44\n",
      "epoch: 2  batch: 341 / 787  loss: 0.004261926884544109  hr: 0  min: 21  sec: 41\n",
      "epoch: 2  batch: 342 / 787  loss: 0.004287327029897181  hr: 0  min: 21  sec: 38\n",
      "epoch: 2  batch: 343 / 787  loss: 0.004323264024735961  hr: 0  min: 21  sec: 34\n",
      "epoch: 2  batch: 344 / 787  loss: 0.004317877371651648  hr: 0  min: 21  sec: 31\n",
      "epoch: 2  batch: 345 / 787  loss: 0.004306476876482138  hr: 0  min: 21  sec: 27\n",
      "epoch: 2  batch: 346 / 787  loss: 0.004296565220906262  hr: 0  min: 21  sec: 24\n",
      "epoch: 2  batch: 347 / 787  loss: 0.004287615870466115  hr: 0  min: 21  sec: 21\n",
      "epoch: 2  batch: 348 / 787  loss: 0.004283205616539117  hr: 0  min: 21  sec: 18\n",
      "epoch: 2  batch: 349 / 787  loss: 0.004277056324378091  hr: 0  min: 21  sec: 15\n",
      "epoch: 2  batch: 350 / 787  loss: 0.004267017849446607  hr: 0  min: 21  sec: 13\n",
      "epoch: 2  batch: 351 / 787  loss: 0.004262639336449497  hr: 0  min: 21  sec: 9\n",
      "epoch: 2  batch: 352 / 787  loss: 0.004251948873595459  hr: 0  min: 21  sec: 10\n",
      "epoch: 2  batch: 353 / 787  loss: 0.004251047376193378  hr: 0  min: 21  sec: 6\n",
      "epoch: 2  batch: 354 / 787  loss: 0.004242556561585345  hr: 0  min: 21  sec: 3\n",
      "epoch: 2  batch: 355 / 787  loss: 0.004231980683660777  hr: 0  min: 20  sec: 59\n",
      "epoch: 2  batch: 356 / 787  loss: 0.0042435940860864165  hr: 0  min: 20  sec: 56\n",
      "epoch: 2  batch: 357 / 787  loss: 0.004234467913468071  hr: 0  min: 20  sec: 53\n",
      "epoch: 2  batch: 358 / 787  loss: 0.004223784425189555  hr: 0  min: 20  sec: 50\n",
      "epoch: 2  batch: 359 / 787  loss: 0.004213442519629143  hr: 0  min: 20  sec: 47\n",
      "epoch: 2  batch: 360 / 787  loss: 0.0042171135721421  hr: 0  min: 20  sec: 44\n",
      "epoch: 2  batch: 361 / 787  loss: 0.004206884817526305  hr: 0  min: 20  sec: 41\n",
      "epoch: 2  batch: 362 / 787  loss: 0.004196131080754026  hr: 0  min: 20  sec: 38\n",
      "epoch: 2  batch: 363 / 787  loss: 0.004197645700046312  hr: 0  min: 20  sec: 38\n",
      "epoch: 2  batch: 364 / 787  loss: 0.004188489007811858  hr: 0  min: 20  sec: 36\n",
      "epoch: 2  batch: 365 / 787  loss: 0.004179083949795363  hr: 0  min: 20  sec: 32\n",
      "epoch: 2  batch: 366 / 787  loss: 0.004169255168050912  hr: 0  min: 20  sec: 29\n",
      "epoch: 2  batch: 367 / 787  loss: 0.004159686577146967  hr: 0  min: 20  sec: 26\n",
      "epoch: 2  batch: 368 / 787  loss: 0.004158147830544887  hr: 0  min: 20  sec: 23\n",
      "epoch: 2  batch: 369 / 787  loss: 0.0041485306687213275  hr: 0  min: 20  sec: 20\n",
      "epoch: 2  batch: 370 / 787  loss: 0.0041407296383309785  hr: 0  min: 20  sec: 17\n",
      "epoch: 2  batch: 371 / 787  loss: 0.004130720486574194  hr: 0  min: 20  sec: 13\n",
      "epoch: 2  batch: 372 / 787  loss: 0.004125296906153458  hr: 0  min: 20  sec: 10\n",
      "epoch: 2  batch: 373 / 787  loss: 0.0041183298921456625  hr: 0  min: 20  sec: 6\n",
      "epoch: 2  batch: 374 / 787  loss: 0.004109575476783469  hr: 0  min: 20  sec: 3\n",
      "epoch: 2  batch: 375 / 787  loss: 0.0041004320095526054  hr: 0  min: 20  sec: 1\n",
      "epoch: 2  batch: 376 / 787  loss: 0.004144644821513175  hr: 0  min: 19  sec: 57\n",
      "epoch: 2  batch: 377 / 787  loss: 0.004139230592571265  hr: 0  min: 19  sec: 54\n",
      "epoch: 2  batch: 378 / 787  loss: 0.004137824782687449  hr: 0  min: 19  sec: 51\n",
      "epoch: 2  batch: 379 / 787  loss: 0.004128048106550981  hr: 0  min: 19  sec: 48\n",
      "epoch: 2  batch: 380 / 787  loss: 0.004120201442664869  hr: 0  min: 19  sec: 45\n",
      "epoch: 2  batch: 381 / 787  loss: 0.004121444340032315  hr: 0  min: 19  sec: 42\n",
      "epoch: 2  batch: 382 / 787  loss: 0.00411296775789306  hr: 0  min: 19  sec: 39\n",
      "epoch: 2  batch: 383 / 787  loss: 0.004102879417955318  hr: 0  min: 19  sec: 35\n",
      "epoch: 2  batch: 384 / 787  loss: 0.004092544055727861  hr: 0  min: 19  sec: 32\n",
      "epoch: 2  batch: 385 / 787  loss: 0.004084430413446147  hr: 0  min: 19  sec: 29\n",
      "epoch: 2  batch: 386 / 787  loss: 0.004075313718448733  hr: 0  min: 19  sec: 28\n",
      "epoch: 2  batch: 387 / 787  loss: 0.004066310024864488  hr: 0  min: 19  sec: 25\n",
      "epoch: 2  batch: 388 / 787  loss: 0.004057940857425829  hr: 0  min: 19  sec: 23\n",
      "epoch: 2  batch: 389 / 787  loss: 0.004051241418249501  hr: 0  min: 19  sec: 20\n",
      "epoch: 2  batch: 390 / 787  loss: 0.004047121134107538  hr: 0  min: 19  sec: 17\n",
      "epoch: 2  batch: 391 / 787  loss: 0.004040186438384071  hr: 0  min: 19  sec: 15\n",
      "epoch: 2  batch: 392 / 787  loss: 0.004032150083136857  hr: 0  min: 19  sec: 12\n",
      "epoch: 2  batch: 393 / 787  loss: 0.004023441655284953  hr: 0  min: 19  sec: 9\n",
      "epoch: 2  batch: 394 / 787  loss: 0.004015304174161632  hr: 0  min: 19  sec: 8\n",
      "epoch: 2  batch: 395 / 787  loss: 0.004012553494009543  hr: 0  min: 19  sec: 4\n",
      "epoch: 2  batch: 396 / 787  loss: 0.004003853357778898  hr: 0  min: 19  sec: 1\n",
      "epoch: 2  batch: 397 / 787  loss: 0.003996601524686274  hr: 0  min: 18  sec: 58\n",
      "epoch: 2  batch: 398 / 787  loss: 0.00398720483202458  hr: 0  min: 18  sec: 55\n",
      "epoch: 2  batch: 399 / 787  loss: 0.004034716170098817  hr: 0  min: 18  sec: 52\n",
      "epoch: 2  batch: 400 / 787  loss: 0.004025175832357491  hr: 0  min: 18  sec: 49\n",
      "epoch: 2  batch: 401 / 787  loss: 0.004015506003329244  hr: 0  min: 18  sec: 46\n",
      "epoch: 2  batch: 402 / 787  loss: 0.0040489253862769645  hr: 0  min: 18  sec: 43\n",
      "epoch: 2  batch: 403 / 787  loss: 0.004039760263645086  hr: 0  min: 18  sec: 41\n",
      "epoch: 2  batch: 404 / 787  loss: 0.004030094569352154  hr: 0  min: 18  sec: 38\n",
      "epoch: 2  batch: 405 / 787  loss: 0.004020940050242733  hr: 0  min: 18  sec: 35\n",
      "epoch: 2  batch: 406 / 787  loss: 0.004011703130860499  hr: 0  min: 18  sec: 32\n",
      "epoch: 2  batch: 407 / 787  loss: 0.004002740912470356  hr: 0  min: 18  sec: 29\n",
      "epoch: 2  batch: 408 / 787  loss: 0.003999767092026042  hr: 0  min: 18  sec: 26\n",
      "epoch: 2  batch: 409 / 787  loss: 0.003991279158774538  hr: 0  min: 18  sec: 23\n",
      "epoch: 2  batch: 410 / 787  loss: 0.003982027161940251  hr: 0  min: 18  sec: 23\n",
      "epoch: 2  batch: 411 / 787  loss: 0.003972894316781346  hr: 0  min: 18  sec: 20\n",
      "epoch: 2  batch: 412 / 787  loss: 0.003963743514123936  hr: 0  min: 18  sec: 18\n",
      "epoch: 2  batch: 413 / 787  loss: 0.003954966149858614  hr: 0  min: 18  sec: 16\n",
      "epoch: 2  batch: 414 / 787  loss: 0.003945935367671818  hr: 0  min: 18  sec: 13\n",
      "epoch: 2  batch: 415 / 787  loss: 0.003937380668336907  hr: 0  min: 18  sec: 10\n",
      "epoch: 2  batch: 416 / 787  loss: 0.003933652930964644  hr: 0  min: 18  sec: 7\n",
      "epoch: 2  batch: 417 / 787  loss: 0.003924836315286102  hr: 0  min: 18  sec: 5\n",
      "epoch: 2  batch: 418 / 787  loss: 0.003923511627223788  hr: 0  min: 18  sec: 3\n",
      "epoch: 2  batch: 419 / 787  loss: 0.003914783897544068  hr: 0  min: 18  sec: 0\n",
      "epoch: 2  batch: 420 / 787  loss: 0.003927203844207161  hr: 0  min: 17  sec: 57\n",
      "epoch: 2  batch: 421 / 787  loss: 0.003924241128422966  hr: 0  min: 17  sec: 54\n",
      "epoch: 2  batch: 422 / 787  loss: 0.00401817592590831  hr: 0  min: 17  sec: 51\n",
      "epoch: 2  batch: 423 / 787  loss: 0.004022689858076094  hr: 0  min: 17  sec: 49\n",
      "epoch: 2  batch: 424 / 787  loss: 0.004020245692375301  hr: 0  min: 17  sec: 46\n",
      "epoch: 2  batch: 425 / 787  loss: 0.004012346627081141  hr: 0  min: 17  sec: 43\n",
      "epoch: 2  batch: 426 / 787  loss: 0.004003250257675306  hr: 0  min: 17  sec: 40\n",
      "epoch: 2  batch: 427 / 787  loss: 0.004019702098800461  hr: 0  min: 17  sec: 37\n",
      "epoch: 2  batch: 428 / 787  loss: 0.0040112333780572915  hr: 0  min: 17  sec: 34\n",
      "epoch: 2  batch: 429 / 787  loss: 0.004002309627239408  hr: 0  min: 17  sec: 30\n",
      "epoch: 2  batch: 430 / 787  loss: 0.0039935161507496245  hr: 0  min: 17  sec: 27\n",
      "epoch: 2  batch: 431 / 787  loss: 0.003988579695828512  hr: 0  min: 17  sec: 24\n",
      "epoch: 2  batch: 432 / 787  loss: 0.003980056196067178  hr: 0  min: 17  sec: 22\n",
      "epoch: 2  batch: 433 / 787  loss: 0.003971761331457148  hr: 0  min: 17  sec: 20\n",
      "epoch: 2  batch: 434 / 787  loss: 0.0039642947017602  hr: 0  min: 17  sec: 16\n",
      "epoch: 2  batch: 435 / 787  loss: 0.00395826874330945  hr: 0  min: 17  sec: 14\n",
      "epoch: 2  batch: 436 / 787  loss: 0.003949934891736137  hr: 0  min: 17  sec: 11\n",
      "epoch: 2  batch: 437 / 787  loss: 0.003942326472911534  hr: 0  min: 17  sec: 8\n",
      "epoch: 2  batch: 438 / 787  loss: 0.003934162118668576  hr: 0  min: 17  sec: 5\n",
      "epoch: 2  batch: 439 / 787  loss: 0.003927027103739073  hr: 0  min: 17  sec: 2\n",
      "epoch: 2  batch: 440 / 787  loss: 0.003919007788268077  hr: 0  min: 16  sec: 59\n",
      "epoch: 2  batch: 441 / 787  loss: 0.0039126410230805745  hr: 0  min: 16  sec: 55\n",
      "epoch: 2  batch: 442 / 787  loss: 0.0039046532481372023  hr: 0  min: 16  sec: 53\n",
      "epoch: 2  batch: 443 / 787  loss: 0.003913167538916278  hr: 0  min: 16  sec: 49\n",
      "epoch: 2  batch: 444 / 787  loss: 0.0039051440845624865  hr: 0  min: 16  sec: 47\n",
      "epoch: 2  batch: 445 / 787  loss: 0.00389873314765936  hr: 0  min: 16  sec: 43\n",
      "epoch: 2  batch: 446 / 787  loss: 0.003892066295961108  hr: 0  min: 16  sec: 40\n",
      "epoch: 2  batch: 447 / 787  loss: 0.003885644182251085  hr: 0  min: 16  sec: 37\n",
      "epoch: 2  batch: 448 / 787  loss: 0.003877960072616458  hr: 0  min: 16  sec: 34\n",
      "epoch: 2  batch: 449 / 787  loss: 0.003870558283035851  hr: 0  min: 16  sec: 31\n",
      "epoch: 2  batch: 450 / 787  loss: 0.0038637804323095933  hr: 0  min: 16  sec: 28\n",
      "epoch: 2  batch: 451 / 787  loss: 0.003856087983602672  hr: 0  min: 16  sec: 25\n",
      "epoch: 2  batch: 452 / 787  loss: 0.003848118618455462  hr: 0  min: 16  sec: 24\n",
      "epoch: 2  batch: 453 / 787  loss: 0.0038413970331133552  hr: 0  min: 16  sec: 20\n",
      "epoch: 2  batch: 454 / 787  loss: 0.003840202943398566  hr: 0  min: 16  sec: 17\n",
      "epoch: 2  batch: 455 / 787  loss: 0.003857403041591489  hr: 0  min: 16  sec: 14\n",
      "epoch: 2  batch: 456 / 787  loss: 0.0038509252166190173  hr: 0  min: 16  sec: 11\n",
      "epoch: 2  batch: 457 / 787  loss: 0.0038432366255387006  hr: 0  min: 16  sec: 7\n",
      "epoch: 2  batch: 458 / 787  loss: 0.003835964781672167  hr: 0  min: 16  sec: 5\n",
      "epoch: 2  batch: 459 / 787  loss: 0.0038287472216681777  hr: 0  min: 16  sec: 1\n",
      "epoch: 2  batch: 460 / 787  loss: 0.003854391008518059  hr: 0  min: 15  sec: 59\n",
      "epoch: 2  batch: 461 / 787  loss: 0.003847381146870899  hr: 0  min: 15  sec: 56\n",
      "epoch: 2  batch: 462 / 787  loss: 0.0038396886926790946  hr: 0  min: 15  sec: 52\n",
      "epoch: 2  batch: 463 / 787  loss: 0.0038329117836068097  hr: 0  min: 15  sec: 49\n",
      "epoch: 2  batch: 464 / 787  loss: 0.0038265273823181534  hr: 0  min: 15  sec: 46\n",
      "epoch: 2  batch: 465 / 787  loss: 0.0038199503107124097  hr: 0  min: 15  sec: 43\n",
      "epoch: 2  batch: 466 / 787  loss: 0.0038184484043405913  hr: 0  min: 15  sec: 40\n",
      "epoch: 2  batch: 467 / 787  loss: 0.0038117063711325056  hr: 0  min: 15  sec: 37\n",
      "epoch: 2  batch: 468 / 787  loss: 0.0038093395326144765  hr: 0  min: 15  sec: 34\n",
      "epoch: 2  batch: 469 / 787  loss: 0.003803983839671326  hr: 0  min: 15  sec: 31\n",
      "epoch: 2  batch: 470 / 787  loss: 0.0037983164912959106  hr: 0  min: 15  sec: 28\n",
      "epoch: 2  batch: 471 / 787  loss: 0.0037914418270047693  hr: 0  min: 15  sec: 25\n",
      "epoch: 2  batch: 472 / 787  loss: 0.0038167535770036796  hr: 0  min: 15  sec: 22\n",
      "epoch: 2  batch: 473 / 787  loss: 0.0038155187831698852  hr: 0  min: 15  sec: 19\n",
      "epoch: 2  batch: 474 / 787  loss: 0.003808507069355645  hr: 0  min: 15  sec: 16\n",
      "epoch: 2  batch: 475 / 787  loss: 0.003806759550720208  hr: 0  min: 15  sec: 12\n",
      "epoch: 2  batch: 476 / 787  loss: 0.004028385669767549  hr: 0  min: 15  sec: 9\n",
      "epoch: 2  batch: 477 / 787  loss: 0.00404108348736355  hr: 0  min: 15  sec: 6\n",
      "epoch: 2  batch: 478 / 787  loss: 0.004036281891667652  hr: 0  min: 15  sec: 4\n",
      "epoch: 2  batch: 479 / 787  loss: 0.0040290402517970435  hr: 0  min: 15  sec: 1\n",
      "epoch: 2  batch: 480 / 787  loss: 0.0040218841977851605  hr: 0  min: 14  sec: 58\n",
      "epoch: 2  batch: 481 / 787  loss: 0.004014757406149251  hr: 0  min: 14  sec: 55\n",
      "epoch: 2  batch: 482 / 787  loss: 0.004007338894647304  hr: 0  min: 14  sec: 54\n",
      "epoch: 2  batch: 483 / 787  loss: 0.003999827601171993  hr: 0  min: 14  sec: 51\n",
      "epoch: 2  batch: 484 / 787  loss: 0.004006051197596335  hr: 0  min: 14  sec: 47\n",
      "epoch: 2  batch: 485 / 787  loss: 0.004017106254408664  hr: 0  min: 14  sec: 44\n",
      "epoch: 2  batch: 486 / 787  loss: 0.004043161450247347  hr: 0  min: 14  sec: 41\n",
      "epoch: 2  batch: 487 / 787  loss: 0.004038137809888104  hr: 0  min: 14  sec: 39\n",
      "epoch: 2  batch: 488 / 787  loss: 0.0040439203374441355  hr: 0  min: 14  sec: 36\n",
      "epoch: 2  batch: 489 / 787  loss: 0.004047181199840175  hr: 0  min: 14  sec: 33\n",
      "epoch: 2  batch: 490 / 787  loss: 0.004046083093123102  hr: 0  min: 14  sec: 30\n",
      "epoch: 2  batch: 491 / 787  loss: 0.004040573496622121  hr: 0  min: 14  sec: 27\n",
      "epoch: 2  batch: 492 / 787  loss: 0.004059389133884891  hr: 0  min: 14  sec: 23\n",
      "epoch: 2  batch: 493 / 787  loss: 0.0040605335538457365  hr: 0  min: 14  sec: 20\n",
      "epoch: 2  batch: 494 / 787  loss: 0.00407119623073947  hr: 0  min: 14  sec: 17\n",
      "epoch: 2  batch: 495 / 787  loss: 0.004074393393092017  hr: 0  min: 14  sec: 14\n",
      "epoch: 2  batch: 496 / 787  loss: 0.00407265797422306  hr: 0  min: 14  sec: 11\n",
      "epoch: 2  batch: 497 / 787  loss: 0.004085328302659344  hr: 0  min: 14  sec: 8\n",
      "epoch: 2  batch: 498 / 787  loss: 0.004079130704167073  hr: 0  min: 14  sec: 5\n",
      "epoch: 2  batch: 499 / 787  loss: 0.004078039775966697  hr: 0  min: 14  sec: 2\n",
      "epoch: 2  batch: 500 / 787  loss: 0.004071768201800296  hr: 0  min: 13  sec: 59\n",
      "epoch: 2  batch: 501 / 787  loss: 0.0040648323472437286  hr: 0  min: 13  sec: 56\n",
      "epoch: 2  batch: 502 / 787  loss: 0.004061636205863966  hr: 0  min: 13  sec: 53\n",
      "epoch: 2  batch: 503 / 787  loss: 0.004054981143039996  hr: 0  min: 13  sec: 50\n",
      "epoch: 2  batch: 504 / 787  loss: 0.00404847260566991  hr: 0  min: 13  sec: 47\n",
      "epoch: 2  batch: 505 / 787  loss: 0.004040903290405197  hr: 0  min: 13  sec: 44\n",
      "epoch: 2  batch: 506 / 787  loss: 0.004035222202233681  hr: 0  min: 13  sec: 41\n",
      "epoch: 2  batch: 507 / 787  loss: 0.00402858923749031  hr: 0  min: 13  sec: 39\n",
      "epoch: 2  batch: 508 / 787  loss: 0.004021711768251321  hr: 0  min: 13  sec: 36\n",
      "epoch: 2  batch: 509 / 787  loss: 0.004019632453138003  hr: 0  min: 13  sec: 32\n",
      "epoch: 2  batch: 510 / 787  loss: 0.004019867875995616  hr: 0  min: 13  sec: 29\n",
      "epoch: 2  batch: 511 / 787  loss: 0.0040260219849168125  hr: 0  min: 13  sec: 27\n",
      "epoch: 2  batch: 512 / 787  loss: 0.0040224434562503575  hr: 0  min: 13  sec: 24\n",
      "epoch: 2  batch: 513 / 787  loss: 0.004016170105922987  hr: 0  min: 13  sec: 20\n",
      "epoch: 2  batch: 514 / 787  loss: 0.0040092953367932005  hr: 0  min: 13  sec: 17\n",
      "epoch: 2  batch: 515 / 787  loss: 0.004008848622336179  hr: 0  min: 13  sec: 14\n",
      "epoch: 2  batch: 516 / 787  loss: 0.004001509531558123  hr: 0  min: 13  sec: 11\n",
      "epoch: 2  batch: 517 / 787  loss: 0.003994221808786263  hr: 0  min: 13  sec: 7\n",
      "epoch: 2  batch: 518 / 787  loss: 0.003986904385948462  hr: 0  min: 13  sec: 4\n",
      "epoch: 2  batch: 519 / 787  loss: 0.0039810732663180665  hr: 0  min: 13  sec: 1\n",
      "epoch: 2  batch: 520 / 787  loss: 0.003976000614644503  hr: 0  min: 12  sec: 58\n",
      "epoch: 2  batch: 521 / 787  loss: 0.003997331807165739  hr: 0  min: 12  sec: 55\n",
      "epoch: 2  batch: 522 / 787  loss: 0.0040083219660136055  hr: 0  min: 12  sec: 52\n",
      "epoch: 2  batch: 523 / 787  loss: 0.0040015400091912255  hr: 0  min: 12  sec: 49\n",
      "epoch: 2  batch: 524 / 787  loss: 0.004006107642779879  hr: 0  min: 12  sec: 45\n",
      "epoch: 2  batch: 525 / 787  loss: 0.0040016485189387045  hr: 0  min: 12  sec: 42\n",
      "epoch: 2  batch: 526 / 787  loss: 0.003995065447273921  hr: 0  min: 12  sec: 39\n",
      "epoch: 2  batch: 527 / 787  loss: 0.00398883492843468  hr: 0  min: 12  sec: 36\n",
      "epoch: 2  batch: 528 / 787  loss: 0.003982874775801699  hr: 0  min: 12  sec: 33\n",
      "epoch: 2  batch: 529 / 787  loss: 0.003976518673454112  hr: 0  min: 12  sec: 30\n",
      "epoch: 2  batch: 530 / 787  loss: 0.003971190676658704  hr: 0  min: 12  sec: 27\n",
      "epoch: 2  batch: 531 / 787  loss: 0.003967359615221274  hr: 0  min: 12  sec: 24\n",
      "epoch: 2  batch: 532 / 787  loss: 0.003963085945604388  hr: 0  min: 12  sec: 21\n",
      "epoch: 2  batch: 533 / 787  loss: 0.003956580377291096  hr: 0  min: 12  sec: 18\n",
      "epoch: 2  batch: 534 / 787  loss: 0.0039537247938916265  hr: 0  min: 12  sec: 15\n",
      "epoch: 2  batch: 535 / 787  loss: 0.00394670690603502  hr: 0  min: 12  sec: 12\n",
      "epoch: 2  batch: 536 / 787  loss: 0.003940229836365715  hr: 0  min: 12  sec: 8\n",
      "epoch: 2  batch: 537 / 787  loss: 0.003933625815941795  hr: 0  min: 12  sec: 6\n",
      "epoch: 2  batch: 538 / 787  loss: 0.0039266896075794136  hr: 0  min: 12  sec: 2\n",
      "epoch: 2  batch: 539 / 787  loss: 0.0039206525608499795  hr: 0  min: 12  sec: 0\n",
      "epoch: 2  batch: 540 / 787  loss: 0.00391560246482388  hr: 0  min: 11  sec: 56\n",
      "epoch: 2  batch: 541 / 787  loss: 0.00390887661612603  hr: 0  min: 11  sec: 53\n",
      "epoch: 2  batch: 542 / 787  loss: 0.003908696086362874  hr: 0  min: 11  sec: 50\n",
      "epoch: 2  batch: 543 / 787  loss: 0.003905454147394839  hr: 0  min: 11  sec: 47\n",
      "epoch: 2  batch: 544 / 787  loss: 0.003899541542737365  hr: 0  min: 11  sec: 43\n",
      "epoch: 2  batch: 545 / 787  loss: 0.003892712051571142  hr: 0  min: 11  sec: 41\n",
      "epoch: 2  batch: 546 / 787  loss: 0.00388774312988385  hr: 0  min: 11  sec: 38\n",
      "epoch: 2  batch: 547 / 787  loss: 0.0039041487049574024  hr: 0  min: 11  sec: 35\n",
      "epoch: 2  batch: 548 / 787  loss: 0.003897562802612678  hr: 0  min: 11  sec: 32\n",
      "epoch: 2  batch: 549 / 787  loss: 0.003895323068612401  hr: 0  min: 11  sec: 29\n",
      "epoch: 2  batch: 550 / 787  loss: 0.003902325105128429  hr: 0  min: 11  sec: 27\n",
      "epoch: 2  batch: 551 / 787  loss: 0.003904435082887867  hr: 0  min: 11  sec: 24\n",
      "epoch: 2  batch: 552 / 787  loss: 0.003897598451579182  hr: 0  min: 11  sec: 22\n",
      "epoch: 2  batch: 553 / 787  loss: 0.003892207765418305  hr: 0  min: 11  sec: 19\n",
      "epoch: 2  batch: 554 / 787  loss: 0.0038904438912889514  hr: 0  min: 11  sec: 16\n",
      "epoch: 2  batch: 555 / 787  loss: 0.0038836678959636925  hr: 0  min: 11  sec: 13\n",
      "epoch: 2  batch: 556 / 787  loss: 0.003880222816674144  hr: 0  min: 11  sec: 11\n",
      "epoch: 2  batch: 557 / 787  loss: 0.003900670409226611  hr: 0  min: 11  sec: 8\n",
      "epoch: 2  batch: 558 / 787  loss: 0.0038945660405609954  hr: 0  min: 11  sec: 5\n",
      "epoch: 2  batch: 559 / 787  loss: 0.003889007157805852  hr: 0  min: 11  sec: 2\n",
      "epoch: 2  batch: 560 / 787  loss: 0.0038858233099485265  hr: 0  min: 10  sec: 59\n",
      "epoch: 2  batch: 561 / 787  loss: 0.00387934259404723  hr: 0  min: 10  sec: 56\n",
      "epoch: 2  batch: 562 / 787  loss: 0.0038735446150862493  hr: 0  min: 10  sec: 53\n",
      "epoch: 2  batch: 563 / 787  loss: 0.0038714266719875965  hr: 0  min: 10  sec: 50\n",
      "epoch: 2  batch: 564 / 787  loss: 0.003865959671028132  hr: 0  min: 10  sec: 47\n",
      "epoch: 2  batch: 565 / 787  loss: 0.0038608266848636033  hr: 0  min: 10  sec: 44\n",
      "epoch: 2  batch: 566 / 787  loss: 0.003855348875596601  hr: 0  min: 10  sec: 41\n",
      "epoch: 2  batch: 567 / 787  loss: 0.0038494495883622214  hr: 0  min: 10  sec: 38\n",
      "epoch: 2  batch: 568 / 787  loss: 0.0038431018059218586  hr: 0  min: 10  sec: 36\n",
      "epoch: 2  batch: 569 / 787  loss: 0.003837045920849696  hr: 0  min: 10  sec: 33\n",
      "epoch: 2  batch: 570 / 787  loss: 0.0038315733609539295  hr: 0  min: 10  sec: 30\n",
      "epoch: 2  batch: 571 / 787  loss: 0.0038649634436054656  hr: 0  min: 10  sec: 27\n",
      "epoch: 2  batch: 572 / 787  loss: 0.0038586102506545554  hr: 0  min: 10  sec: 24\n",
      "epoch: 2  batch: 573 / 787  loss: 0.003852121847965933  hr: 0  min: 10  sec: 23\n",
      "epoch: 2  batch: 574 / 787  loss: 0.0038461739996004806  hr: 0  min: 10  sec: 20\n",
      "epoch: 2  batch: 575 / 787  loss: 0.003840279217642408  hr: 0  min: 10  sec: 16\n",
      "epoch: 2  batch: 576 / 787  loss: 0.0038339586881951013  hr: 0  min: 10  sec: 14\n",
      "epoch: 2  batch: 577 / 787  loss: 0.003828091838612363  hr: 0  min: 10  sec: 11\n",
      "epoch: 2  batch: 578 / 787  loss: 0.003821941485084073  hr: 0  min: 10  sec: 8\n",
      "epoch: 2  batch: 579 / 787  loss: 0.0038708620062566385  hr: 0  min: 10  sec: 5\n",
      "epoch: 2  batch: 580 / 787  loss: 0.0038645115035439537  hr: 0  min: 10  sec: 2\n",
      "epoch: 2  batch: 581 / 787  loss: 0.003858144945671811  hr: 0  min: 9  sec: 59\n",
      "epoch: 2  batch: 582 / 787  loss: 0.003852198341372503  hr: 0  min: 9  sec: 56\n",
      "epoch: 2  batch: 583 / 787  loss: 0.0038646231454359382  hr: 0  min: 9  sec: 53\n",
      "epoch: 2  batch: 584 / 787  loss: 0.0038597425040254713  hr: 0  min: 9  sec: 50\n",
      "epoch: 2  batch: 585 / 787  loss: 0.0039021837510144672  hr: 0  min: 9  sec: 47\n",
      "epoch: 2  batch: 586 / 787  loss: 0.0039158077830482125  hr: 0  min: 9  sec: 44\n",
      "epoch: 2  batch: 587 / 787  loss: 0.0039280346306984895  hr: 0  min: 9  sec: 41\n",
      "epoch: 2  batch: 588 / 787  loss: 0.003926531631570012  hr: 0  min: 9  sec: 38\n",
      "epoch: 2  batch: 589 / 787  loss: 0.0039215375767532165  hr: 0  min: 9  sec: 35\n",
      "epoch: 2  batch: 590 / 787  loss: 0.00396996492537733  hr: 0  min: 9  sec: 32\n",
      "epoch: 2  batch: 591 / 787  loss: 0.003964770557031874  hr: 0  min: 9  sec: 29\n",
      "epoch: 2  batch: 592 / 787  loss: 0.003960328707270909  hr: 0  min: 9  sec: 27\n",
      "epoch: 2  batch: 593 / 787  loss: 0.003958584903013024  hr: 0  min: 9  sec: 23\n",
      "epoch: 2  batch: 594 / 787  loss: 0.00399950265099282  hr: 0  min: 9  sec: 21\n",
      "epoch: 2  batch: 595 / 787  loss: 0.003995214936131721  hr: 0  min: 9  sec: 18\n",
      "epoch: 2  batch: 596 / 787  loss: 0.003992179653317443  hr: 0  min: 9  sec: 15\n",
      "epoch: 2  batch: 597 / 787  loss: 0.00398902279854061  hr: 0  min: 9  sec: 12\n",
      "epoch: 2  batch: 598 / 787  loss: 0.003983269924399938  hr: 0  min: 9  sec: 9\n",
      "epoch: 2  batch: 599 / 787  loss: 0.004023926222882698  hr: 0  min: 9  sec: 6\n",
      "epoch: 2  batch: 600 / 787  loss: 0.004027443587838206  hr: 0  min: 9  sec: 3\n",
      "epoch: 2  batch: 601 / 787  loss: 0.004025075339712978  hr: 0  min: 9  sec: 0\n",
      "epoch: 2  batch: 602 / 787  loss: 0.004019244568595706  hr: 0  min: 8  sec: 57\n",
      "epoch: 2  batch: 603 / 787  loss: 0.0040176387314080795  hr: 0  min: 8  sec: 54\n",
      "epoch: 2  batch: 604 / 787  loss: 0.004013086368086199  hr: 0  min: 8  sec: 51\n",
      "epoch: 2  batch: 605 / 787  loss: 0.004008516236708676  hr: 0  min: 8  sec: 48\n",
      "epoch: 2  batch: 606 / 787  loss: 0.004028959994330363  hr: 0  min: 8  sec: 45\n",
      "epoch: 2  batch: 607 / 787  loss: 0.004040557647176099  hr: 0  min: 8  sec: 43\n",
      "epoch: 2  batch: 608 / 787  loss: 0.004035691173301434  hr: 0  min: 8  sec: 40\n",
      "epoch: 2  batch: 609 / 787  loss: 0.0040517167445526065  hr: 0  min: 8  sec: 37\n",
      "epoch: 2  batch: 610 / 787  loss: 0.004051084543356947  hr: 0  min: 8  sec: 34\n",
      "epoch: 2  batch: 611 / 787  loss: 0.004066754179951809  hr: 0  min: 8  sec: 31\n",
      "epoch: 2  batch: 612 / 787  loss: 0.004061531886405896  hr: 0  min: 8  sec: 28\n",
      "epoch: 2  batch: 613 / 787  loss: 0.004059719683987591  hr: 0  min: 8  sec: 25\n",
      "epoch: 2  batch: 614 / 787  loss: 0.004077489041410089  hr: 0  min: 8  sec: 23\n",
      "epoch: 2  batch: 615 / 787  loss: 0.004085236607115655  hr: 0  min: 8  sec: 20\n",
      "epoch: 2  batch: 616 / 787  loss: 0.004079376582177991  hr: 0  min: 8  sec: 17\n",
      "epoch: 2  batch: 617 / 787  loss: 0.004099131015871275  hr: 0  min: 8  sec: 14\n",
      "epoch: 2  batch: 618 / 787  loss: 0.00409419210444952  hr: 0  min: 8  sec: 11\n",
      "epoch: 2  batch: 619 / 787  loss: 0.004088935368263309  hr: 0  min: 8  sec: 8\n",
      "epoch: 2  batch: 620 / 787  loss: 0.004102600759154396  hr: 0  min: 8  sec: 5\n",
      "epoch: 2  batch: 621 / 787  loss: 0.00409657593028044  hr: 0  min: 8  sec: 3\n",
      "epoch: 2  batch: 622 / 787  loss: 0.004101573444032215  hr: 0  min: 7  sec: 59\n",
      "epoch: 2  batch: 623 / 787  loss: 0.00409623880046203  hr: 0  min: 7  sec: 57\n",
      "epoch: 2  batch: 624 / 787  loss: 0.0040907071844953746  hr: 0  min: 7  sec: 54\n",
      "epoch: 2  batch: 625 / 787  loss: 0.004086863614898175  hr: 0  min: 7  sec: 51\n",
      "epoch: 2  batch: 626 / 787  loss: 0.004082271237779534  hr: 0  min: 7  sec: 48\n",
      "epoch: 2  batch: 627 / 787  loss: 0.004077269365298792  hr: 0  min: 7  sec: 45\n",
      "epoch: 2  batch: 628 / 787  loss: 0.004080196196056908  hr: 0  min: 7  sec: 42\n",
      "epoch: 2  batch: 629 / 787  loss: 0.004075388414937032  hr: 0  min: 7  sec: 39\n",
      "epoch: 2  batch: 630 / 787  loss: 0.004073953772396115  hr: 0  min: 7  sec: 36\n",
      "epoch: 2  batch: 631 / 787  loss: 0.004068348740420132  hr: 0  min: 7  sec: 33\n",
      "epoch: 2  batch: 632 / 787  loss: 0.004065103169373691  hr: 0  min: 7  sec: 31\n",
      "epoch: 2  batch: 633 / 787  loss: 0.004059349446629923  hr: 0  min: 7  sec: 28\n",
      "epoch: 2  batch: 634 / 787  loss: 0.004062557848082629  hr: 0  min: 7  sec: 25\n",
      "epoch: 2  batch: 635 / 787  loss: 0.00406056172860691  hr: 0  min: 7  sec: 22\n",
      "epoch: 2  batch: 636 / 787  loss: 0.00405586574508687  hr: 0  min: 7  sec: 19\n",
      "epoch: 2  batch: 637 / 787  loss: 0.004049809355601439  hr: 0  min: 7  sec: 16\n",
      "epoch: 2  batch: 638 / 787  loss: 0.004045745278017991  hr: 0  min: 7  sec: 13\n",
      "epoch: 2  batch: 639 / 787  loss: 0.004042921028589481  hr: 0  min: 7  sec: 10\n",
      "epoch: 2  batch: 640 / 787  loss: 0.004047307493306107  hr: 0  min: 7  sec: 7\n",
      "epoch: 2  batch: 641 / 787  loss: 0.004042677513272691  hr: 0  min: 7  sec: 4\n",
      "epoch: 2  batch: 642 / 787  loss: 0.004037204949776214  hr: 0  min: 7  sec: 1\n",
      "epoch: 2  batch: 643 / 787  loss: 0.00403169638767942  hr: 0  min: 6  sec: 58\n",
      "epoch: 2  batch: 644 / 787  loss: 0.004028222213859159  hr: 0  min: 6  sec: 55\n",
      "epoch: 2  batch: 645 / 787  loss: 0.00402264029535288  hr: 0  min: 6  sec: 53\n",
      "epoch: 2  batch: 646 / 787  loss: 0.0040168146373357715  hr: 0  min: 6  sec: 49\n",
      "epoch: 2  batch: 647 / 787  loss: 0.004033984582549302  hr: 0  min: 6  sec: 47\n",
      "epoch: 2  batch: 648 / 787  loss: 0.004028110493127764  hr: 0  min: 6  sec: 44\n",
      "epoch: 2  batch: 649 / 787  loss: 0.004023002628248669  hr: 0  min: 6  sec: 41\n",
      "epoch: 2  batch: 650 / 787  loss: 0.00401769866264658  hr: 0  min: 6  sec: 38\n",
      "epoch: 2  batch: 651 / 787  loss: 0.0040127320041690095  hr: 0  min: 6  sec: 35\n",
      "epoch: 2  batch: 652 / 787  loss: 0.004007118935474485  hr: 0  min: 6  sec: 32\n",
      "epoch: 2  batch: 653 / 787  loss: 0.004007508210205134  hr: 0  min: 6  sec: 29\n",
      "epoch: 2  batch: 654 / 787  loss: 0.004001702254198225  hr: 0  min: 6  sec: 26\n",
      "epoch: 2  batch: 655 / 787  loss: 0.003997653516525765  hr: 0  min: 6  sec: 23\n",
      "epoch: 2  batch: 656 / 787  loss: 0.00404101949627103  hr: 0  min: 6  sec: 20\n",
      "epoch: 2  batch: 657 / 787  loss: 0.004052342526559559  hr: 0  min: 6  sec: 17\n",
      "epoch: 2  batch: 658 / 787  loss: 0.0040496740975011335  hr: 0  min: 6  sec: 14\n",
      "epoch: 2  batch: 659 / 787  loss: 0.004048350718574937  hr: 0  min: 6  sec: 11\n",
      "epoch: 2  batch: 660 / 787  loss: 0.004043120261617746  hr: 0  min: 6  sec: 8\n",
      "epoch: 2  batch: 661 / 787  loss: 0.004038876348881173  hr: 0  min: 6  sec: 5\n",
      "epoch: 2  batch: 662 / 787  loss: 0.004033266606634614  hr: 0  min: 6  sec: 2\n",
      "epoch: 2  batch: 663 / 787  loss: 0.0040291911061527625  hr: 0  min: 5  sec: 59\n",
      "epoch: 2  batch: 664 / 787  loss: 0.004026464313214837  hr: 0  min: 5  sec: 57\n",
      "epoch: 2  batch: 665 / 787  loss: 0.004021930541073785  hr: 0  min: 5  sec: 54\n",
      "epoch: 2  batch: 666 / 787  loss: 0.004038632562644982  hr: 0  min: 5  sec: 51\n",
      "epoch: 2  batch: 667 / 787  loss: 0.004054413616052141  hr: 0  min: 5  sec: 48\n",
      "epoch: 2  batch: 668 / 787  loss: 0.004049348397377194  hr: 0  min: 5  sec: 45\n",
      "epoch: 2  batch: 669 / 787  loss: 0.004043969678164735  hr: 0  min: 5  sec: 42\n",
      "epoch: 2  batch: 670 / 787  loss: 0.004039018453118815  hr: 0  min: 5  sec: 39\n",
      "epoch: 2  batch: 671 / 787  loss: 0.0040338766879231035  hr: 0  min: 5  sec: 36\n",
      "epoch: 2  batch: 672 / 787  loss: 0.004029890616406885  hr: 0  min: 5  sec: 33\n",
      "epoch: 2  batch: 673 / 787  loss: 0.004024407800352222  hr: 0  min: 5  sec: 30\n",
      "epoch: 2  batch: 674 / 787  loss: 0.004018872023066292  hr: 0  min: 5  sec: 27\n",
      "epoch: 2  batch: 675 / 787  loss: 0.00401339863485191  hr: 0  min: 5  sec: 24\n",
      "epoch: 2  batch: 676 / 787  loss: 0.004007994634361583  hr: 0  min: 5  sec: 21\n",
      "epoch: 2  batch: 677 / 787  loss: 0.0040044629235263226  hr: 0  min: 5  sec: 18\n",
      "epoch: 2  batch: 678 / 787  loss: 0.003999938885024563  hr: 0  min: 5  sec: 15\n",
      "epoch: 2  batch: 679 / 787  loss: 0.004029593326452954  hr: 0  min: 5  sec: 12\n",
      "epoch: 2  batch: 680 / 787  loss: 0.004024588142548215  hr: 0  min: 5  sec: 9\n",
      "epoch: 2  batch: 681 / 787  loss: 0.004019481863715568  hr: 0  min: 5  sec: 6\n",
      "epoch: 2  batch: 682 / 787  loss: 0.004061622585471592  hr: 0  min: 5  sec: 3\n",
      "epoch: 2  batch: 683 / 787  loss: 0.004057191970132088  hr: 0  min: 5  sec: 0\n",
      "epoch: 2  batch: 684 / 787  loss: 0.004052169539529813  hr: 0  min: 4  sec: 57\n",
      "epoch: 2  batch: 685 / 787  loss: 0.004046721577905221  hr: 0  min: 4  sec: 55\n",
      "epoch: 2  batch: 686 / 787  loss: 0.0040503413705235015  hr: 0  min: 4  sec: 52\n",
      "epoch: 2  batch: 687 / 787  loss: 0.0040452058607801525  hr: 0  min: 4  sec: 49\n",
      "epoch: 2  batch: 688 / 787  loss: 0.004039794971842724  hr: 0  min: 4  sec: 46\n",
      "epoch: 2  batch: 689 / 787  loss: 0.004035033696936478  hr: 0  min: 4  sec: 43\n",
      "epoch: 2  batch: 690 / 787  loss: 0.004047342193160253  hr: 0  min: 4  sec: 40\n",
      "epoch: 2  batch: 691 / 787  loss: 0.004043098013876202  hr: 0  min: 4  sec: 37\n",
      "epoch: 2  batch: 692 / 787  loss: 0.004038512796149933  hr: 0  min: 4  sec: 34\n",
      "epoch: 2  batch: 693 / 787  loss: 0.004032974528506751  hr: 0  min: 4  sec: 32\n",
      "epoch: 2  batch: 694 / 787  loss: 0.004029915608499724  hr: 0  min: 4  sec: 29\n",
      "epoch: 2  batch: 695 / 787  loss: 0.004024756317744668  hr: 0  min: 4  sec: 26\n",
      "epoch: 2  batch: 696 / 787  loss: 0.004021308033268975  hr: 0  min: 4  sec: 23\n",
      "epoch: 2  batch: 697 / 787  loss: 0.004115787051823371  hr: 0  min: 4  sec: 20\n",
      "epoch: 2  batch: 698 / 787  loss: 0.004110261493669869  hr: 0  min: 4  sec: 17\n",
      "epoch: 2  batch: 699 / 787  loss: 0.004124229741881492  hr: 0  min: 4  sec: 14\n",
      "epoch: 2  batch: 700 / 787  loss: 0.004119478098727996  hr: 0  min: 4  sec: 11\n",
      "epoch: 2  batch: 701 / 787  loss: 0.004114426117098609  hr: 0  min: 4  sec: 8\n",
      "epoch: 2  batch: 702 / 787  loss: 0.004123614139123083  hr: 0  min: 4  sec: 5\n",
      "epoch: 2  batch: 703 / 787  loss: 0.0041180514076774  hr: 0  min: 4  sec: 3\n",
      "epoch: 2  batch: 704 / 787  loss: 0.004113505814220017  hr: 0  min: 4  sec: 0\n",
      "epoch: 2  batch: 705 / 787  loss: 0.004119101881585414  hr: 0  min: 3  sec: 57\n",
      "epoch: 2  batch: 706 / 787  loss: 0.004113730640563996  hr: 0  min: 3  sec: 54\n",
      "epoch: 2  batch: 707 / 787  loss: 0.004109582255305123  hr: 0  min: 3  sec: 51\n",
      "epoch: 2  batch: 708 / 787  loss: 0.004105440757906105  hr: 0  min: 3  sec: 48\n",
      "epoch: 2  batch: 709 / 787  loss: 0.004100425148983578  hr: 0  min: 3  sec: 45\n",
      "epoch: 2  batch: 710 / 787  loss: 0.004135295534715496  hr: 0  min: 3  sec: 42\n",
      "epoch: 2  batch: 711 / 787  loss: 0.004133511281423447  hr: 0  min: 3  sec: 39\n",
      "epoch: 2  batch: 712 / 787  loss: 0.00414529974158939  hr: 0  min: 3  sec: 36\n",
      "epoch: 2  batch: 713 / 787  loss: 0.00418334029829061  hr: 0  min: 3  sec: 33\n",
      "epoch: 2  batch: 714 / 787  loss: 0.004186745635011261  hr: 0  min: 3  sec: 31\n",
      "epoch: 2  batch: 715 / 787  loss: 0.004182033236134452  hr: 0  min: 3  sec: 28\n",
      "epoch: 2  batch: 716 / 787  loss: 0.004180336288366221  hr: 0  min: 3  sec: 25\n",
      "epoch: 2  batch: 717 / 787  loss: 0.0041780868937731286  hr: 0  min: 3  sec: 22\n",
      "epoch: 2  batch: 718 / 787  loss: 0.004199552023422568  hr: 0  min: 3  sec: 19\n",
      "epoch: 2  batch: 719 / 787  loss: 0.004194240588297315  hr: 0  min: 3  sec: 16\n",
      "epoch: 2  batch: 720 / 787  loss: 0.004195843136343077  hr: 0  min: 3  sec: 13\n",
      "epoch: 2  batch: 721 / 787  loss: 0.00419179851218077  hr: 0  min: 3  sec: 10\n",
      "epoch: 2  batch: 722 / 787  loss: 0.004189929918717713  hr: 0  min: 3  sec: 8\n",
      "epoch: 2  batch: 723 / 787  loss: 0.004185666676445988  hr: 0  min: 3  sec: 5\n",
      "epoch: 2  batch: 724 / 787  loss: 0.0041824229310553854  hr: 0  min: 3  sec: 2\n",
      "epoch: 2  batch: 725 / 787  loss: 0.0043090892210000615  hr: 0  min: 2  sec: 59\n",
      "epoch: 2  batch: 726 / 787  loss: 0.004311568884903946  hr: 0  min: 2  sec: 56\n",
      "epoch: 2  batch: 727 / 787  loss: 0.0043103249318312605  hr: 0  min: 2  sec: 53\n",
      "epoch: 2  batch: 728 / 787  loss: 0.004306709005694566  hr: 0  min: 2  sec: 50\n",
      "epoch: 2  batch: 729 / 787  loss: 0.004307161721619534  hr: 0  min: 2  sec: 47\n",
      "epoch: 2  batch: 730 / 787  loss: 0.004319168317346189  hr: 0  min: 2  sec: 44\n",
      "epoch: 2  batch: 731 / 787  loss: 0.004374694890823125  hr: 0  min: 2  sec: 41\n",
      "epoch: 2  batch: 732 / 787  loss: 0.004370624299325422  hr: 0  min: 2  sec: 38\n",
      "epoch: 2  batch: 733 / 787  loss: 0.004368836416182042  hr: 0  min: 2  sec: 36\n",
      "epoch: 2  batch: 734 / 787  loss: 0.00436403878294937  hr: 0  min: 2  sec: 33\n",
      "epoch: 2  batch: 735 / 787  loss: 0.004371567248769871  hr: 0  min: 2  sec: 30\n",
      "epoch: 2  batch: 736 / 787  loss: 0.004398795268274546  hr: 0  min: 2  sec: 27\n",
      "epoch: 2  batch: 737 / 787  loss: 0.0043963044893865  hr: 0  min: 2  sec: 24\n",
      "epoch: 2  batch: 738 / 787  loss: 0.0044156690758533415  hr: 0  min: 2  sec: 21\n",
      "epoch: 2  batch: 739 / 787  loss: 0.004411555128112966  hr: 0  min: 2  sec: 18\n",
      "epoch: 2  batch: 740 / 787  loss: 0.004410951572877034  hr: 0  min: 2  sec: 15\n",
      "epoch: 2  batch: 741 / 787  loss: 0.004408289918707597  hr: 0  min: 2  sec: 12\n",
      "epoch: 2  batch: 742 / 787  loss: 0.004432574768286014  hr: 0  min: 2  sec: 9\n",
      "epoch: 2  batch: 743 / 787  loss: 0.004433217711254238  hr: 0  min: 2  sec: 7\n",
      "epoch: 2  batch: 744 / 787  loss: 0.004506955792885981  hr: 0  min: 2  sec: 4\n",
      "epoch: 2  batch: 745 / 787  loss: 0.0045048300041887444  hr: 0  min: 2  sec: 1\n",
      "epoch: 2  batch: 746 / 787  loss: 0.004499220630557722  hr: 0  min: 1  sec: 58\n",
      "epoch: 2  batch: 747 / 787  loss: 0.004494898478447402  hr: 0  min: 1  sec: 55\n",
      "epoch: 2  batch: 748 / 787  loss: 0.004515108202606966  hr: 0  min: 1  sec: 52\n",
      "epoch: 2  batch: 749 / 787  loss: 0.004526732998939128  hr: 0  min: 1  sec: 49\n",
      "epoch: 2  batch: 750 / 787  loss: 0.004523757042343883  hr: 0  min: 1  sec: 46\n",
      "epoch: 2  batch: 751 / 787  loss: 0.004573986685124707  hr: 0  min: 1  sec: 43\n",
      "epoch: 2  batch: 752 / 787  loss: 0.004571449609411889  hr: 0  min: 1  sec: 40\n",
      "epoch: 2  batch: 753 / 787  loss: 0.004567052209378  hr: 0  min: 1  sec: 38\n",
      "epoch: 2  batch: 754 / 787  loss: 0.0045629035277081845  hr: 0  min: 1  sec: 35\n",
      "epoch: 2  batch: 755 / 787  loss: 0.004557960652896598  hr: 0  min: 1  sec: 32\n",
      "epoch: 2  batch: 756 / 787  loss: 0.004566604393239782  hr: 0  min: 1  sec: 29\n",
      "epoch: 2  batch: 757 / 787  loss: 0.004564476433566703  hr: 0  min: 1  sec: 26\n",
      "epoch: 2  batch: 758 / 787  loss: 0.00457015826764543  hr: 0  min: 1  sec: 23\n",
      "epoch: 2  batch: 759 / 787  loss: 0.00456707617083527  hr: 0  min: 1  sec: 20\n",
      "epoch: 2  batch: 760 / 787  loss: 0.004561563801900392  hr: 0  min: 1  sec: 17\n",
      "epoch: 2  batch: 761 / 787  loss: 0.004560894095501801  hr: 0  min: 1  sec: 14\n",
      "epoch: 2  batch: 762 / 787  loss: 0.004557055370700197  hr: 0  min: 1  sec: 12\n",
      "epoch: 2  batch: 763 / 787  loss: 0.004552148472351269  hr: 0  min: 1  sec: 9\n",
      "epoch: 2  batch: 764 / 787  loss: 0.004546963165521637  hr: 0  min: 1  sec: 6\n",
      "epoch: 2  batch: 765 / 787  loss: 0.00454212020524531  hr: 0  min: 1  sec: 3\n",
      "epoch: 2  batch: 766 / 787  loss: 0.004537249039318297  hr: 0  min: 1  sec: 0\n",
      "epoch: 2  batch: 767 / 787  loss: 0.004532134082934678  hr: 0  min: 0  sec: 57\n",
      "epoch: 2  batch: 768 / 787  loss: 0.0045268539986219975  hr: 0  min: 0  sec: 54\n",
      "epoch: 2  batch: 769 / 787  loss: 0.004522333529190298  hr: 0  min: 0  sec: 51\n",
      "epoch: 2  batch: 770 / 787  loss: 0.004516853613634564  hr: 0  min: 0  sec: 49\n",
      "epoch: 2  batch: 771 / 787  loss: 0.004529339712084906  hr: 0  min: 0  sec: 46\n",
      "epoch: 2  batch: 772 / 787  loss: 0.004538370636726852  hr: 0  min: 0  sec: 43\n",
      "epoch: 2  batch: 773 / 787  loss: 0.004535905824526741  hr: 0  min: 0  sec: 40\n",
      "epoch: 2  batch: 774 / 787  loss: 0.004551739280777039  hr: 0  min: 0  sec: 37\n",
      "epoch: 2  batch: 775 / 787  loss: 0.004546478584863155  hr: 0  min: 0  sec: 34\n",
      "epoch: 2  batch: 776 / 787  loss: 0.004542188882680289  hr: 0  min: 0  sec: 31\n",
      "epoch: 2  batch: 777 / 787  loss: 0.0045461387971053235  hr: 0  min: 0  sec: 28\n",
      "epoch: 2  batch: 778 / 787  loss: 0.00457837019387796  hr: 0  min: 0  sec: 25\n",
      "epoch: 2  batch: 779 / 787  loss: 0.004578590898124342  hr: 0  min: 0  sec: 23\n",
      "epoch: 2  batch: 780 / 787  loss: 0.004573776953997693  hr: 0  min: 0  sec: 20\n",
      "epoch: 2  batch: 781 / 787  loss: 0.004599714416954052  hr: 0  min: 0  sec: 17\n",
      "epoch: 2  batch: 782 / 787  loss: 0.00459674313351291  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 783 / 787  loss: 0.004591434421723246  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 784 / 787  loss: 0.0045860753313848234  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 785 / 787  loss: 0.00459395489233893  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 786 / 787  loss: 0.004594708779590819  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 787 / 787  loss: 0.004596433640493044  hr: 0  min: 0  sec: 0\n",
      "Wall time: 1h 51min 53s\n"
     ]
    }
   ],
   "source": [
    "%time train_model_ATE(train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ATE = load_model(model_ATE, 'bert_ATE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 4s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     78615\n",
      "           1       0.81      0.81      0.81      4146\n",
      "           2       0.81      0.76      0.78      2083\n",
      "\n",
      "    accuracy                           0.97     84844\n",
      "   macro avg       0.87      0.85      0.86     84844\n",
      "weighted avg       0.97      0.97      0.97     84844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ATE(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (4.30.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: datasets in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (2.13.2)\n",
      "Requirement already satisfied: filelock in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: importlib-metadata in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: xxhash in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: colorama in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\fyp_a+\\project_update\\ateabsa\\.venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Aspect Term Extraction (ATE) model\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    # ... (your existing ATE prediction code)\n",
    "    # Replace this with your implementation for ATE prediction\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "# Load emotion detection model\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\n",
    "\n",
    "# Function to combine ATE and emotion prediction\n",
    "def ATE_emotion_prediction(text, model_ATE, tokenizer, emotion_classifier):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    \n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            if len(word) != 0:\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "    \n",
    "    if len(word) != 0:\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "            \n",
    "    print(\"Tokens:\", x)\n",
    "    print(\"ATE:\", terms)\n",
    "    print(type(terms))\n",
    "    \n",
    "    if len(terms) != 0:\n",
    "        for aspect in terms:\n",
    "            aspect_text = \" \".join(aspect)\n",
    "            print(aspect_text)\n",
    "            prediction = emotion_classifier(text)\n",
    "            \n",
    "            print(\"Aspect:\", aspect, \"Predicted Emotions:\")\n",
    "            print(prediction)\n",
    "            # for emotion in prediction[0]:\n",
    "            #     print(f\"  - {emotion['label']}: {emotion['score']}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Aspect Term Extraction (ATE) model\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    # ... (your existing ATE prediction code)\n",
    "    # Replace this with your implementation for ATE prediction\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "# Load emotion detection model\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\n",
    "\n",
    "def ATE_emotion_prediction(text, model_ATE, tokenizer, emotion_classifier, window_size_ratio=0.2):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    \n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            if len(word) != 0:\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "    \n",
    "    if len(word) != 0:\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "            \n",
    "    print(\"Tokens:\", x)\n",
    "    print(\"ATE:\", terms)\n",
    "    \n",
    "    if len(terms) != 0:\n",
    "        for aspect in terms:\n",
    "            aspect_text = \" \".join(aspect)\n",
    "            print(\"Aspect:\", aspect, \"Predicted Emotions:\")\n",
    "            \n",
    "            # Find the position of the aspect term in the tokenized sentence\n",
    "            aspect_position = find_aspect_position(aspect.split(), x)\n",
    "            \n",
    "            if aspect_position != -1:\n",
    "                # Calculate the dynamic window size as a fraction of the sentence length\n",
    "                sentence_length = len(x)\n",
    "                dynamic_window_size = max(1, int(sentence_length * window_size_ratio))\n",
    "                \n",
    "                # Extract the context around the aspect term\n",
    "                start_context = max(0, aspect_position - dynamic_window_size)\n",
    "                end_context = min(len(x), aspect_position + dynamic_window_size + len(aspect))\n",
    "                context_tokens = x[start_context:end_context]\n",
    "                print(start_context)\n",
    "                print(end_context)\n",
    "                print(context_tokens)\n",
    "                # Convert context tokens back to text\n",
    "                context_text = tokenizer.convert_tokens_to_string(context_tokens)\n",
    "                \n",
    "                # Predict emotions based on the context\n",
    "                prediction = emotion_classifier(context_text)\n",
    "                \n",
    "                # Print the raw prediction\n",
    "                print(prediction)\n",
    "            \n",
    "            print()\n",
    "\n",
    "def find_aspect_position(aspect, tokens):\n",
    "    aspect_len = len(aspect)\n",
    "    for i in range(len(tokens) - aspect_len + 1):\n",
    "        if tokens[i:i + aspect_len] == aspect:\n",
    "            return i\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'food', 'is', 'delicious', 'but', 'the', 'parking', 'is', 'hard', 'to', 'find']\n",
      "ATE: ['food', 'parking']\n",
      "Aspect: f o o d\n",
      "Aspect: p a r k i n g\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Function to combine ATE and emotion prediction without ABSA tokenizer\n",
    "def ATE_emotion_prediction(text, model_ATE, tokenizer, emotion_classifier):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    \n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "    \n",
    "    # Initialize variables to store sentences corresponding to each aspect\n",
    "    aspect_sentences = {}\n",
    "    current_aspect = []  # Initialize current_aspect here\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            if len(word) != 0:\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            word = x[i]\n",
    "        elif y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "        elif current_aspect:  # Check if current_aspect is not empty\n",
    "            aspect_text = \" \".join(current_aspect)\n",
    "            \n",
    "            # Store the sentence corresponding to the current aspect\n",
    "            if aspect_text not in aspect_sentences:\n",
    "                aspect_sentences[aspect_text] = []\n",
    "            aspect_sentences[aspect_text].append(word)  # Append word instead of the full text\n",
    "            \n",
    "            current_aspect = []\n",
    "\n",
    "    if len(word) != 0:\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "            \n",
    "    print(\"Tokens:\", x)\n",
    "    print(\"ATE:\", terms)\n",
    "    \n",
    "    if len(terms) != 0:\n",
    "        for aspect in terms:\n",
    "            aspect_text = \" \".join(aspect)\n",
    "            print(\"Aspect:\", aspect_text)\n",
    "            \n",
    "            # Emotion prediction for each sentence corresponding to the aspect\n",
    "            sentences_for_aspect = aspect_sentences.get(aspect_text, [])\n",
    "            for sentence in sentences_for_aspect:\n",
    "                print(\"Sentence for Emotion Prediction:\", sentence)\n",
    "                \n",
    "                # Emotion prediction for the sentence\n",
    "                emotion_prediction = emotion_classifier(sentence)\n",
    "                print(\"Predicted Emotions for the sentence:\")\n",
    "                print(emotion_prediction)\n",
    "                print()\n",
    "\n",
    "# Example usage\n",
    "text = \"The food is delicious but the parking is hard to find\"\n",
    "ATE_emotion_prediction(text, model_ATE, tokenizer, classifier_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'food', 'is', 'delicious', '.', 'the', 'parking', 'is', 'hard', 'to', 'find']\n",
      "ATE: ['food', 'parking']\n",
      "<class 'list'>\n",
      "f o o d\n",
      "Aspect: food Predicted Emotions:\n",
      "[[{'label': 'sadness', 'score': 0.0002427650906611234}, {'label': 'joy', 'score': 0.9982357025146484}, {'label': 'love', 'score': 0.0009890911169350147}, {'label': 'anger', 'score': 0.0001619469840079546}, {'label': 'fear', 'score': 0.00012241268996149302}, {'label': 'surprise', 'score': 0.0002479747054167092}]]\n",
      "\n",
      "p a r k i n g\n",
      "Aspect: parking Predicted Emotions:\n",
      "[[{'label': 'sadness', 'score': 0.0002427650906611234}, {'label': 'joy', 'score': 0.9982357025146484}, {'label': 'love', 'score': 0.0009890911169350147}, {'label': 'anger', 'score': 0.0001619469840079546}, {'label': 'fear', 'score': 0.00012241268996149302}, {'label': 'surprise', 'score': 0.0002479747054167092}]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"The food is delicious.The parking is hard to find\"\n",
    "ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food': ['I love the food.'], 'staff': ['The staff is rude.'], 'parking': ['The parking is hard to find']}\n",
      "food sentences:\n",
      "- I love the food.\n",
      "\n",
      "staff sentences:\n",
      "- The staff is rude.\n",
      "\n",
      "parking sentences:\n",
      "- The parking is hard to find\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def extract_aspect_sentences(text, aspects):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    aspect_sentences = {aspect: [] for aspect in aspects}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        for aspect in aspects:\n",
    "            aspect_keywords = [word.lower() for word in aspect.split()]\n",
    "\n",
    "            if any(keyword in [word.lower() for word, pos in pos_tags] for keyword in aspect_keywords):\n",
    "                aspect_sentences[aspect].append(sentence)\n",
    "\n",
    "    return aspect_sentences\n",
    "\n",
    "# Example usage:\n",
    "text = \"I love the food. The staff is rude. The parking is hard to find\"\n",
    "aspects_to_extract = [\"food\", \"staff\",\"parking\"]\n",
    "\n",
    "result = extract_aspect_sentences(text, aspects_to_extract)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "d:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_aspect_sentences(text, aspects):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    aspect_sentences = {aspect: [] for aspect in aspects}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        for aspect in aspects:\n",
    "            aspect_keywords = [word.lower() for word in aspect.split()]\n",
    "\n",
    "            if any(keyword in [word.lower() for word, pos in pos_tags] for keyword in aspect_keywords):\n",
    "                aspect_sentences[aspect].append(sentence)\n",
    "\n",
    "    return aspect_sentences\n",
    "\n",
    "\n",
    "# Load Aspect Term Extraction (ATE) model\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    # ... (your existing ATE prediction code)\n",
    "    # Replace this with your implementation for ATE prediction\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "# Load emotion detection model\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\n",
    "\n",
    "# Function to combine ATE and emotion prediction\n",
    "def ATE_emotion_prediction(text, model_ATE, tokenizer, emotion_classifier):\n",
    "\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "\n",
    "    # Assuming that predict_model_ATE returns the tokens x, labels y, and other information (_)\n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            # If the label is 1, it indicates the start of a new aspect term\n",
    "            if len(word) != 0:\n",
    "                # If there's an existing word, append it to the terms list after removing \" ##\"\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            # Start a new word with the current token\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "\n",
    "    if len(word) != 0:\n",
    "        # print(terms)\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "\n",
    "    combined_terms = []\n",
    "    for term in terms:\n",
    "        if \"#\" in term:\n",
    "            if combined_terms:\n",
    "                combined_terms[-1] += term\n",
    "        else:\n",
    "            combined_terms.append(term)\n",
    "\n",
    "    # Remove hashtags from the combined terms\n",
    "    terms_without_hashtag = [term.replace(\"#\", \"\") for term in combined_terms]\n",
    "    # Replace specific words with desired format\n",
    "    for i in range(len(terms_without_hashtag)):\n",
    "        if  terms_without_hashtag[i] == 'nasilemak':\n",
    "            terms_without_hashtag[i] = 'nasi lemak'\n",
    "           \n",
    "\n",
    "\n",
    "    # Print the original tokens and the extracted aspect terms\n",
    "    print(\"Tokens:\", x)\n",
    "    print(\"ATE:\", terms_without_hashtag)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    result = extract_aspect_sentences(text, terms_without_hashtag)\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "    for aspect, sentences in result.items():\n",
    "        print(f\"{aspect} sentences:\")\n",
    "        \n",
    "        # Loop over each sentence for the current aspect\n",
    "        for sentence in sentences:\n",
    "            print(f\"- {sentence}\")\n",
    "            \n",
    "            # Assuming you have an emotion_classifier function\n",
    "            emotion_result = emotion_classifier(sentence)\n",
    "            print(type(emotion_result))\n",
    "            print(f\"Emotion result for '{aspect}': {emotion_result}\")\n",
    "    \n",
    "        print()  # Print a new line after all sentences for the current aspect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'queue', 'is', 'very', 'long', 'but', 'worth', 'it', '.', 'don', \"'\", 't', 'talk', 'about', 'car', 'parking', ';', 'it', \"'\", 's', 'crowded', '.', 'it', \"'\", 's', 'better', 'to', 'take', 'the', 'l', '##rt', ',', 'stop', 'near', 'kam', '##pu', '##ng', 'b', '##har', '##u', ',', 'and', 'then', 'walk', 'around', '3', 'minutes', ';', 'the', 'distance', 'is', 'short', '.', 'their', 'system', 'is', 'very', 'efficient', ';', 'you', 'pay', 'first', 'before', 'eating', '.']\n",
      "ATE: ['queue', 'parking', 'system']\n",
      "\n",
      "<class 'dict'>\n",
      "{'queue': ['The queue is very long but worth it.'], 'parking': [\"Don't talk about car parking; it's crowded.\"], 'system': ['Their system is very efficient; you pay first before eating.']}\n",
      "queue sentences:\n",
      "- The queue is very long but worth it.\n",
      "<class 'list'>\n",
      "Emotion result for 'queue': [[{'label': 'sadness', 'score': 0.0016604585107415915}, {'label': 'joy', 'score': 0.992912232875824}, {'label': 'love', 'score': 0.0012597484746947885}, {'label': 'anger', 'score': 0.0032829458359628916}, {'label': 'fear', 'score': 0.0003869585052598268}, {'label': 'surprise', 'score': 0.0004976686323061585}]]\n",
      "\n",
      "parking sentences:\n",
      "- Don't talk about car parking; it's crowded.\n",
      "<class 'list'>\n",
      "Emotion result for 'parking': [[{'label': 'sadness', 'score': 0.005228467285633087}, {'label': 'joy', 'score': 0.004873242229223251}, {'label': 'love', 'score': 0.0009705265401862562}, {'label': 'anger', 'score': 0.9601970911026001}, {'label': 'fear', 'score': 0.02750849351286888}, {'label': 'surprise', 'score': 0.0012221024371683598}]]\n",
      "\n",
      "system sentences:\n",
      "- Their system is very efficient; you pay first before eating.\n",
      "<class 'list'>\n",
      "Emotion result for 'system': [[{'label': 'sadness', 'score': 0.0016750189242884517}, {'label': 'joy', 'score': 0.9961572289466858}, {'label': 'love', 'score': 0.0005191961536183953}, {'label': 'anger', 'score': 0.000940426136367023}, {'label': 'fear', 'score': 0.00039362310781143606}, {'label': 'surprise', 'score': 0.0003146238159388304}]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"The queue is very long but worth it. Don't talk about car parking; it's crowded. It's better to take the LRT, stop near Kampung Bharu, and then walk around 3 minutes; the distance is short. Their system is very efficient; you pay first before eating.\"\n",
    "\n",
    "ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['when', 'it', 'arrives', 'around', 'before', 'dusk', ',', 'the', 'line', 'is', 'not', 'long', '.', 'but', 'here', 'is', 'a', 'prayer', 'room', ',', 'don', \"'\", 't', 'worry', '.']\n",
      "ATE: ['line', 'prayer room']\n",
      "\n",
      "line sentences:\n",
      "- When it arrives around before dusk, the line is not long.\n",
      "Emotion result for 'line': [[{'label': 'sadness', 'score': 0.008317738771438599}, {'label': 'joy', 'score': 0.01027082558721304}, {'label': 'love', 'score': 0.0015612308634445071}, {'label': 'anger', 'score': 0.04784228280186653}, {'label': 'fear', 'score': 0.9286301732063293}, {'label': 'surprise', 'score': 0.0033777335193008184}]]\n",
      "\n",
      "prayer room sentences:\n",
      "- But here is a prayer room, don't worry.\n",
      "Emotion result for 'prayer room': [[{'label': 'sadness', 'score': 0.009689624421298504}, {'label': 'joy', 'score': 0.8718135356903076}, {'label': 'love', 'score': 0.0990135595202446}, {'label': 'anger', 'score': 0.01375632919371128}, {'label': 'fear', 'score': 0.0045320275239646435}, {'label': 'surprise', 'score': 0.0011948839528486133}]]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23560\\3644554190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CSV_Review.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0moutput_excel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CSV_ReviewDashboard.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprocess_csv_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_excel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23560\\3644554190.py\u001b[0m in \u001b[0;36mprocess_csv_file\u001b[1;34m(input_file_path, output_file_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# Perform ATE and emotion prediction for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mATE_emotion_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_ATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Append the results to the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read CSV file and process each row\n",
    "def process_csv_file(input_file_path, output_file_path):\n",
    "    # Create an empty DataFrame to store the results\n",
    "    results_df = pd.DataFrame(columns=['Text', 'Aspect', 'Emotion'])\n",
    "\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            \n",
    "            # Perform ATE and emotion prediction for each row\n",
    "            terms, result = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df = results_df.append({'Text': text, 'Aspect': terms, 'Emotion': result}, ignore_index=True)\n",
    "\n",
    "    # Write the results to an Excel file\n",
    "    results_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "csv_file_path = 'CSV_Review.csv' \n",
    "output_excel_path = 'CSV_ReviewDashboard.csv'  \n",
    "process_csv_file(csv_file_path, output_excel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "d:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\n",
    "\n",
    "def extract_aspect_sentences(text, aspects):\n",
    "    sentences = sent_tokenize(text)\n",
    "    aspect_sentences = {aspect: [] for aspect in aspects}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        for aspect in aspects:\n",
    "            aspect_keywords = [word.lower() for word in aspect.split()]\n",
    "\n",
    "            if any(keyword in [word.lower() for word, pos in pos_tags] for keyword in aspect_keywords):\n",
    "                aspect_sentences[aspect].append(sentence)\n",
    "\n",
    "    return aspect_sentences\n",
    "\n",
    "\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "\n",
    "def ATE_emotion_prediction(text, model_ATE, tokenizer, emotion_classifier):\n",
    "\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "\n",
    "    # Assuming that predict_model_ATE returns the tokens x, labels y, and other information (_)\n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            # If the label is 1, it indicates the start of a new aspect term\n",
    "            if len(word) != 0:\n",
    "                # If there's an existing word, append it to the terms list after removing \" ##\"\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            # Start a new word with the current token\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "\n",
    "    if len(word) != 0:\n",
    "        # print(terms)\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "\n",
    "    combined_terms = []\n",
    "    for term in terms:\n",
    "        if \"#\" in term:\n",
    "            if combined_terms:\n",
    "                combined_terms[-1] += term\n",
    "        else:\n",
    "            combined_terms.append(term)\n",
    "\n",
    "    # Remove hashtags from the combined terms\n",
    "    terms_without_hashtag = [term.replace(\"#\", \"\") for term in combined_terms]\n",
    "    # Replace specific words with desired format\n",
    "    for i in range(len(terms_without_hashtag)):\n",
    "        if  terms_without_hashtag[i] == 'nasilemak':\n",
    "            terms_without_hashtag[i] = 'nasi lemak'\n",
    "           \n",
    "\n",
    "\n",
    "    # Print the original tokens and the extracted aspect terms\n",
    "    # print(\"Tokens:\", x)\n",
    "    print(\"ATE:\", terms_without_hashtag)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    result = extract_aspect_sentences(text, terms_without_hashtag)\n",
    "    emotion_dict = {}\n",
    "    emotion_dict = copy.deepcopy(result)\n",
    "    \n",
    "    # print(type(result))\n",
    "    print(f\"Result : {result}\")\n",
    "    for aspect, sentences in result.items():\n",
    "        # print(f\"{aspect} sentences:\")\n",
    "        # print(\"iteration aspect sentence\")\n",
    "        # Loop over each sentence for the current aspect\n",
    "        for sentence in sentences:\n",
    "            # print(f\"- {sentence}\")\n",
    "            \n",
    "            # Assuming you have an emotion_classifier function\n",
    "            emotion_result = emotion_classifier(sentence)\n",
    "            emotion_dict[aspect]+= emotion_result\n",
    "            \n",
    "\n",
    "            # print(emotion_result)\n",
    "            # print(print(emotion_dict))\n",
    "     \n",
    "        print()  # Print a new line after all sentences for the current aspect\n",
    "    \n",
    "    return terms_without_hashtag, emotion_dict\n",
    "\n",
    "\n",
    "def process_csv_file(input_file_path, output_file_path):\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Aspect', 'Emotion'])\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            \n",
    "            # Perform ATE and emotion prediction for each row\n",
    "            terms, result = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df = results_df.append({'Text': text, 'Aspect': terms, 'Emotion': result}, ignore_index=True)\n",
    "\n",
    "    # Write the results to an Excel file\n",
    "    results_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When it arrives around before dusk, the line is not long. But here is a prayer room, don't worry.\n",
      "ATE: ['line', 'prayer room']\n",
      "\n",
      "Result : {'line': ['When it arrives around before dusk, the line is not long.'], 'prayer room': [\"But here is a prayer room, don't worry.\"]}\n",
      "\n",
      "\n",
      "Hola :{'line': ['When it arrives around before dusk, the line is not long.', [{'label': 'sadness', 'score': 0.008317738771438599}, {'label': 'joy', 'score': 0.01027082558721304}, {'label': 'love', 'score': 0.0015612308634445071}, {'label': 'anger', 'score': 0.04784228280186653}, {'label': 'fear', 'score': 0.9286301732063293}, {'label': 'surprise', 'score': 0.0033777335193008184}]], 'prayer room': [\"But here is a prayer room, don't worry.\", [{'label': 'sadness', 'score': 0.009689624421298504}, {'label': 'joy', 'score': 0.8718135356903076}, {'label': 'love', 'score': 0.0990135595202446}, {'label': 'anger', 'score': 0.01375632919371128}, {'label': 'fear', 'score': 0.0045320275239646435}, {'label': 'surprise', 'score': 0.0011948839528486133}]]}\n",
      "Fast-moving lane and self service. Spacious dining area, can pay extra for AC space. Food is above average. Took chicken, cockles, squid with a syrup drink is RM 22. You can get free filter water at the hand wash.\n",
      "ATE: ['dining area', 'food', 'filter water']\n",
      "\n",
      "Result : {'dining area': ['Spacious dining area, can pay extra for AC space.'], 'food': ['Food is above average.'], 'filter water': ['You can get free filter water at the hand wash.']}\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'dining area': ['Spacious dining area, can pay extra for AC space.', [{'label': 'sadness', 'score': 0.0006867002812214196}, {'label': 'joy', 'score': 0.9953076243400574}, {'label': 'love', 'score': 0.0017599303973838687}, {'label': 'anger', 'score': 0.0003829124034382403}, {'label': 'fear', 'score': 0.0007618078961968422}, {'label': 'surprise', 'score': 0.001100933994166553}]], 'food': ['Food is above average.', [{'label': 'sadness', 'score': 0.004451553337275982}, {'label': 'joy', 'score': 0.9900580048561096}, {'label': 'love', 'score': 0.0008496023365296423}, {'label': 'anger', 'score': 0.002438510302454233}, {'label': 'fear', 'score': 0.0011346605606377125}, {'label': 'surprise', 'score': 0.0010676109232008457}]], 'filter water': ['You can get free filter water at the hand wash.', [{'label': 'sadness', 'score': 0.003469607327133417}, {'label': 'joy', 'score': 0.9852370619773865}, {'label': 'love', 'score': 0.0023572964128106833}, {'label': 'anger', 'score': 0.006796883884817362}, {'label': 'fear', 'score': 0.0011851026210933924}, {'label': 'surprise', 'score': 0.0009540374157950282}]]}\n",
      "Their nasi lemak tasted normal.\n",
      "ATE: ['nasi lemak']\n",
      "\n",
      "Result : {'nasi lemak': ['Their nasi lemak tasted normal.']}\n",
      "\n",
      "Hola :{'nasi lemak': ['Their nasi lemak tasted normal.', [{'label': 'sadness', 'score': 0.0013413196429610252}, {'label': 'joy', 'score': 0.9964738488197327}, {'label': 'love', 'score': 0.0004348277871031314}, {'label': 'anger', 'score': 0.0005947561585344374}, {'label': 'fear', 'score': 0.0005948285106569529}, {'label': 'surprise', 'score': 0.0005603227182291448}]]}\n",
      "Had this famous nasi Lemak for breakfast today. Came at 8:30am (public holiday today). It was open and already half full. There was a que but it was moving fast, thanks to ordering process.\n",
      "ATE: ['que', 'ordering process']\n",
      "\n",
      "Result : {'que': ['There was a que but it was moving fast, thanks to ordering process.'], 'ordering process': ['There was a que but it was moving fast, thanks to ordering process.']}\n",
      "\n",
      "\n",
      "Hola :{'que': ['There was a que but it was moving fast, thanks to ordering process.', [{'label': 'sadness', 'score': 0.0018041139701381326}, {'label': 'joy', 'score': 0.09420374035835266}, {'label': 'love', 'score': 0.0027129878289997578}, {'label': 'anger', 'score': 0.8534577488899231}, {'label': 'fear', 'score': 0.04168770834803581}, {'label': 'surprise', 'score': 0.006133750546723604}]], 'ordering process': ['There was a que but it was moving fast, thanks to ordering process.', [{'label': 'sadness', 'score': 0.0018041139701381326}, {'label': 'joy', 'score': 0.09420374035835266}, {'label': 'love', 'score': 0.0027129878289997578}, {'label': 'anger', 'score': 0.8534577488899231}, {'label': 'fear', 'score': 0.04168770834803581}, {'label': 'surprise', 'score': 0.006133750546723604}]]}\n",
      "Serving only nasi lemak, this huge restaurant offers various sides, fruit juices/smoothies, and desserts. There are 2 lines for either dining in or take out. Portions were not too big. The food was delicious. Lots of workers and plenty of \n",
      "ATE: ['portions', 'food']\n",
      "\n",
      "Result : {'portions': ['Portions were not too big.'], 'food': ['The food was delicious.']}\n",
      "\n",
      "\n",
      "Hola :{'portions': ['Portions were not too big.', [{'label': 'sadness', 'score': 0.022300809621810913}, {'label': 'joy', 'score': 0.4017271101474762}, {'label': 'love', 'score': 0.007890555076301098}, {'label': 'anger', 'score': 0.47642767429351807}, {'label': 'fear', 'score': 0.0816337838768959}, {'label': 'surprise', 'score': 0.010019989684224129}]], 'food': ['The food was delicious.', [{'label': 'sadness', 'score': 0.0002636945864651352}, {'label': 'joy', 'score': 0.9979211688041687}, {'label': 'love', 'score': 0.001038845512084663}, {'label': 'anger', 'score': 0.00027077971026301384}, {'label': 'fear', 'score': 0.00014136781101115048}, {'label': 'surprise', 'score': 0.00036411549081094563}]]}\n",
      "Nice and clean place. Price reasonable. Can try.\n",
      "ATE: ['clean', 'place', 'price']\n",
      "\n",
      "Result : {'clean': ['Nice and clean place.'], 'place': ['Nice and clean place.'], 'price': ['Price reasonable.']}\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'clean': ['Nice and clean place.', [{'label': 'sadness', 'score': 0.0008865142590366304}, {'label': 'joy', 'score': 0.9966512322425842}, {'label': 'love', 'score': 0.0013574990443885326}, {'label': 'anger', 'score': 0.0006723219412378967}, {'label': 'fear', 'score': 0.0002080494159599766}, {'label': 'surprise', 'score': 0.00022430402168538421}]], 'place': ['Nice and clean place.', [{'label': 'sadness', 'score': 0.0008865142590366304}, {'label': 'joy', 'score': 0.9966512322425842}, {'label': 'love', 'score': 0.0013574990443885326}, {'label': 'anger', 'score': 0.0006723219412378967}, {'label': 'fear', 'score': 0.0002080494159599766}, {'label': 'surprise', 'score': 0.00022430402168538421}]], 'price': ['Price reasonable.', [{'label': 'sadness', 'score': 0.0019600526429712772}, {'label': 'joy', 'score': 0.9931898713111877}, {'label': 'love', 'score': 0.001667940290644765}, {'label': 'anger', 'score': 0.0012555764988064766}, {'label': 'fear', 'score': 0.0009766689036041498}, {'label': 'surprise', 'score': 0.000949795707128942}]]}\n",
      "The food is very delicious and great!. Cheap price, recommend everyone to try it!!!\n",
      "ATE: ['food', 'price']\n",
      "\n",
      "Result : {'food': ['The food is very delicious and great!.'], 'price': ['Cheap price, recommend everyone to try it!!']}\n",
      "\n",
      "\n",
      "Hola :{'food': ['The food is very delicious and great!.', [{'label': 'sadness', 'score': 0.0001904518430819735}, {'label': 'joy', 'score': 0.9985927939414978}, {'label': 'love', 'score': 0.0007152028265409172}, {'label': 'anger', 'score': 0.00016839372983668}, {'label': 'fear', 'score': 0.00010042158100986853}, {'label': 'surprise', 'score': 0.00023260682064574212}]], 'price': ['Cheap price, recommend everyone to try it!!', [{'label': 'sadness', 'score': 0.00994091760367155}, {'label': 'joy', 'score': 0.9151105284690857}, {'label': 'love', 'score': 0.0036616658326238394}, {'label': 'anger', 'score': 0.06875220686197281}, {'label': 'fear', 'score': 0.0009052632376551628}, {'label': 'surprise', 'score': 0.0016294163651764393}]]}\n",
      "Good to try. Taste better. But sambal not hot.\n",
      "ATE: ['taste', 'sambal']\n",
      "\n",
      "Result : {'taste': ['Taste better.'], 'sambal': ['But sambal not hot.']}\n",
      "\n",
      "\n",
      "Hola :{'taste': ['Taste better.', [{'label': 'sadness', 'score': 0.0019580780062824488}, {'label': 'joy', 'score': 0.990574061870575}, {'label': 'love', 'score': 0.006047884002327919}, {'label': 'anger', 'score': 0.0006597329629585147}, {'label': 'fear', 'score': 0.00044531660387292504}, {'label': 'surprise', 'score': 0.0003149710246361792}]], 'sambal': ['But sambal not hot.', [{'label': 'sadness', 'score': 0.0008282499620690942}, {'label': 'joy', 'score': 0.0025958060286939144}, {'label': 'love', 'score': 0.9886528849601746}, {'label': 'anger', 'score': 0.003619024297222495}, {'label': 'fear', 'score': 0.001290048472583294}, {'label': 'surprise', 'score': 0.0030139328446239233}]]}\n",
      "Don't buy their food online. Terrible service, after 1 hour still in process with an excuse that they have too many customers onsite. Asked for refund also not prompt\n",
      "ATE: ['food', 'service']\n",
      "\n",
      "Result : {'food': [\"Don't buy their food online.\"], 'service': ['Terrible service, after 1 hour still in process with an excuse that they have too many customers onsite.']}\n",
      "\n",
      "\n",
      "Hola :{'food': [\"Don't buy their food online.\", [{'label': 'sadness', 'score': 0.037673842161893845}, {'label': 'joy', 'score': 0.6616054177284241}, {'label': 'love', 'score': 0.015698881819844246}, {'label': 'anger', 'score': 0.2708771228790283}, {'label': 'fear', 'score': 0.011113928630948067}, {'label': 'surprise', 'score': 0.0030309080611914396}]], 'service': ['Terrible service, after 1 hour still in process with an excuse that they have too many customers onsite.', [{'label': 'sadness', 'score': 0.6221145391464233}, {'label': 'joy', 'score': 0.0022220900282263756}, {'label': 'love', 'score': 0.0006424832390621305}, {'label': 'anger', 'score': 0.367695152759552}, {'label': 'fear', 'score': 0.006692319642752409}, {'label': 'surprise', 'score': 0.0006334829377010465}]]}\n",
      "The sambal so delicious \n",
      "ATE: ['sambal']\n",
      "\n",
      "Result : {'sambal': ['The sambal so delicious']}\n",
      "\n",
      "Hola :{'sambal': ['The sambal so delicious', [{'label': 'sadness', 'score': 0.00045843367115594447}, {'label': 'joy', 'score': 0.9966140389442444}, {'label': 'love', 'score': 0.0018990641692653298}, {'label': 'anger', 'score': 0.0004294153186492622}, {'label': 'fear', 'score': 0.00022558752971235663}, {'label': 'surprise', 'score': 0.00037337769754230976}]]}\n",
      "The overall food is okay. The sambal is good. The portion of nasi lemak is very small compared to the price. Even for someone like me who eats less rice, the portion seems insufficient. Hehe, but overall, it's okay; there is just a need for improvement in terms of portion. Thank you\n",
      "ATE: ['food', 'sambal', 'portion', 'price', 'portion', 'portion']\n",
      "\n",
      "Result : {'food': ['The overall food is okay.'], 'sambal': ['The sambal is good.'], 'portion': ['The portion of nasi lemak is very small compared to the price.', 'The portion of nasi lemak is very small compared to the price.', 'The portion of nasi lemak is very small compared to the price.', 'Even for someone like me who eats less rice, the portion seems insufficient.', 'Even for someone like me who eats less rice, the portion seems insufficient.', 'Even for someone like me who eats less rice, the portion seems insufficient.', \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\", \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\", \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\"], 'price': ['The portion of nasi lemak is very small compared to the price.']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'food': ['The overall food is okay.', [{'label': 'sadness', 'score': 0.0004726024635601789}, {'label': 'joy', 'score': 0.9977053999900818}, {'label': 'love', 'score': 0.001068092416971922}, {'label': 'anger', 'score': 0.000336056953528896}, {'label': 'fear', 'score': 0.00016944852541200817}, {'label': 'surprise', 'score': 0.000248369702603668}]], 'sambal': ['The sambal is good.', [{'label': 'sadness', 'score': 0.0009935878915712237}, {'label': 'joy', 'score': 0.9950683116912842}, {'label': 'love', 'score': 0.001498122699558735}, {'label': 'anger', 'score': 0.0016817506402730942}, {'label': 'fear', 'score': 0.00031431883689947426}, {'label': 'surprise', 'score': 0.0004440226184669882}]], 'portion': ['The portion of nasi lemak is very small compared to the price.', 'The portion of nasi lemak is very small compared to the price.', 'The portion of nasi lemak is very small compared to the price.', 'Even for someone like me who eats less rice, the portion seems insufficient.', 'Even for someone like me who eats less rice, the portion seems insufficient.', 'Even for someone like me who eats less rice, the portion seems insufficient.', \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\", \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\", \"Hehe, but overall, it's okay; there is just a need for improvement in terms of portion.\", [{'label': 'sadness', 'score': 0.21367613971233368}, {'label': 'joy', 'score': 0.3686217665672302}, {'label': 'love', 'score': 0.010259848088026047}, {'label': 'anger', 'score': 0.28943830728530884}, {'label': 'fear', 'score': 0.10869456827640533}, {'label': 'surprise', 'score': 0.009309392422437668}], [{'label': 'sadness', 'score': 0.21367613971233368}, {'label': 'joy', 'score': 0.3686217665672302}, {'label': 'love', 'score': 0.010259848088026047}, {'label': 'anger', 'score': 0.28943830728530884}, {'label': 'fear', 'score': 0.10869456827640533}, {'label': 'surprise', 'score': 0.009309392422437668}], [{'label': 'sadness', 'score': 0.21367613971233368}, {'label': 'joy', 'score': 0.3686217665672302}, {'label': 'love', 'score': 0.010259848088026047}, {'label': 'anger', 'score': 0.28943830728530884}, {'label': 'fear', 'score': 0.10869456827640533}, {'label': 'surprise', 'score': 0.009309392422437668}], [{'label': 'sadness', 'score': 0.9919941425323486}, {'label': 'joy', 'score': 0.0005538872210308909}, {'label': 'love', 'score': 0.0002531435457058251}, {'label': 'anger', 'score': 0.0053222584538161755}, {'label': 'fear', 'score': 0.0016532824374735355}, {'label': 'surprise', 'score': 0.0002231129037681967}], [{'label': 'sadness', 'score': 0.9919941425323486}, {'label': 'joy', 'score': 0.0005538872210308909}, {'label': 'love', 'score': 0.0002531435457058251}, {'label': 'anger', 'score': 0.0053222584538161755}, {'label': 'fear', 'score': 0.0016532824374735355}, {'label': 'surprise', 'score': 0.0002231129037681967}], [{'label': 'sadness', 'score': 0.9919941425323486}, {'label': 'joy', 'score': 0.0005538872210308909}, {'label': 'love', 'score': 0.0002531435457058251}, {'label': 'anger', 'score': 0.0053222584538161755}, {'label': 'fear', 'score': 0.0016532824374735355}, {'label': 'surprise', 'score': 0.0002231129037681967}], [{'label': 'sadness', 'score': 0.0009194735903292894}, {'label': 'joy', 'score': 0.9979402422904968}, {'label': 'love', 'score': 0.0005470026517286897}, {'label': 'anger', 'score': 0.00024026776372920722}, {'label': 'fear', 'score': 0.00015938005526550114}, {'label': 'surprise', 'score': 0.00019359767611604184}], [{'label': 'sadness', 'score': 0.0009194735903292894}, {'label': 'joy', 'score': 0.9979402422904968}, {'label': 'love', 'score': 0.0005470026517286897}, {'label': 'anger', 'score': 0.00024026776372920722}, {'label': 'fear', 'score': 0.00015938005526550114}, {'label': 'surprise', 'score': 0.00019359767611604184}], [{'label': 'sadness', 'score': 0.0009194735903292894}, {'label': 'joy', 'score': 0.9979402422904968}, {'label': 'love', 'score': 0.0005470026517286897}, {'label': 'anger', 'score': 0.00024026776372920722}, {'label': 'fear', 'score': 0.00015938005526550114}, {'label': 'surprise', 'score': 0.00019359767611604184}]], 'price': ['The portion of nasi lemak is very small compared to the price.', [{'label': 'sadness', 'score': 0.21367613971233368}, {'label': 'joy', 'score': 0.3686217665672302}, {'label': 'love', 'score': 0.010259848088026047}, {'label': 'anger', 'score': 0.28943830728530884}, {'label': 'fear', 'score': 0.10869456827640533}, {'label': 'surprise', 'score': 0.009309392422437668}]]}\n",
      "Fast service. Delicious nasi lemak. Tasty drink\n",
      "ATE: ['service', 'nasi lemak', 'drink']\n",
      "\n",
      "Result : {'service': ['Fast service.'], 'nasi lemak': ['Delicious nasi lemak.'], 'drink': ['Tasty drink']}\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'service': ['Fast service.', [{'label': 'sadness', 'score': 0.0011811527656391263}, {'label': 'joy', 'score': 0.994471549987793}, {'label': 'love', 'score': 0.0013315260875970125}, {'label': 'anger', 'score': 0.001107054646126926}, {'label': 'fear', 'score': 0.0010571792954578996}, {'label': 'surprise', 'score': 0.0008515319786965847}]], 'nasi lemak': ['Delicious nasi lemak.', [{'label': 'sadness', 'score': 0.00044033623998984694}, {'label': 'joy', 'score': 0.9972967505455017}, {'label': 'love', 'score': 0.0014302709605544806}, {'label': 'anger', 'score': 0.0003237625933252275}, {'label': 'fear', 'score': 0.0002052333438768983}, {'label': 'surprise', 'score': 0.00030367961153388023}]], 'drink': ['Tasty drink', [{'label': 'sadness', 'score': 0.0016768844798207283}, {'label': 'joy', 'score': 0.8157925605773926}, {'label': 'love', 'score': 0.16765443980693817}, {'label': 'anger', 'score': 0.00903165340423584}, {'label': 'fear', 'score': 0.001998430583626032}, {'label': 'surprise', 'score': 0.003846133127808571}]]}\n",
      "Ate here today... whoa, they need to tell the staff to smile more\n",
      "ATE: ['staff']\n",
      "\n",
      "Result : {'staff': ['Ate here today... whoa, they need to tell the staff to smile more']}\n",
      "\n",
      "Hola :{'staff': ['Ate here today... whoa, they need to tell the staff to smile more', [{'label': 'sadness', 'score': 0.001115216058678925}, {'label': 'joy', 'score': 0.9878057241439819}, {'label': 'love', 'score': 0.002375273033976555}, {'label': 'anger', 'score': 0.006631975062191486}, {'label': 'fear', 'score': 0.001144874026067555}, {'label': 'surprise', 'score': 0.000926931097637862}]]}\n",
      "The food server is very rude. No need to sit in front, sis, if you're going to treat people like that. The price of the dishes is considered cheap because the chicken is big. But charging the same price for rice and the dish is just not right. It's ridiculously minimal; you want to charge RM5.50 for this?! Capitalist\n",
      "ATE: ['food', 'server', 'price', 'price', 'dish']\n",
      "\n",
      "Result : {'food': ['The food server is very rude.'], 'server': ['The food server is very rude.'], 'price': ['The price of the dishes is considered cheap because the chicken is big.', 'The price of the dishes is considered cheap because the chicken is big.', 'But charging the same price for rice and the dish is just not right.', 'But charging the same price for rice and the dish is just not right.'], 'dish': ['But charging the same price for rice and the dish is just not right.']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'food': ['The food server is very rude.', [{'label': 'sadness', 'score': 0.00026295820134691894}, {'label': 'joy', 'score': 0.0002483269199728966}, {'label': 'love', 'score': 0.0005371593870222569}, {'label': 'anger', 'score': 0.9974365830421448}, {'label': 'fear', 'score': 0.0009794231737032533}, {'label': 'surprise', 'score': 0.0005355121684260666}]], 'server': ['The food server is very rude.', [{'label': 'sadness', 'score': 0.00026295820134691894}, {'label': 'joy', 'score': 0.0002483269199728966}, {'label': 'love', 'score': 0.0005371593870222569}, {'label': 'anger', 'score': 0.9974365830421448}, {'label': 'fear', 'score': 0.0009794231737032533}, {'label': 'surprise', 'score': 0.0005355121684260666}]], 'price': ['The price of the dishes is considered cheap because the chicken is big.', 'The price of the dishes is considered cheap because the chicken is big.', 'But charging the same price for rice and the dish is just not right.', 'But charging the same price for rice and the dish is just not right.', [{'label': 'sadness', 'score': 0.005128221120685339}, {'label': 'joy', 'score': 0.9849237203598022}, {'label': 'love', 'score': 0.003837579395622015}, {'label': 'anger', 'score': 0.0033476881217211485}, {'label': 'fear', 'score': 0.001409293501637876}, {'label': 'surprise', 'score': 0.0013535114703699946}], [{'label': 'sadness', 'score': 0.005128221120685339}, {'label': 'joy', 'score': 0.9849237203598022}, {'label': 'love', 'score': 0.003837579395622015}, {'label': 'anger', 'score': 0.0033476881217211485}, {'label': 'fear', 'score': 0.001409293501637876}, {'label': 'surprise', 'score': 0.0013535114703699946}], [{'label': 'sadness', 'score': 0.019164983183145523}, {'label': 'joy', 'score': 0.6260015368461609}, {'label': 'love', 'score': 0.007921380922198296}, {'label': 'anger', 'score': 0.33798086643218994}, {'label': 'fear', 'score': 0.004135376773774624}, {'label': 'surprise', 'score': 0.004795880988240242}], [{'label': 'sadness', 'score': 0.019164983183145523}, {'label': 'joy', 'score': 0.6260015368461609}, {'label': 'love', 'score': 0.007921380922198296}, {'label': 'anger', 'score': 0.33798086643218994}, {'label': 'fear', 'score': 0.004135376773774624}, {'label': 'surprise', 'score': 0.004795880988240242}]], 'dish': ['But charging the same price for rice and the dish is just not right.', [{'label': 'sadness', 'score': 0.019164983183145523}, {'label': 'joy', 'score': 0.6260015368461609}, {'label': 'love', 'score': 0.007921380922198296}, {'label': 'anger', 'score': 0.33798086643218994}, {'label': 'fear', 'score': 0.004135376773774624}, {'label': 'surprise', 'score': 0.004795880988240242}]]}\n",
      "Not so hype, and the food was okay. The environment was good. The chili sauce was kind of sweet for me which I do not prefer.\n",
      "ATE: ['food', 'environment', 'chili sauce']\n",
      "\n",
      "Result : {'food': ['Not so hype, and the food was okay.'], 'environment': ['The environment was good.'], 'chili sauce': ['The chili sauce was kind of sweet for me which I do not prefer.']}\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'food': ['Not so hype, and the food was okay.', [{'label': 'sadness', 'score': 0.0007784454501233995}, {'label': 'joy', 'score': 0.9966013431549072}, {'label': 'love', 'score': 0.0014502647100016475}, {'label': 'anger', 'score': 0.00047012424329295754}, {'label': 'fear', 'score': 0.0002860375680029392}, {'label': 'surprise', 'score': 0.00041375376167707145}]], 'environment': ['The environment was good.', [{'label': 'sadness', 'score': 0.0018679515924304724}, {'label': 'joy', 'score': 0.9946019649505615}, {'label': 'love', 'score': 0.001196073368191719}, {'label': 'anger', 'score': 0.0011465889401733875}, {'label': 'fear', 'score': 0.000525754876434803}, {'label': 'surprise', 'score': 0.0006617737817578018}]], 'chili sauce': ['The chili sauce was kind of sweet for me which I do not prefer.', [{'label': 'sadness', 'score': 0.0007023120997473598}, {'label': 'joy', 'score': 0.02876969240605831}, {'label': 'love', 'score': 0.9655871391296387}, {'label': 'anger', 'score': 0.00224359636195004}, {'label': 'fear', 'score': 0.0007900995551608503}, {'label': 'surprise', 'score': 0.0019072259310632944}]]}\n",
      "Limited parking space but accessible..\n",
      "ATE: ['parking']\n",
      "\n",
      "Result : {'parking': ['Limited parking space but accessible..']}\n",
      "\n",
      "Hola :{'parking': ['Limited parking space but accessible..', [{'label': 'sadness', 'score': 0.04090908542275429}, {'label': 'joy', 'score': 0.34075868129730225}, {'label': 'love', 'score': 0.013754722662270069}, {'label': 'anger', 'score': 0.23534661531448364}, {'label': 'fear', 'score': 0.34938284754753113}, {'label': 'surprise', 'score': 0.019848046824336052}]]}\n",
      "For me, it's kind of overrated because to me the sambal was ordinary...but serves many kinds of dishes too. Parking: Very crowded area, must pay to park at the parking area nearby\n",
      "ATE: ['sambal', 'crowded']\n",
      "\n",
      "Result : {'sambal': [\"For me, it's kind of overrated because to me the sambal was ordinary...but serves many kinds of dishes too.\"], 'crowded': ['Parking: Very crowded area, must pay to park at the parking area nearby']}\n",
      "\n",
      "\n",
      "Hola :{'sambal': [\"For me, it's kind of overrated because to me the sambal was ordinary...but serves many kinds of dishes too.\", [{'label': 'sadness', 'score': 0.0024179876782000065}, {'label': 'joy', 'score': 0.9900446534156799}, {'label': 'love', 'score': 0.0006929507362656295}, {'label': 'anger', 'score': 0.0011509608011692762}, {'label': 'fear', 'score': 0.0007692293147556484}, {'label': 'surprise', 'score': 0.004924126900732517}]], 'crowded': ['Parking: Very crowded area, must pay to park at the parking area nearby', [{'label': 'sadness', 'score': 0.022416600957512856}, {'label': 'joy', 'score': 0.00400309544056654}, {'label': 'love', 'score': 0.0007803397602401674}, {'label': 'anger', 'score': 0.9159384965896606}, {'label': 'fear', 'score': 0.055587850511074066}, {'label': 'surprise', 'score': 0.0012737184297293425}]]}\n",
      "Okay the portion of the rice is small\n",
      "ATE: ['portion', 'rice']\n",
      "\n",
      "Result : {'portion': ['Okay the portion of the rice is small'], 'rice': ['Okay the portion of the rice is small']}\n",
      "\n",
      "\n",
      "Hola :{'portion': ['Okay the portion of the rice is small', [{'label': 'sadness', 'score': 0.006391617469489574}, {'label': 'joy', 'score': 0.9833383560180664}, {'label': 'love', 'score': 0.002548340242356062}, {'label': 'anger', 'score': 0.005276545416563749}, {'label': 'fear', 'score': 0.0016633484046906233}, {'label': 'surprise', 'score': 0.0007817976293154061}]], 'rice': ['Okay the portion of the rice is small', [{'label': 'sadness', 'score': 0.006391617469489574}, {'label': 'joy', 'score': 0.9833383560180664}, {'label': 'love', 'score': 0.002548340242356062}, {'label': 'anger', 'score': 0.005276545416563749}, {'label': 'fear', 'score': 0.0016633484046906233}, {'label': 'surprise', 'score': 0.0007817976293154061}]]}\n",
      "Took sotong which was ok. Sambal good but on the sweet side. Rice was a total let down. It was soggy and wet with no hint of coconut milk. Overall a bit overrated. Nasi lemak RM 10 and Nescafe drink RM4. Parking RM5. Price wise ok.\n",
      "ATE: ['sotong', 'sambal', 'rice', 'parking', 'price']\n",
      "\n",
      "Result : {'sotong': ['Took sotong which was ok. Sambal good but on the sweet side.'], 'sambal': ['Took sotong which was ok. Sambal good but on the sweet side.'], 'rice': ['Rice was a total let down.'], 'parking': ['Parking RM5.'], 'price': ['Price wise ok.']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hola :{'sotong': ['Took sotong which was ok. Sambal good but on the sweet side.', [{'label': 'sadness', 'score': 0.0013068937696516514}, {'label': 'joy', 'score': 0.1826002597808838}, {'label': 'love', 'score': 0.8108959794044495}, {'label': 'anger', 'score': 0.0032816894818097353}, {'label': 'fear', 'score': 0.0005612343666143715}, {'label': 'surprise', 'score': 0.0013539559440687299}]], 'sambal': ['Took sotong which was ok. Sambal good but on the sweet side.', [{'label': 'sadness', 'score': 0.0013068937696516514}, {'label': 'joy', 'score': 0.1826002597808838}, {'label': 'love', 'score': 0.8108959794044495}, {'label': 'anger', 'score': 0.0032816894818097353}, {'label': 'fear', 'score': 0.0005612343666143715}, {'label': 'surprise', 'score': 0.0013539559440687299}]], 'rice': ['Rice was a total let down.', [{'label': 'sadness', 'score': 0.7632854580879211}, {'label': 'joy', 'score': 0.0027132306713610888}, {'label': 'love', 'score': 0.000790874008089304}, {'label': 'anger', 'score': 0.2284916639328003}, {'label': 'fear', 'score': 0.0037461298052221537}, {'label': 'surprise', 'score': 0.000972711481153965}]], 'parking': ['Parking RM5.', [{'label': 'sadness', 'score': 0.09233900904655457}, {'label': 'joy', 'score': 0.3153575360774994}, {'label': 'love', 'score': 0.009496946819126606}, {'label': 'anger', 'score': 0.396518349647522}, {'label': 'fear', 'score': 0.17087236046791077}, {'label': 'surprise', 'score': 0.015415805391967297}]], 'price': ['Price wise ok.', [{'label': 'sadness', 'score': 0.0009524889755994081}, {'label': 'joy', 'score': 0.9961329698562622}, {'label': 'love', 'score': 0.0010690671624615788}, {'label': 'anger', 'score': 0.0009879450080916286}, {'label': 'fear', 'score': 0.00043604211532510817}, {'label': 'surprise', 'score': 0.0004213849315419793}]]}\n",
      "Took squid which was ok. The chili gravy good but on the sweet side. Rice was a total let down. It was soggy and wet with no hint of coconut milk. Overall a bit overrated. Nasi lemak RM 10 and Nescafe drink RM4. Parking RM5. Price wise ok.\n",
      "ATE: ['squid', 'chili gravy', 'rice', 'parking', 'price']\n",
      "\n",
      "Result : {'squid': ['Took squid which was ok.'], 'chili gravy': ['The chili gravy good but on the sweet side.'], 'rice': ['Rice was a total let down.'], 'parking': ['Parking RM5.'], 'price': ['Price wise ok.']}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20000\\2692410551.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Perform ATE and emotion prediction for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mATE_emotion_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_ATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# print (terms)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Hola :{result}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20000\\3698924235.py\u001b[0m in \u001b[0;36mATE_emotion_prediction\u001b[1;34m(text, model_ATE, tokenizer, emotion_classifier)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# Assuming you have an emotion_classifier function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0memotion_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotion_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[0memotion_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maspect\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0memotion_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"top_k\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             )\n\u001b[0;32m   1119\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36mrun_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m         \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1024\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                     \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1569\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1570\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1571\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1572\u001b[0m         )\n\u001b[0;32m   1573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m         )\n\u001b[0;32m   1032\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    615\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m                 )\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m         )\n\u001b[0;32m    502\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         )\n\u001b[0;32m    434\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "results_df = pd.DataFrame(columns=['Aspect', 'Emotion'])\n",
    "with open('CSV_Review.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        text = row['Text']\n",
    "        print(text)\n",
    "        # Perform ATE and emotion prediction for each row\n",
    "        terms, result = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "        # print (terms)\n",
    "        print(f\"Hola :{result}\")\n",
    "        # Append the results to the DataFrame\n",
    "        results_df = results_df.append({'Text': text, 'Aspect': terms, 'Emotion': result}, ignore_index=True)\n",
    "        # results_df.to_csv('dashboard.csv', index=False)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Aspect  label_1  score_1 label_2  score_2 label_3  score_3   label_4  \\\n",
      "0   sotong     love    0.811     joy    0.183   anger    0.003  surprise   \n",
      "1   sambal     love    0.811     joy    0.183   anger    0.003  surprise   \n",
      "2     rice  sadness    0.763   anger    0.228    fear    0.004       joy   \n",
      "3  parking    anger    0.397     joy    0.315    fear    0.171   sadness   \n",
      "4    price      joy    0.996    love    0.001   anger    0.001   sadness   \n",
      "\n",
      "   score_4   label_5  score_5   label_6  score_6  \n",
      "0    0.001   sadness    0.001      fear    0.001  \n",
      "1    0.001   sadness    0.001      fear    0.001  \n",
      "2    0.003  surprise    0.001      love    0.001  \n",
      "3    0.092  surprise    0.015      love    0.009  \n",
      "4    0.001      fear    0.000  surprise    0.000  \n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each aspect in the result dictionary\n",
    "for aspect, data in result.items():\n",
    "    # Extract the emotions list from the aspect entry\n",
    "    emotions_list = data[1]\n",
    "\n",
    "    # Custom sorting function based on the 'score' values\n",
    "    def custom_sort(emotion):\n",
    "        return float(emotion['score'])\n",
    "\n",
    "    # Sort the emotions list based on the 'score' values in descending order using the custom sorting function\n",
    "    sorted_emotion_data = sorted(emotions_list, key=custom_sort, reverse=True)\n",
    "\n",
    "    # Round the 'score' values to 3 decimal places in the sorted list\n",
    "    rounded_sorted_emotion_data = [{'label': entry['label'], 'score': round(float(entry['score']), 3)} for entry in sorted_emotion_data]\n",
    "\n",
    "    # Create a DataFrame for the current aspect\n",
    "    df = pd.DataFrame(rounded_sorted_emotion_data)\n",
    "\n",
    "    # Add the aspect name as a new column\n",
    "    df['Aspect'] = aspect\n",
    "\n",
    "    # Reshape the DataFrame for the desired output\n",
    "    df_output = df.set_index(['Aspect', df.groupby('Aspect').cumcount() + 1]).unstack().sort_index(axis=1, level=1)\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    df_output.columns = [f\"{col[0]}_{col[1]}\" for col in df_output.columns]\n",
    "\n",
    "    # Reset index for better formatting\n",
    "    df_output.reset_index(inplace=True)\n",
    "\n",
    "    # Append the current aspect's DataFrame to the final DataFrame\n",
    "    final_df = final_df.append(df_output, ignore_index=True)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "d:\\FYP_A+\\Project_Update\\ATEABSA\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "         Aspect Label1  Score1   Label2  Score2    Label3  Score3   Label4  \\\n",
      "0          line   fear   0.929    anger   0.048       joy   0.010  sadness   \n",
      "1   prayer room    joy   0.872     love   0.099     anger   0.014  sadness   \n",
      "2   dining area    joy   0.995     love   0.002  surprise   0.001     fear   \n",
      "3          food    joy   0.990  sadness   0.004     anger   0.002     fear   \n",
      "4  filter water    joy   0.985    anger   0.007   sadness   0.003     love   \n",
      "\n",
      "   Score4    Label5  Score5    Label6  Score6  \n",
      "0   0.008  surprise   0.003      love   0.002  \n",
      "1   0.010      fear   0.005  surprise   0.001  \n",
      "2   0.001   sadness   0.001     anger   0.000  \n",
      "3   0.001  surprise   0.001      love   0.001  \n",
      "4   0.002      fear   0.001  surprise   0.001  \n"
     ]
    }
   ],
   "source": [
    "#CSV FILE\n",
    "import csv\n",
    "import torch\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\n",
    "\n",
    "def extract_aspect_sentences(text, aspects):\n",
    "    sentences = sent_tokenize(text)\n",
    "    aspect_sentences = {aspect: [] for aspect in aspects}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "        for aspect in aspects:\n",
    "            aspect_keywords = [word.lower() for word in aspect.split()]\n",
    "            if any(keyword in [word.lower() for word, pos in pos_tags] for keyword in aspect_keywords):\n",
    "                aspect_sentences[aspect].append(sentence)\n",
    "\n",
    "    return aspect_sentences\n",
    "\n",
    "\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "\n",
    "def ATE_emotion_prediction_multi(text, model_ATE, tokenizer, emotion_classifier):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            # If the label is 1, it indicates the start of a new aspect term\n",
    "            if len(word) != 0:\n",
    "                # If there's an existing word, append it to the terms list after removing \" ##\"\n",
    "                terms.append(word.replace(\" ##\", \"\"))\n",
    "            # Start a new word with the current token\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "\n",
    "    if len(word) != 0:\n",
    "        terms.append(word.replace(\" ##\", \"\"))\n",
    "\n",
    "    combined_terms = []\n",
    "    for term in terms:\n",
    "        if \"#\" in term:\n",
    "            if combined_terms:\n",
    "                combined_terms[-1] += term\n",
    "        else:\n",
    "            combined_terms.append(term)\n",
    "\n",
    "    terms_without_hashtag = [term.replace(\"#\", \"\") for term in combined_terms]\n",
    "    for i in range(len(terms_without_hashtag)):\n",
    "        if  terms_without_hashtag[i] == 'nasilemak':\n",
    "            terms_without_hashtag[i] = 'nasi lemak'\n",
    "           \n",
    "    # Print the original tokens and the extracted aspect terms\n",
    "    # print(\"Tokens:\", x)\n",
    "    # print(\"ATE:\", terms_without_hashtag)\n",
    "    # print()\n",
    "\n",
    "    result = extract_aspect_sentences(text, terms_without_hashtag)\n",
    "    emotion_dict = {}\n",
    "    emotion_dict = copy.deepcopy(result)\n",
    "    \n",
    "    # print(type(result))\n",
    "    # print(f\"Result : {result}\")\n",
    "    for aspect, sentences in result.items():\n",
    "        # print(f\"{aspect} sentences:\")\n",
    "        # print(\"iteration aspect sentence\")\n",
    "        # Loop over each sentence for the current aspect\n",
    "        for sentence in sentences:\n",
    "            # print(f\"- {sentence}\")\n",
    "\n",
    "            emotion_result = emotion_classifier(sentence)\n",
    "            emotion_dict[aspect]+= emotion_result\n",
    "\n",
    "        print()  \n",
    "\n",
    "    return terms_without_hashtag, emotion_dict\n",
    "\n",
    "\n",
    "def process_csv_file(input_file_path, output_file_path):\n",
    "    results_df = pd.DataFrame(columns=['Aspect', 'Label1', 'Score1', 'Label2', 'Score2', 'Label3', 'Score3', 'Label4', 'Score4', 'Label5', 'Score5', 'Label6', 'Score6'])\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            terms, result_prediction = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "            aspect_df = pd.DataFrame()\n",
    "            for aspect, data in result_prediction.items():\n",
    "                emotions_list = data[1]\n",
    "\n",
    "                # Custom sorting function based on the 'score' values\n",
    "                def custom_sort(emotion):\n",
    "                    try:\n",
    "                        return float(emotion['score'])\n",
    "                    except (ValueError, TypeError):\n",
    "                        return 0.0\n",
    "\n",
    "                sorted_emotion_data = sorted(emotions_list, key=custom_sort, reverse=True)\n",
    "                rounded_sorted_emotion_data = [{'label': entry['label'], 'score': round(float(entry['score']), 3)} for entry in sorted_emotion_data]\n",
    "\n",
    "                # Create a new row for each aspect\n",
    "                new_row = {'Aspect': aspect}\n",
    "                for i, entry in enumerate(rounded_sorted_emotion_data):\n",
    "                    new_row[f'Label{i+1}'] = entry['label']\n",
    "                    new_row[f'Score{i+1}'] = entry['score']\n",
    "\n",
    "                # Append the new row to the DataFrame\n",
    "                results_df = results_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    print(results_df.head())       \n",
    "    results_df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = 'CSV_Review.csv'\n",
    "output_file_path = 'LALALa.csv'\n",
    "process_csv_file(input_file_path, output_file_path)\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(input_file_path, output_file_path):\n",
    "    results_df = pd.DataFrame(columns=['Aspect', 'Label1','Score1','Label2','Score2','Label3','Score3','Label4','Score4','Label5','Score5','Label6','Score6'])\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            terms, result_prediction = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "            print(\"Result lepas predict: \",result_prediction)\n",
    "            aspect_df = pd.DataFrame()\n",
    "            for aspect, data in result_prediction.items():\n",
    "                print(\"ASPECT : \",aspect)\n",
    "                emotions_list = data[1]\n",
    "\n",
    "                # Custom sorting function based on the 'score' values\n",
    "                # def custom_sort(emotion):\n",
    "                #     return float(emotion['score'])\n",
    "\n",
    "                def custom_sort(emotion):\n",
    "                    try:\n",
    "                        return float(emotion['score'])\n",
    "                    except (ValueError, TypeError):\n",
    "                        return 0.0 \n",
    "\n",
    "                print(\"nak amik list emotion ja: \",emotions_list)\n",
    "                sorted_emotion_data = sorted(emotions_list, key=custom_sort, reverse=True)\n",
    "                rounded_sorted_emotion_data = [{'label': entry['label'], 'score': round(float(entry['score']), 3)} for entry in sorted_emotion_data]\n",
    "                print(\"Round value score : \", rounded_sorted_emotion_data)\n",
    "\n",
    "                # Add a new row\n",
    "                new_row = {'Aspect': aspect, 'Label1': , 'Score1': , 'Label2': , 'Score2': , 'Label3': , 'Score3': , 'Label4': , 'Score4': , 'Label5': , 'Score5': , 'Label6': , 'Score6':  }\n",
    "                results_df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "                df = pd.DataFrame(rounded_sorted_emotion_data)\n",
    "                results['Aspect'] = aspect\n",
    "                df_output = df.set_index(['Aspect', df.groupby('Aspect').cumcount() + 1]).unstack().sort_index(axis=1, level=1)\n",
    "                df_output.columns = [f\"{col[0]}_{col[1]}\" for col in df_output.columns]\n",
    "                df_output.reset_index(inplace=True)\n",
    "                aspect_df = aspect_df.append(df_output, ignore_index=True)\n",
    "\n",
    "            results_df = results_df.append({'Text': text, 'Aspect': terms, 'Emotion': aspect_df}, ignore_index=True)\n",
    "            \n",
    "    results_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: ['food', 'parking']\n",
      "\n",
      "Result : {'food': ['I love the food.'], 'parking': ['Parking is hard']}\n",
      "\n",
      "\n",
      "<class 'tuple'>\n",
      "{'food': ['I love the food.', [{'label': 'sadness', 'score': 0.001409263932146132}, {'label': 'joy', 'score': 0.9703583121299744}, {'label': 'love', 'score': 0.024186555296182632}, {'label': 'anger', 'score': 0.0032605831511318684}, {'label': 'fear', 'score': 0.0002018398226937279}, {'label': 'surprise', 'score': 0.0005834508338011801}]], 'parking': ['Parking is hard', [{'label': 'sadness', 'score': 0.008521218784153461}, {'label': 'joy', 'score': 0.0030597879085689783}, {'label': 'love', 'score': 0.0005783014930784702}, {'label': 'anger', 'score': 0.9835397005081177}, {'label': 'fear', 'score': 0.0037728159222751856}, {'label': 'surprise', 'score': 0.0005281605990603566}]]}\n",
      "\n",
      "Aspect: food\n",
      "\n",
      "Aspect: parking\n",
      "Emotion Prediction DataFrame:\n",
      "    Aspect Label1  Score1   Label2  Score2 Label3  Score3   Label4  Score4  \\\n",
      "0     food    joy   0.970     love   0.024  anger   0.003  sadness   0.001   \n",
      "1  parking  anger   0.984  sadness   0.009   fear   0.004      joy   0.003   \n",
      "\n",
      "     Label5  Score5    Label6  Score6  \n",
      "0  surprise   0.001      fear   0.000  \n",
      "1      love   0.001  surprise   0.001  \n"
     ]
    }
   ],
   "source": [
    "def sorted_round(list_emotion):\n",
    "    def custom_sort(emotion):\n",
    "            try:\n",
    "                return float(emotion['score'])\n",
    "            except (ValueError, TypeError):\n",
    "                return 0.0\n",
    "\n",
    "    sorted_emotion_data = sorted(list_emotion, key=custom_sort, reverse=True)\n",
    "    rounded_sorted_emotion_data = [{'label': entry['label'], 'score': round(float(entry['score']),3)} for entry in sorted_emotion_data]\n",
    "\n",
    "    return rounded_sorted_emotion_data\n",
    "text = \"I love the food. Parking is hard\"\n",
    "\n",
    "prediction_result = ATE_emotion_prediction(text, model_ATE, tokenizer, classifier)\n",
    "print(type(prediction_result))\n",
    "print(prediction_result[1])\n",
    "results_df = pd.DataFrame(columns=['Aspect', 'Label1', 'Score1', 'Label2', 'Score2', 'Label3', 'Score3', 'Label4', 'Score4', 'Label5', 'Score5', 'Label6', 'Score6'])\n",
    "            \n",
    "for aspect, aspect_emotions in prediction_result[1].items():\n",
    "    print(f\"\\nAspect: {aspect}\")\n",
    "    if aspect_emotions:\n",
    "        emotion_list = aspect_emotions[1]  \n",
    "        rounded_sorted_emotion_data = sorted_round(emotion_list)\n",
    "\n",
    "        new_row = {'Aspect': aspect}\n",
    "        for i, entry in enumerate(rounded_sorted_emotion_data):\n",
    "            new_row[f'Label{i+1}'] = entry['label']\n",
    "            new_row[f'Score{i+1}'] = entry['score']\n",
    "        results_df = results_df.append(new_row, ignore_index=True)\n",
    "\n",
    "print(\"Emotion Prediction DataFrame:\")\n",
    "print(results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
